{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terminology - Project\n",
    "Authors: CÃ©cile MACAIRE & Ludivine ROBERT "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librairies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "spacy_nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from lexicon\n",
    "def read_data(file):\n",
    "    \"\"\"Read data file with pandas dataframe\"\"\"\n",
    "    return pd.read_csv(file, sep='\\t')\n",
    "\n",
    "def select_data(dataframe):\n",
    "    \"\"\"Lemmatization of lexicon with scapy\"\"\"\n",
    "    terms = dataframe['pilot']\n",
    "    lemma = []\n",
    "    for el in terms:\n",
    "        doc = spacy_nlp(el.lower())\n",
    "        tmp = [token.lemma_ for token in doc]\n",
    "        lemma = [l.replace(' - ', '-') for l in lemma]\n",
    "        lemma.append(' '.join(tmp))\n",
    "    df = pd.DataFrame({'pattern':dataframe['pattern'], 'pilot':dataframe['pilot'], 'lemma':lemma})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text\n",
    "def read_file(file):\n",
    "    with open(file, 'r') as f:\n",
    "        return f.read()\n",
    "        \n",
    "def lemma_posttag(file):\n",
    "    \"\"\"Convert post-tag scapy into corresponding pattern from lexicon\"\"\"\n",
    "    text = read_file(file)\n",
    "    doc_a = spacy_nlp(text)\n",
    "    doc = spacy_nlp(text.lower())\n",
    "    new_pos = []\n",
    "    pos = []\n",
    "    lemma = []\n",
    "    t = []\n",
    "    original = [token.text for token in doc_a]\n",
    "    for token in doc:\n",
    "        t.append(token.text)\n",
    "        lemma.append(token.lemma_)\n",
    "        pos.append(token.pos_)\n",
    "        if token.pos_ == 'NOUN' or token.pos_ == 'PROPN':\n",
    "            new_pos.append('N')\n",
    "        elif token.pos_ == 'VERB':\n",
    "            new_pos.append('V')\n",
    "        elif token.pos_ == 'ADJ':\n",
    "            new_pos.append('A')\n",
    "        elif token.pos_ == 'CCONJ' or token.pos_ == 'SCONJ':\n",
    "            new_pos.append('C')\n",
    "        elif token.pos_ == 'PART' or token.pos_ == 'ADP':\n",
    "            new_pos.append('P')\n",
    "        else:\n",
    "            new_pos.append('')\n",
    "    frame = pd.DataFrame({'tokens': original,'tokens_lower':t, 'lemma':lemma, 'pos':pos, 'pattern':new_pos})\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rules(terms_dataframe, text_dataframe):\n",
    "    \"\"\"Define rules from terms according to their pattern\"\"\"\n",
    "    new_terms = []\n",
    "    for terms in terms_dataframe['lemma']:\n",
    "        # Get the same structure of terms as in text dataframe\n",
    "        tmp = ' '.join(terms.split('-'))\n",
    "        new_terms.append(tmp.split(' '))\n",
    "    for i, token in enumerate(text_dataframe['lemma']):\n",
    "        for j, t in enumerate(new_terms):\n",
    "            # Case 1: term of size 3 seperated by dashes (ex: text-to-speech) and followed by 1, 2 Nouns or 1 Adj and 1 Noun is a term \n",
    "            if len(t) == 3 and len(text_dataframe['lemma']) >= i+5:\n",
    "                if token == t[0] and text_dataframe['lemma'][i+1] == '-' and (text_dataframe['lemma'][i+2] == 'to' or text_dataframe['lemma'][i+2] == 'of' or text_dataframe['pattern'][i+2] == 'N') and text_dataframe['lemma'][i+3] == '-' and text_dataframe['lemma'][i+4] == t[2]:\n",
    "                    # followed by 2 nouns (ex: text-to-speech modal synthesis)\n",
    "                    if (text_dataframe['pattern'][i+4] == 'N' or text_dataframe['pattern'][i+4] == 'A') and text_dataframe['pattern'][i+5] == 'N':\n",
    "                        text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                        text_dataframe['tokens'][i+5] = text_dataframe['tokens'][i+5]+']'                        \n",
    "                    elif text_dataframe['pattern'][i+4] == 'N':\n",
    "                        # followed by 1 noun (ex: text-to-speech system)\n",
    "                        text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                        text_dataframe['tokens'][i+4] = text_dataframe['tokens'][i+4]+']'\n",
    "                    else:\n",
    "                        text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]+']'\n",
    "            # Case 2: term of size 2 separated by dashes (ex: encoder-decoder) and followed by 0,1,2 or 3 nouns is a term\n",
    "            elif len(t) >= 2 and len(text_dataframe['lemma']) >= i+3 and i != 0:\n",
    "                if token == 'front' and text_dataframe['lemma'][i+1] == '-' and text_dataframe['lemma'][i+2] == 'end':\n",
    "                    if text_dataframe['pattern'][i-1] == 'N':\n",
    "                        text_dataframe['tokens'][i-1] = '['+text_dataframe['tokens'][i-1]\n",
    "                        text_dataframe['tokens'][i+2] = text_dataframe['tokens'][i+2]+']'\n",
    "                if token == t[0] and text_dataframe['lemma'][i+1] == '-' and text_dataframe['lemma'][i+2] == t[1]:\n",
    "                    # followed by 3 nouns (ex: HMM-based generation synthesis approach)\n",
    "                    if text_dataframe['pattern'][i+3] == 'N' and text_dataframe['pattern'][i+4] == 'N' and text_dataframe['pattern'][i+5] == 'N':\n",
    "                        text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                        text_dataframe['tokens'][i+5] = text_dataframe['tokens'][i+5]+']'\n",
    "                    # followed by 2 nouns (ex: HMM-based generation synthesis)\n",
    "                    elif (text_dataframe['pattern'][i+3] == 'N' or text_dataframe['pattern'][i+3] == 'A') and text_dataframe['pattern'][i+4] == 'N':\n",
    "                        text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                        text_dataframe['tokens'][i+4] = text_dataframe['tokens'][i+4]+']'\n",
    "                    # followed by 1 noun (ex: cross-lingual adaptation)\n",
    "                    elif text_dataframe['pattern'][i+3] == 'N':\n",
    "                            text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                            text_dataframe['tokens'][i+3] = text_dataframe['tokens'][i+3]+']'\n",
    "                    # followed by nothing (ex: mel-spectrogram)\n",
    "                    else:\n",
    "                        text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                        text_dataframe['tokens'][i+2] = text_dataframe['tokens'][i+2]+']'\n",
    "        if (token == 'data' or token == 'datum' or token == 'speaker' or token == 'dataset' or token == 'database' or token == 'feature' or token == 'corpus' or token == 'language') and i != 0 and len(text_dataframe['lemma']) >= i+1:\n",
    "            if text_dataframe['pattern'][i-1] == 'N' or text_dataframe['pattern'][i-1] == 'A':\n",
    "                text_dataframe['tokens'][i-1] = '['+text_dataframe['tokens'][i-1]\n",
    "                text_dataframe['tokens'][i] = text_dataframe['tokens'][i]+']'\n",
    "            elif text_dataframe['pattern'][i+1] == 'N':\n",
    "                text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                text_dataframe['tokens'][i+1] = text_dataframe['tokens'][i+1]+']'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_4 = ['system', 'model', 'synthesis', 'translation', 'recognition', 'signal', 'research', \n",
    "          'processing', 'conversion', 'technique', 'accuracy', 'synthesizer', 'architecture', \n",
    "          'form', 'transcription', 'alignment', 'optimization', 'task', 'function','token',\n",
    "         'activation', 'layer', 'experiment', 'output', 'representation', 'setting', 'control', \n",
    "         'network', 'quality', 'file', 'learning', 'framework', 'transform', 'sequence', 'length',\n",
    "         'tool']\n",
    "def annotate(terms_dataframe, text_dataframe):\n",
    "    \"\"\"Annotate the terms of the text thanks to list of terms + applied rules\"\"\"\n",
    "    rules(terms_dataframe, text_dataframe)  # apply rules\n",
    "    for i, token in enumerate(text_dataframe['lemma']):\n",
    "        for term in terms_dataframe['lemma']:\n",
    "            term = term.split(' ')\n",
    "            # Case 1: if terms of length 4, we check if each word from text corresponds to each word in the term\n",
    "            if len(term) == 4:\n",
    "                term_1 = term[0]\n",
    "                if token == term_1 and len(text_dataframe['lemma']) >= i+4:\n",
    "                    if text_dataframe['lemma'][i+1] == term[1] and text_dataframe['lemma'][i+2] == term[2] and text_dataframe['lemma'][i+3] == term[3]:\n",
    "                        if text_dataframe['lemma'][i+4] in rule_4:\n",
    "                            text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                            text_dataframe['tokens'][i+4] = text_dataframe['tokens'][i+4]+']'\n",
    "                        else:\n",
    "                            text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                            text_dataframe['tokens'][i+3] = text_dataframe['tokens'][i+3]+']'\n",
    "            # Case 2: terms of length 3\n",
    "            elif len(term) == 3:\n",
    "                term_1 = term[0]\n",
    "                if token == term_1 and len(text_dataframe['lemma']) > i+3:\n",
    "                    if text_dataframe['lemma'][i+1] == term[1] and text_dataframe['lemma'][i+2] == term[2]:\n",
    "                        if text_dataframe['lemma'][i+3] in rule_4:\n",
    "                            text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                            text_dataframe['tokens'][i+3] = text_dataframe['tokens'][i+3]+']'\n",
    "                        else:\n",
    "                            text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                            text_dataframe['tokens'][i+2] = text_dataframe['tokens'][i+2]+']'\n",
    "            # Case 3: terms of length 2\n",
    "            elif len(term) == 2:\n",
    "                if token == term[0] and len(text_dataframe['lemma']) > i+2:\n",
    "                    if text_dataframe['lemma'][i+1] == term[1]:\n",
    "                        if text_dataframe['lemma'][i+2] in rule_4:\n",
    "                            text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                            text_dataframe['tokens'][i+2] = text_dataframe['tokens'][i+2]+']'\n",
    "                        else:\n",
    "                            text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                            text_dataframe['tokens'][i+1] = text_dataframe['tokens'][i+1]+']'\n",
    "            # Case 4: term of length 1\n",
    "            elif token == term[0] and i > 1 and text_dataframe['lemma'][i-1] == 'of' and text_dataframe['lemma'][i-2] == 'sequence':\n",
    "                text_dataframe['tokens'][i-2] = '['+text_dataframe['tokens'][i-2]\n",
    "                text_dataframe['tokens'][i] = text_dataframe['tokens'][i]+']'\n",
    "            elif token == term[0] and len(term) == 1 and len(text_dataframe['lemma']) >= i+2 and text_dataframe['lemma'][i+1] == ')':\n",
    "                if text_dataframe['lemma'][i+2] in rule_4:\n",
    "                    text_dataframe['tokens'][i-1] = '['+text_dataframe['tokens'][i-1]\n",
    "                    text_dataframe['tokens'][i+2] = text_dataframe['tokens'][i+2]+']'\n",
    "                else:\n",
    "                    text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]+']'\n",
    "            elif token == term[0] and len(term) == 1 and len(text_dataframe['lemma']) >= i+1:\n",
    "                if text_dataframe['lemma'][i+1] in rule_4:\n",
    "                    text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                    text_dataframe['tokens'][i+1] = text_dataframe['tokens'][i+1]+']'\n",
    "                else:\n",
    "                    text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]+']'\n",
    "    return text_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_annotated_text(text_dataframe):\n",
    "    \"\"\"Return the text from the annotated text dataframe with the correct annotation of brackets\"\"\"\n",
    "    content = ' '.join(text_dataframe['tokens'].to_list())\n",
    "    compt = 0\n",
    "    compt2 = 0\n",
    "    string = ''\n",
    "    for i in content:\n",
    "        if i == '[':\n",
    "            if compt == 0:\n",
    "                compt += 1\n",
    "                string += i\n",
    "            elif compt >= 1:\n",
    "                compt += 1\n",
    "        elif i == ']':\n",
    "            if compt-1 != compt2:\n",
    "                compt2 += 1\n",
    "            else:\n",
    "                string += i\n",
    "                compt = 0\n",
    "                compt2 = 0\n",
    "        else:\n",
    "            string += i\n",
    "    string2 = ''\n",
    "    string = string.replace('] [', ' ')\n",
    "    string = string.replace(' .', '.')\n",
    "    string = string.replace(' â', 'â')\n",
    "    string = string.replace(' ,', ',')\n",
    "    string = string.replace(' - ', '-')\n",
    "    string = string.replace('( ', '(')\n",
    "    string = string.replace(' )', ')')\n",
    "    string = string.replace(']-[', '-')\n",
    "    string = string.replace('.]', '].')\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FastSpeech] : Fast, Robust and Controllable [Text to Speech] \n",
      "\n",
      " Abstract \n",
      "\n",
      " [Neural network] based [end-to-end text to speech] ([TTS]) has significantly improved the quality of [synthesized speech]. \n",
      " Prominent methods (e.g., [Tacotron] 2) usually first generate [mel-spectrogram] from text, and then [synthesize speech] from the [mel-spectrogram] using [vocoder] such as [WaveNet]. \n",
      " Compared with traditional concatenative and [statistical parametric approaches], [neural network] based [end-to-end models] suffer from [slow inference speed], and the [synthesized speech] is usually not robust (i.e., some words are skipped or repeated) and lack of controllability ([voice speed] or [prosody control]). \n",
      " In this work, we propose a novel [feed-forward network] based on [Transformer] to generate [mel-spectrogram] in parallel for [TTS]. \n",
      " Specifically, we extract [attention alignments] from an [encoder-decoder] based teacher model for [phoneme duration prediction], which is used by a length regulator to expand the source [phoneme sequence] to match the length of the target [mel-spectrogram sequence] for parallel [mel-spectrogram generation]. \n",
      " Experiments on the [LJSpeech dataset] show that our parallel model matches [autoregressive models] in terms of [speech quality], nearly eliminates the problem of word skipping and repeating in particularly hard cases, and can adjust [voice speed] smoothly. \n",
      " Most importantly, compared with [autoregressive Transformer TTS], our model speeds up [mel-spectrogram generation] by 270x and the [end-to-end speech synthesis] by 38x. \n",
      " Therefore, we call our model [FastSpeech]. \n",
      "\n",
      " [Synthesized speech] samples can be found in https://speechresearch.github.io/fastspeech/. \n",
      "\n",
      " Introduction \n",
      "\n",
      " [Text to speech] ([TTS]) has attracted a lot of attention in recent years due to the advance in [deep learning]. [Deep neural network] based systems have become more and more popular for [TTS], such as [Tacotron], [Tacotron] 2, [Deep Voice] 3, and the fully [end-to-end ClariNet]. \n",
      " Those models usually first generate [mel-spectrogram autoregressively] from text input and then [synthesize speech] from the [mel-spectrogram] using [vocoder] such as [Griffin-Lim], [WaveNet], Parallel [WaveNet], or [WaveGlow]. \n",
      " [Neural network] based [TTS] has outperformed conventional concatenative and [statistical parametric approaches] in terms of [speech quality]. \n",
      " In current [neural network] based [TTS] systems, [mel-spectrogram] is generated [autoregressively]. \n",
      " Due to the long sequence of the [mel-spectrogram] and the autoregressive nature, those systems face several challenges : \n",
      " â¢ [Slow inference speed] for [mel-spectrogram generation]. \n",
      " Although [CNN] and [Transformer] based [TTS] can speed up the training over [RNN-based models  22 ], all models generate a [mel-spectrogram] conditioned on the previously generated [mel-spectrograms] and suffer from [slow inference speed], given the [mel-spectrogram sequence] is usually with a length of hundreds or thousands. \n",
      " â¢ [Synthesized speech] is usually not robust. \n",
      " Due to error propagation and the wrong [attention alignments] between text and [speech] in the autoregressive generation, the generated [mel-spectrogram] is usually deficient with the problem of words skipping and repeating. \n",
      " â¢ [Synthesized speech] is lack of controllability. \n",
      " Previous [autoregressive models] generate [mel-spectrograms one] by one automatically, without explicitly leveraging the alignments between text and [speech]. \n",
      " As a consequence, it is usually hard to directly control the [voice speed] and [prosody] in the autoregressive generation. \n",
      "\n",
      " Considering the monotonous alignment between text and [speech], to speed up [mel-spectrogram generation], in this work, we propose a novel model, [FastSpeech], which takes a text [(phoneme) sequence] as input and generates [mel-spectrograms] non-[autoregressively]. \n",
      " It adopts a [feed-forward network] based on the [self-attention] in [Transformer] and [1D convolution]. \n",
      " Since a [mel-spectrogram sequence] is much longer than its corresponding [phoneme sequence], in order to solve the problem of length mismatch between the two sequences, [FastSpeech] adopts a length regulator that up-samples the [phoneme sequence] according to the [phoneme duration] (i.e., the number of [mel-spectrograms] that each [phoneme] corresponds to) to match the length of the [mel-spectrogram sequence]. \n",
      " The regulator is built on a [phoneme duration predictor], which predicts the duration of each [phoneme]. \n",
      "\n",
      " Our proposed [FastSpeech] can address the above-mentioned three challenges as follows : \n",
      " â¢ Through parallel [mel-spectrogram generation], [FastSpeech] greatly speeds up the synthesis process. \n",
      " â¢ [Phoneme duration predictor] ensures hard alignments between a [phoneme] and its [mel-spectrograms], which is very different from soft and automatic [attention alignments] in the [autoregressive models]. \n",
      " Thus, [FastSpeech] avoids the issues of error propagation and wrong [attention alignments], consequently reducing the ratio of the skipped words and repeated words. \n",
      " â¢ The length regulator can easily adjust [voice speed] by lengthening or shortening the [phoneme duration] to determine the length of the generated [mel-spectrograms], and can also control part of the [prosody] by adding breaks between adjacent [phonemes]. \n",
      "\n",
      " We conduct experiments on the [LJSpeech dataset] to test [FastSpeech]. \n",
      " The results show that in terms of [speech quality], [FastSpeech] nearly matches the autoregressive [Transformer model]. \n",
      " Furthermore, [FastSpeech] achieves 270x speedup on [mel-spectrogram generation] and 38x speedup on final [speech synthesis] compared with the [autoregressive Transformer TTS model], almost eliminates the problem of word skipping and repeating, and can adjust [voice speed] smoothly. \n",
      " We attach some [audio files] generated by our method in the supplementary materials. \n",
      "\n",
      " Although [ClariNet] is fully [end-to-end], it still first generates [mel-spectrogram autoregressively] and then [synthesizes speech] in one model. \n",
      "\n",
      " Background \n",
      "\n",
      " In this section, we briefly overview the background of this work, including [text to speech], [sequence to sequence learning], and [non-autoregressive sequence generation]. \n",
      "\n",
      " [Text to Speech] â [TTS], which aims to synthesize natural and intelligible [speech] given text, has long been a hot research topic in the field of artificial intelligence. \n",
      " The research on [TTS] has shifted from early [concatenative synthesis  9 ], [statistical parametric synthesis] to [neural network] based [parametric synthesis] and [end-to-end models], and the quality of the [synthesized speech] by [end-to-end models] is close to human parity. \n",
      " [Neural network] based [end-to-end TTS models] usually first convert the text to [acoustic features] (e.g., [mel]-spectrograms) and then transform [mel-spectrograms] into [audio samples]. \n",
      " However, most [neural TTS systems] generate [mel-spectrograms autoregressively], which suffers from [slow inference speed], and [synthesized speech] usually lacks of robustness (word skipping and repeating) and controllability ([voice speed] or [prosody control]). \n",
      " In this work, we propose [FastSpeech] to generate [mel-spectrograms] non-[autoregressively], which sufficiently handles the above problems. \n",
      "\n",
      " [Sequence to Sequence Learning] â [Sequence to sequence learning] is usually built on the [encoder-decoder framework] : The [encoder] takes the source sequence as input and generates a set of representations. \n",
      " After that, the [decoder] estimates the conditional probability of each target element given the source representations and its preceding elements. \n",
      " The [attention mechanism] is further introduced between the [encoder] and [decoder] in order to find which source representations to focus on when predicting the current element, and is an important component for [sequence to sequence learning]. \n",
      " In this work, instead of using the conventional [encoder-attention-decoder framework] for [sequence to sequence learning], we propose a [feed-forward network] to generate a sequence in parallel. \n",
      "\n",
      " [Non-Autoregressive Sequence Generation] â Unlike [autoregressive sequence generation], [non-autoregressive models] generate sequence in parallel, without explicitly depending on the previous elements, which can greatly speed up the inference process. \n",
      " [Non-autoregressive generation] has been studied in some sequence generation tasks such as [neural machine translation  7, 8, 26 ] and [audio synthesis]. \n",
      " Our [FastSpeech] differs from the above works in two aspects : 1) Previous works adopt [non-autoregressive generation] in [neural machine translation] or [audio synthesis] mainly for inference speedup, while [FastSpeech] focuses on both inference speedup and improving the robustness and controllability of the [synthesized speech] in [TTS]. \n",
      " 2) For [TTS], although Parallel [WaveNet], [ClariNet] and [WaveGlow] generate [audio] in parallel, they are conditioned on [mel-spectrograms], which are still generated [autoregressively]. \n",
      " Therefore, they do not address the challenges considered in this work. \n",
      " There is a concurrent work that also generates [mel-spectrogram] in parallel. \n",
      " However, it still adopts the [encoder-decoder framework] with [attention mechanism], which 1) requires 2â¼3x model parameters compared with the teacher model and thus achieves slower inference speedup than [FastSpeech] ; 2) can not totally solve the problems of word skipping and repeating while [FastSpeech] nearly eliminates these issues. \n",
      "\n",
      " [FastSpeech] \n",
      "\n",
      " In this section, we introduce the architecture design of [FastSpeech]. \n",
      " To generate a target [mel-spectrogram sequence] in parallel, we design a novel [feed-forward structure], instead of using the [encoder-attention-decoder] based architecture as adopted by most [sequence to sequence] based autoregressive and [non-autoregressive generation]. \n",
      " The overall [model architecture] of [FastSpeech] is shown in Figure 1. \n",
      " We describe the components in detail in the following subsections. \n",
      "\n",
      " [Feed-Forward Transformer] \n",
      "\n",
      " The architecture for [FastSpeech] is a [feed-forward structure] based on [self-attention] in [Transformer] and [1D convolution]. \n",
      " We call this structure as [Feed-Forward Transformer] (FFT), as shown in Figure 1a. \n",
      " [Feed-Forward] Transformer stacks multiple [FFT blocks] for [phoneme] to [mel-spectrogram transformation], with N blocks on the [phoneme] side, and N blocks on the [mel-spectrogram side], with a length regulator (which will be described in the next subsection) in between to bridge the length gap between the [phoneme] and [mel-spectrogram sequence]. \n",
      " Each [FFT block] consists of a [self-attention] and [1D convolutional network], as shown in Figure 1b. \n",
      " The [self-attention network] consists of a [multi-head attention] to extract the cross-position information. \n",
      " Different from the 2-layer dense network in [Transformer], we use a 2-layer [1D convolutional network] with [ReLU activation]. \n",
      " The motivation is that the adjacent hidden states are more closely related in the character / [phoneme] and [mel-spectrogram sequence] in [speech tasks]. \n",
      " We evaluate the effectiveness of the [1D convolutional network] in the experimental section. \n",
      " Following Transformer, [residual connections], [layer normalization], and [dropout] are added after the [self-attention network] and [1D convolutional network] respectively. \n",
      "\n",
      " Length Regulator \n",
      "\n",
      " The length regulator (Figure 1c) is used to solve the problem of length mismatch between the [phoneme] and [spectrogram sequence] in the [Feed-Forward Transformer], as well as to control the [voice speed] and part of [prosody]. \n",
      " The length of a [phoneme sequence] is usually smaller than that of its [mel-spectrogram sequence], and each [phoneme] corresponds to several [mel-spectrograms]. \n",
      " We refer to the length of the [mel-spectrograms] that corresponds to a [phoneme] as the [phoneme duration] (we will describe how to predict [phoneme duration] in the next subsection). \n",
      " Based on the [phoneme duration] d, the length regulator expands the hidden states of the [phoneme sequence] d times, and then the total length of the hidden states equals the length of the [mel-spectrograms]. \n",
      " Denote the hidden states of the [phoneme sequence] as H pho = [ h1, h2,..., hn ], where n is the length of the sequence. \n",
      " Denote the [phoneme duration sequence] as D = [ d1, d2,..., dn ], where Î£ni=1 di = m and m is the length of the [mel-spectrogram sequence]. \n",
      " We denote the length regulator LR as \n",
      " where Î± is a [hyperparameter] to determine the length of the expanded sequence Hmel, thereby controlling the [voice speed]. \n",
      " For example, given H pho = [ h1, h2, h3, h4 ] and the corresponding [phoneme duration sequence] D = [ 2, 2, 3, 1 ], the expanded sequence Hmel based on Equation 1 becomes [ h1, h1, h2, h2, h3, h3, h3, h4 ] if Î± = 1 (normal speed). \n",
      " When Î± = 1.3 (slow speed) and 0.5 (fast speed), the duration sequences become DÎ±=1.3 = [ 2.6, 2.6, 3.9, 1.3 ] â [ 3, 3, 4, 1 ] and DÎ±=0.5 = [ 1, 1, 1.5, 0.5 ] â [ 1, 1, 2, 1 ], and the expanded sequences become [ h1, h1, h1, h2, h2, h2, h3, h3, h3, h3, h4 ] and [ h1, h2, h3, h3, h4 ] respectively. \n",
      " We can also control the break between words by adjusting the duration of the space characters in the sentence, so as to adjust part of [prosody] of the [synthesized speech]. \n",
      "\n",
      " [Duration Predictor] \n",
      "\n",
      " [Phoneme duration prediction] is important for the length regulator. \n",
      " As shown in Figure 1d, the [duration predictor] consists of a 2-layer [1D convolutional network] with [ReLU activation], each followed by the [layer normalization] and the [dropout layer], and an extra linear layer to output a scalar, which is exactly the predicted [phoneme duration]. \n",
      " Note that this module is stacked on top of the [FFT blocks] on the [phoneme] side and is jointly trained with the [FastSpeech model] to predict the length of [mel-spectrograms] for each [phoneme] with the [mean square error] ([MSE]) loss. \n",
      " We predict the length in the logarithmic domain, which makes them more Gaussian and easier to train. \n",
      " Note that the trained [duration predictor] is only used in the [TTS] inference phase, because we can directly use the [phoneme duration] extracted from an autoregressive teacher model in training (see following discussions). \n",
      "\n",
      " In order to train the [duration predictor], we extract the ground-truth [phoneme duration] from an autoregressive teacher [TTS model], as shown in Figure 1d. \n",
      " We describe the detailed steps as follows : \n",
      "\n",
      " â¢ We first train an autoregressive [encoder-attention-decoder] based [Transformer TTS model] following [ 14 ]. \n",
      " â¢ For each training sequence pair, we extract the [decoder]-to-[encoder attention alignments] from the trained teacher model. \n",
      " There are multiple [attention alignments] due to the [multi-head self-attention  25 ], and not all attention heads demonstrate the diagonal property (the [phoneme] and [mel-spectrogram sequence] are monotonously aligned). \n",
      " We propose a focus P S rate F to measure how an attention head is close to diagonal : F = S 1 s=1 max 1â¤tâ¤T a s, t, where S and T are the lengths of the ground-truth [spectrograms] and [phonemes], a s, t donates the element in the s-th row and t-th column of the attention matrix. \n",
      " We compute the focus rate for each head and choose the head with the largest F as the [attention alignments]. \n",
      " â¢ Finally, we extract the [phoneme duration sequence] D = [ d1, d2,..., dn ] according to the P S duration extractor d i = s=1 [ arg max t a s, t = i ]. \n",
      " That is, the duration of a [phoneme] is the number of [mel-spectrograms] attended to it according to the attention head selected in the above step. \n",
      "\n",
      " Experimental Setup \n",
      "\n",
      " Datasets \n",
      "\n",
      " We conduct experiments on [LJSpeech dataset], which contains 13,100 English [audio clips] and the corresponding text transcripts, with the total [audio] length of approximate 24 hours. \n",
      " We randomly split the dataset into 3 sets : 12500 samples for training, 300 samples for validation and 300 samples for testing. \n",
      " In order to alleviate the mispronunciation problem, we convert the text sequence into the [phoneme sequence] with our internal [grapheme-to-phoneme conversion] tool, following [ 1, 22, 27 ]. \n",
      " For the [speech data], we convert the [raw waveform] into [mel-spectrograms] following [ 22 ]. \n",
      " Our frame size and hop size are set to 1024 and 256, respectively. \n",
      " In order to evaluate the robustness of our proposed [FastSpeech], we also choose 50 sentences which are particularly hard for [TTS system], following the practice in [ 19 ]. \n",
      "\n",
      " Model Configuration \n",
      "\n",
      " [FastSpeech model] â Our [FastSpeech model] consists of 6 [FFT blocks] on both the [phoneme] side and the [mel-spectrogram side]. \n",
      " The size of the [phoneme] vocabulary is 51, including punctuations. \n",
      " The dimension of [phoneme] embeddings, the hidden size of the [self-attention] and [1D convolution] in the [FFT block] are all set to 384. \n",
      " The number of attention heads is set to 2. \n",
      " The kernel sizes of the [1D convolution] in the 2-layer [convolutional network] are both set to 3, with input / output size of 384/1536 for the first layer and 1536/384 in the second layer. \n",
      " The output linear layer converts the 384-dimensional hidden into 80-dimensional [mel-spectrogram]. \n",
      " In our [duration predictor], the kernel sizes of the [1D convolution] are set to 3, with input / output sizes of 384/384 for both layers. \n",
      "\n",
      " [Autoregressive Transformer TTS model] â The [autoregressive Transformer TTS model] serves two purposes in our work : 1) to extract the [phoneme duration] as the target to train the [duration predictor] ; 2) to generate [mel-spectrogram] in the [sequence-level knowledge distillation] (which will be introduced in the next subsection). \n",
      " We refer to for the configurations of this model, which consists of a 6-layer [encoder], a 6-layer [decoder], except that we use [1D convolution network] instead of position-wise FFN. \n",
      " The number of parameters of this teacher model is similar to that of our [FastSpeech model]. \n",
      "\n",
      " Training and Inference \n",
      "\n",
      " We first train the [autoregressive Transformer TTS model] on 4 NVIDIA V100 [GPUs], with batchsize of 16 sentences on each GPU. \n",
      " We use the [Adam optimizer] with Î² 1 = 0.9, Î² 2 = 0.98, Îµ = 10 â9 and follow the same learning rate schedule in [ 25 ]. \n",
      " It takes 80k steps for training until convergence. \n",
      " We feed the text and [speech] pairs in the training set to the model again to obtain the [encoder-decoder attention alignments], which are used to train the [duration predictor]. \n",
      " In addition, we also leverage [sequence-level knowledge distillation  12 ] that has achieved good performance in [non-autoregressive machine translation  7, 8, 26 ] to transfer the knowledge from the teacher model to the student model. \n",
      " For each source text sequence, we generate the [mel-spectrograms] with the [autoregressive Transformer TTS model] and take the source text and the generated [mel-spectrograms] as the paired data for [FastSpeech model] training. \n",
      " We train the [FastSpeech model] together with the [duration predictor]. \n",
      " The optimizer and other [hyper-parameters] for [FastSpeech] are the same as the [autoregressive Transformer TTS model]. \n",
      " The [FastSpeech model] training takes about 80k steps on 4 NVIDIA V100 [GPUs]. \n",
      " In the inference process, the output [mel-spectrograms] of our [FastSpeech model] are transformed into [audio samples] using the pretrained [WaveGlow  20 ] (5). \n",
      "\n",
      " https://github.com/NVIDIA/waveglow \n",
      "\n",
      " Results \n",
      "\n",
      " In this section, we evaluate the performance of [FastSpeech] in terms of [audio quality], inference speedup, robustness, and controllability. \n",
      "\n",
      " [Audio Quality] We conduct the [MOS] ([mean opinion score]) evaluation on the test set to measure the [audio quality]. \n",
      " We keep the text content consistent among different models so as to exclude other interference factors, only examining the [audio quality]. \n",
      " Each [audio] is listened by at least 20 testers, who are all native [English speakers]. \n",
      " We compare the [MOS] of the generated [audio samples] by our \n",
      " [FastSpeech model] with other systems, which include 1) GT, the [ground truth audio] ; 2) GT ([Mel] + [WaveGlow]), where we first convert the [ground truth audio] into [mel-spectrograms], and then convert the [mel-spectrograms] back to [audio] using [WaveGlow] ; 3) [Tacotron] 2 ([Mel] + [WaveGlow]) ; 4) [Transformer TTS] ([Mel] + [WaveGlow]). \n",
      " 5) [Merlin] (WORLD), a popular [parametric TTS system] with WORLD as the [vocoder]. \n",
      " The results are shown in Table 1. \n",
      " It can be seen that our [FastSpeech] can nearly match the quality of the [Transformer TTS model] and [Tacotron] 2. \n",
      "\n",
      " Table : The [MOS] with 95 % confidence intervals. \n",
      "\n",
      " Inference Speedup We evaluate the inference latency of [FastSpeech] compared with the [autoregressive Transformer TTS model], which has similar number of model parameters with [FastSpeech]. \n",
      " We first show the inference speedup for [mel-spectrogram generation] in Table 2. \n",
      " It can be seen that [FastSpeech] speeds up the [mel-spectrogram generation] by 269.40x, compared with the [Transformer TTS model]. \n",
      " We then show the [end-to-end speedup] when using [WaveGlow] as the [vocoder]. \n",
      " It can be seen that [FastSpeech] can still achieve 38.30x speedup for [audio generation]. \n",
      "\n",
      " According to our further comprehensive experiments on our [internal datasets], the [voice quality] of [FastSpeech] can always match that of the teacher model on [multiple languages] and multiple [voices], if we use more unlabeled text for knowledge distillation. \n",
      "\n",
      " Table : The comparison of inference latency with 95 % confidence intervals. \n",
      " The evaluation is conducted on a server with 12 Intel Xeon CPU, 256 GB memory, 1 NVIDIA V100 GPU and [batch size] of 1. \n",
      " The average length of the generated [mel-spectrograms] for the two systems are both about 560. \n",
      "\n",
      " We also visualize the relationship between the inference latency and the length of the predicted [mel-spectrogram sequence] in the test set. \n",
      " Figure 2 shows that the inference latency barely increases with the length of the predicted [mel-spectrogram] for [FastSpeech], while increases largely in [Transformer TTS]. \n",
      " This indicates that the inference speed of our method is not sensitive to the length of generated [audio] due to parallel generation. \n",
      "\n",
      " Robustness The [encoder-decoder attention mechanism] in the [autoregressive model] may cause wrong [attention alignments] between [phoneme] and [mel-spectrogram], resulting in instability with word repeating and word skipping. \n",
      " To evaluate the robustness of [FastSpeech], we select 50 sentences which are particularly hard for [TTS system]. \n",
      " Word error counts are listed in Table 3. \n",
      " It can be seen that [Transformer TTS] is not robust to these hard cases and gets 34 % error rate, while [FastSpeech] can effectively eliminate word repeating and skipping to improve intelligibility. \n",
      "\n",
      " Table : The comparison of robustness between [FastSpeech] and other systems on the 50 particularly hard sentences. \n",
      " Each kind of word error is counted at most once [per] sentence. \n",
      "\n",
      " These cases include single letters, spellings, repeated numbers, and long sentences. We list the cases in the supplementary materials. \n",
      "\n",
      " Length Control As mentioned in Section 3.2, [FastSpeech] can control the [voice speed] as well as part of the [prosody] by adjusting the [phoneme duration], which can not be supported by other [end-to-end TTS systems]. \n",
      " We show the [mel-spectrograms] before and after the length control, and also put the [audio samples] in the supplementary material for reference. \n",
      " [Voice Speed] The generated [mel-spectrograms] with different [voice speeds] by lengthening or shortening the [phoneme duration] are shown in Figure 3. \n",
      " We also attach several [audio samples] in the supplementary material for reference. As demonstrated by the samples, [FastSpeech] can adjust the [voice speed] from 0.5x to 1.5x smoothly, with stable and almost unchanged [pitch]. \n",
      "\n",
      " Breaks Between Words [FastSpeech] can add breaks between adjacent words by lengthening the duration of the space characters in the sentence, which can improve the [prosody] of [voice]. \n",
      " We show an example in Figure 4, where we add breaks in two positions of the sentence to improve the [prosody]. \n",
      "\n",
      " Ablation Study We conduct ablation studies to verify the effectiveness of several components in [FastSpeech], including [1D Convolution] and [sequence-level knowledge distillation]. \n",
      " We conduct CMOS evaluation for these ablation studies. \n",
      "\n",
      " Table : [CMOS] comparison in the ablation studies. \n",
      " \n",
      " [1D Convolution] in [FFT Block] We propose to replace the original fully connected layer (adopted in [Transformer]) with [1D convolution] in [FFT block], as described in Section 3.1. \n",
      " Here we conduct experiments to compare the performance of [1D convolution] to the fully connected layer with similar number of parameters. \n",
      " As shown in Table 4, replacing [1D convolution] with fully connected layer results in -0.113 CMOS, which demonstrates the effectiveness of [1D convolution]. \n",
      " [Sequence-Level Knowledge Distillation] As described in Section 4.3, we leverage [sequence-level knowledge distillation] for [FastSpeech]. \n",
      " We conduct CMOS evaluation to compare the performance of [FastSpeech] with and without [sequence-level knowledge distillation], as shown in Table 4. \n",
      " We find that removing [sequence-level knowledge distillation results] in -0.325 [CMOS], which demonstrates the effectiveness of [sequence-level knowledge distillation]. \n",
      "\n",
      " Conclusions \n",
      "\n",
      " In this work, we have proposed [FastSpeech] : a fast, robust and controllable [neural TTS system]. \n",
      " [FastSpeech] has a novel [feed-forward network] to generate [mel-spectrogram] in parallel, which consists of several key components including [feed-forward Transformer blocks], a length regulator and a [duration predictor]. \n",
      " Experiments on [LJSpeech dataset] demonstrate that our proposed [FastSpeech] can nearly match the [autoregressive Transformer TTS model] in terms of [speech quality], speed up the [mel-spectrogram generation] by 270x and the [end-to-end speech synthesis] by 38x, almost eliminate the problem of word skipping and repeating, and can adjust [voice speed] (0.5x-1.5x) smoothly. \n",
      " For future work, we will continue to improve the quality of the [synthesized speech], and apply [FastSpeech] to [multi-speaker] and low-resource settings. \n",
      " We will also train [FastSpeech] jointly with a [parallel neural vocoder] to make it fully [end-to-end] and parallel. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__== \"__main__\":\n",
    "    \"\"\"Main -> to modify by putting all steps in one fonction\"\"\"\n",
    "    init_data = read_data('tts-lexicon4.tsv')\n",
    "    data = select_data(init_data)\n",
    "    text_dataframe = lemma_posttag('/home/macaire/Bureau/M2_NLP/Terminology/terminology_project/tts-articles/txt/12.txt')\n",
    "#     text_dataframe = lemma_posttag('test2.txt')\n",
    "#     print(text_dataframe.head(60))\n",
    "    data.to_csv(r'terms.txt', header=None, index=None, sep=' ', mode='a')\n",
    "    annotate(data, text_dataframe)\n",
    "    print(construct_annotated_text(text_dataframe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
