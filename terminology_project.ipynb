{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terminology - Project\n",
    "Authors: CÃ©cile MACAIRE & Ludivine ROBERT "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librairies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "spacy_nlp = spacy.load('en_core_web_sm')\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from lexicon\n",
    "def read_data(file):\n",
    "    \"\"\"Read data file with pandas dataframe\"\"\"\n",
    "    return pd.read_csv(file, sep='\\t')\n",
    "\n",
    "def select_data(dataframe):\n",
    "    \"\"\"Lemmatization of lexicon with scapy\"\"\"\n",
    "    terms = dataframe['pilot']\n",
    "    lemma = []\n",
    "    for el in terms:\n",
    "        doc = spacy_nlp(el.lower())\n",
    "        tmp = [token.lemma_ for token in doc]\n",
    "        lemma = [l.replace(' - ', '-') for l in lemma]\n",
    "        lemma.append(' '.join(tmp))\n",
    "    df = pd.DataFrame({'pattern':dataframe['pattern'], 'pilot':dataframe['pilot'], 'lemma':lemma})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text\n",
    "def read_file(file):\n",
    "    with open(file, 'r') as f:\n",
    "        return f.read()\n",
    "        \n",
    "def lemma_posttag(file):\n",
    "    \"\"\"Convert post-tag scapy into corresponding pattern from lexicon\"\"\"\n",
    "    text = read_file(file)\n",
    "    doc_a = spacy_nlp(text)\n",
    "    doc = spacy_nlp(text.lower())\n",
    "    new_pos = []\n",
    "    pos = []\n",
    "    lemma = []\n",
    "    t = []\n",
    "    original = [token.text for token in doc_a]\n",
    "    for token in doc:\n",
    "        t.append(token.text)\n",
    "        lemma.append(token.lemma_)\n",
    "        pos.append(token.pos_)\n",
    "        if token.pos_ == 'NOUN' or token.pos_ == 'PROPN':\n",
    "            new_pos.append('N')\n",
    "        elif token.pos_ == 'VERB':\n",
    "            new_pos.append('V')\n",
    "        elif token.pos_ == 'ADJ':\n",
    "            new_pos.append('A')\n",
    "        elif token.pos_ == 'CCONJ' or token.pos_ == 'SCONJ':\n",
    "            new_pos.append('C')\n",
    "        elif token.pos_ == 'PART' or token.pos_ == 'ADP':\n",
    "            new_pos.append('P')\n",
    "        else:\n",
    "            new_pos.append('')\n",
    "#     print(len(original))\n",
    "#     print(len(lemma))\n",
    "#     print(len(t))\n",
    "#     print(len(pos))\n",
    "#     print(len(new_pos))\n",
    "    frame = pd.DataFrame({'tokens': original,'tokens_lower':t, 'lemma':lemma, 'pos':pos, 'pattern':new_pos})\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 4,
=======
   "execution_count": 27,
>>>>>>> 277df5128ba97fff0bc6433c27da671025a97b13
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_adj = ['acoustic', 'accented', 'artificial', 'attentional', 'autoregressive', 'bidirectional', 'bilingual', 'cross-lingual', 'fluent',\n",
    "            'gated', 'generated', 'intelligible', 'labelled', 'phonetic', 'monolingual', 'multilingual', 'multispeaker', 'neural',\n",
    "            'substantial', 'supervised', 'training', 'unlabelled', 'unsupervised']\n",
    "def rules(terms_dataframe, text_dataframe):\n",
    "    \"\"\"Define rules from terms according to their pattern\"\"\"\n",
    "    new_terms = []\n",
    "    for terms in terms_dataframe['lemma']:\n",
    "        # Get the same structure of terms as in text dataframe\n",
    "        tmp = ' '.join(terms.split('-'))\n",
    "        new_terms.append(tmp.split(' '))\n",
    "    for i, token in enumerate(text_dataframe['lemma']):\n",
    "        for j, t in enumerate(new_terms):\n",
    "            # Case 1: term of size 3 seperated by dashes (ex: text-to-speech) and followed by 1, 2 Nouns or 1 Adj and 1 Noun is a term \n",
    "            if len(t) == 3 and len(text_dataframe['lemma']) >= i+5:\n",
    "                if token == t[0] and text_dataframe['lemma'][i+1] == '-' and (text_dataframe['lemma'][i+2] == 'to' or text_dataframe['lemma'][i+2] == 'of' or text_dataframe['lemma'][i+2] == 'by' or text_dataframe['pattern'][i+2] == 'N') and text_dataframe['lemma'][i+3] == '-' and text_dataframe['lemma'][i+4] == t[2]:\n",
    "                    # followed by 2 nouns (ex: text-to-speech modal synthesis)\n",
    "                    if (text_dataframe['pattern'][i+5] == 'N' or text_dataframe['pattern'][i+4] == 'A') and text_dataframe['pattern'][i+6] == 'N':\n",
    "                        text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                        text_dataframe['tokens'][i+6] = text_dataframe['tokens'][i+6]+']'                        \n",
    "                    elif text_dataframe['pattern'][i+5] == 'N':\n",
    "                        # followed by 1 noun (ex: text-to-speech system)\n",
    "                        text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                        text_dataframe['tokens'][i+5] = text_dataframe['tokens'][i+5]+']'\n",
    "                    else:\n",
    "                        text_dataframe['tokens'][i] = '[' + text_dataframe['tokens'][i]\n",
    "                        text_dataframe['tokens'][i + 4] = text_dataframe['tokens'][i + 4] + ']'\n",
    "            # Case 2: term of size 2 separated by dashes (ex: encoder-decoder) and followed by 0,1,2 or 3 nouns is a term\n",
    "            elif len(t) >= 2 and len(text_dataframe['lemma']) >= i+3 and i != 0:\n",
    "                if token == 'front' and text_dataframe['lemma'][i+1] == '-' and text_dataframe['lemma'][i+2] == 'end':\n",
    "                    if text_dataframe['pattern'][i-1] == 'N':\n",
    "                        text_dataframe['tokens'][i-1] = '['+text_dataframe['tokens'][i-1]\n",
    "                        text_dataframe['tokens'][i+2] = text_dataframe['tokens'][i+2]+']'\n",
    "                if token == t[0] and text_dataframe['lemma'][i+1] == '-' and text_dataframe['lemma'][i+2] == t[1]:\n",
    "                    # followed by 3 nouns (ex: HMM-based generation synthesis approach)\n",
    "                    if text_dataframe['pattern'][i+3] == 'N' and text_dataframe['pattern'][i+4] == 'N' and text_dataframe['pattern'][i+5] == 'N':\n",
    "                        text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                        text_dataframe['tokens'][i+5] = text_dataframe['tokens'][i+5]+']'\n",
    "                    # followed by 2 nouns (ex: HMM-based generation synthesis)\n",
    "                    elif (text_dataframe['pattern'][i+3] == 'N' or text_dataframe['pattern'][i+3] == 'A' or text_dataframe['pattern'][i + 3] == 'V') and text_dataframe['pattern'][i+4] == 'N':\n",
    "                        text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                        text_dataframe['tokens'][i+4] = text_dataframe['tokens'][i+4]+']'\n",
    "                    # followed by 1 noun (ex: cross-lingual adaptation)\n",
    "                    elif text_dataframe['pattern'][i+3] == 'N':\n",
    "                            text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                            text_dataframe['tokens'][i+3] = text_dataframe['tokens'][i+3]+']'\n",
    "                    # followed by nothing (ex: mel-spectrogram)\n",
    "                    else:\n",
    "                        text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                        text_dataframe['tokens'][i+2] = text_dataframe['tokens'][i+2]+']'\n",
    "        if (token == 'data' or token == 'voice' or token == 'datum' or token == 'speaker' or token == 'dataset' or token == 'database' or token == 'feature' or token == 'corpus' or token == 'language') and i != 0 and len(text_dataframe['lemma']) >= i+1:\n",
    "            if text_dataframe['pattern'][i-1] == 'N' or text_dataframe['pattern'][i-1] == 'A':\n",
    "                text_dataframe['tokens'][i-1] = '['+text_dataframe['tokens'][i-1]\n",
    "                text_dataframe['tokens'][i] = text_dataframe['tokens'][i]+']'\n",
    "            elif text_dataframe['pattern'][i+1] == 'N':\n",
    "                text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                text_dataframe['tokens'][i+1] = text_dataframe['tokens'][i+1]+']'\n",
    "        if i != 0:\n",
    "            if text_dataframe['lemma'][i-1] in rule_adj and '[' in text_dataframe['tokens'][i]:\n",
    "                text_dataframe['tokens'][i-1] = '['+text_dataframe['tokens'][i-1]+']'\n",
    "            elif i >= 3 and text_dataframe['lemma'][i-1] in rule_adj and text_dataframe['lemma'][i-3] == 'non' and '[' in text_dataframe['tokens'][i]:\n",
    "                    text_dataframe['tokens'][i-3] = '['+text_dataframe['tokens'][i-3]\n",
    "                    text_dataframe['tokens'][i-3] = text_dataframe['tokens'][i-1] + ']'"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 5,
=======
   "execution_count": 28,
>>>>>>> 277df5128ba97fff0bc6433c27da671025a97b13
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_4 = ['accent', 'accuracy', 'activation', 'adaptation', 'algorithm', 'alignment', 'approach', \n",
    "          'architecture', 'attribute', 'boundary', 'cell', 'class', 'classifier', 'cluster', 'component', \n",
    "          'concatenation', 'content', 'contour', 'control', 'conversion', 'coverage', 'detection', \n",
    "          'detection', 'device', 'dictionary', 'embedding', 'encoding', 'engineering', 'entry', 'error', \n",
    "          'evaluation', 'experiment', 'expertise', 'file', 'filter', 'form', 'framework', 'function', \n",
    "          'generation', 'identification', 'implementation', 'improvement', 'inference', 'input', 'kernel', 'layer', 'learning', \n",
    "          'length', 'location', 'mapping', 'method', 'model', 'module', 'naturalness', 'network', \n",
    "          'nonlinearity', 'optimization', 'output', 'pair', 'parameter', 'pipeline', 'posterior', 'prediction', \n",
    "          'process', 'processing', 'quality', 'realization', 'recognition', 'representation', 'research', \n",
    "          'result', 'sample', 'score', 'sequence', 'set', 'setting', 'signal', 'string', 'study', 'symbol', \n",
    "          'synthesis', 'synthesizer', 'system', 'task', 'technique', 'technique', 'technology', 'token', 'tool', \n",
    "          'toolkit', 'training', 'transcription', 'transfer', 'transform', 'translation', 'value', 'generator',\n",
    "         'corpora', 'tilt', 'knowledge', 'category', 'track', 'tagger', 'unit', 'label']\n",
    "def annotate(terms_dataframe, text_dataframe):\n",
    "    \"\"\"Annotate the terms of the text thanks to list of terms + applied rules\"\"\"\n",
    "    rules(terms_dataframe, text_dataframe)  # apply rules\n",
    "    for i, token in enumerate(text_dataframe['lemma']):\n",
    "        for term in terms_dataframe['lemma']:\n",
    "            term = term.split(' ')\n",
    "            # Case 1: if terms of length 4, we check if each word from text corresponds to each word in the term\n",
    "            if len(term) == 4:\n",
    "                term_1 = term[0]\n",
    "                if token == term_1 and len(text_dataframe['lemma']) >= i+4:\n",
    "                    if text_dataframe['lemma'][i+1] == term[1] and text_dataframe['lemma'][i+2] == term[2] and text_dataframe['lemma'][i+3] == term[3]:\n",
    "                        if text_dataframe['lemma'][i+4] in rule_4:\n",
    "                            text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                            text_dataframe['tokens'][i+4] = text_dataframe['tokens'][i+4]+']'\n",
    "                        else:\n",
    "                            text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                            text_dataframe['tokens'][i+3] = text_dataframe['tokens'][i+3]+']'\n",
    "            # Case 2: terms of length 3\n",
    "            elif len(term) == 3:\n",
    "                term_1 = term[0]\n",
    "                if token == term_1 and len(text_dataframe['lemma']) > i+3:\n",
    "                    if text_dataframe['lemma'][i+1] == term[1] and text_dataframe['lemma'][i+2] == term[2]:\n",
    "                        if text_dataframe['lemma'][i+3] in rule_4:\n",
    "                            text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                            text_dataframe['tokens'][i+3] = text_dataframe['tokens'][i+3]+']'\n",
    "                        else:\n",
    "                            text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                            text_dataframe['tokens'][i+2] = text_dataframe['tokens'][i+2]+']'\n",
    "            # Case 3: terms of length 2\n",
    "            elif len(term) == 2:\n",
    "                if token == term[0] and len(text_dataframe['lemma']) > i+2:\n",
    "                    if text_dataframe['lemma'][i+1] == term[1]:\n",
    "                        if text_dataframe['lemma'][i+2] in rule_4:\n",
    "                            text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                            text_dataframe['tokens'][i+2] = text_dataframe['tokens'][i+2]+']'\n",
    "                        else:\n",
    "                            text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                            text_dataframe['tokens'][i+1] = text_dataframe['tokens'][i+1]+']'\n",
    "            # Case 4: term of length 1\n",
    "            elif token == term[0] and i > 1 and text_dataframe['lemma'][i-1] == 'of' and text_dataframe['lemma'][i-2] == 'sequence':\n",
    "                text_dataframe['tokens'][i-2] = '['+text_dataframe['tokens'][i-2]\n",
    "                text_dataframe['tokens'][i] = text_dataframe['tokens'][i]+']'\n",
    "            elif token == term[0] and len(term) == 1 and len(text_dataframe['lemma']) >= i+2 and text_dataframe['lemma'][i+1] == ')':\n",
    "                if text_dataframe['lemma'][i+2] in rule_4:\n",
    "                    text_dataframe['tokens'][i-1] = '['+text_dataframe['tokens'][i-1]\n",
    "                    text_dataframe['tokens'][i+2] = text_dataframe['tokens'][i+2]+']'\n",
    "                else:\n",
    "                    text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]+']'\n",
    "            elif token == term[0] and len(term) == 1 and len(text_dataframe['lemma']) >= i+1:\n",
    "                if text_dataframe['lemma'][i+1] in rule_4:\n",
    "                    text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                    text_dataframe['tokens'][i+1] = text_dataframe['tokens'][i+1]+']'\n",
    "                else:\n",
    "                    text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]+']'\n",
    "        if i != 0:\n",
    "            if text_dataframe['lemma'][i-1] in rule_adj and '[' in text_dataframe['tokens'][i]:\n",
    "                text_dataframe['tokens'][i-1] = '['+text_dataframe['tokens'][i-1]+']'\n",
    "            elif i >= 3 and text_dataframe['lemma'][i-1] in rule_adj and text_dataframe['lemma'][i-3] == 'non' and '[' in text_dataframe['tokens'][i]:\n",
    "                text_dataframe['tokens'][i-3] = '['+text_dataframe['tokens'][i-3]\n",
    "                text_dataframe['tokens'][i-3] = text_dataframe['tokens'][i-1] + ']'\n",
    "    return text_dataframe"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 6,
=======
   "execution_count": 29,
>>>>>>> 277df5128ba97fff0bc6433c27da671025a97b13
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_annotated_text(text_dataframe):\n",
    "    \"\"\"Return the text from the annotated text dataframe with the correct annotation of brackets\"\"\"\n",
    "    content = ' '.join(text_dataframe['tokens'].to_list())\n",
    "    compt = 0\n",
    "    compt2 = 0\n",
    "    string = ''\n",
    "    for i in content:\n",
    "        if i == '[':\n",
    "            if compt == 0:\n",
    "                compt += 1\n",
    "                string += i\n",
    "            elif compt >= 1:\n",
    "                compt += 1\n",
    "        elif i == ']':\n",
    "            if compt-1 != compt2:\n",
    "                compt2 += 1\n",
    "            else:\n",
    "                string += i\n",
    "                compt = 0\n",
    "                compt2 = 0\n",
    "        else:\n",
    "            string += i\n",
    "    string2 = ''\n",
    "    string = string.replace('] [', ' ')\n",
    "    string = string.replace(' .', '.')\n",
    "    string = string.replace(' â', 'â')\n",
    "    string = string.replace(' ,', ',')\n",
    "    string = string.replace(' - ', '-')\n",
    "    string = string.replace('( ', '(')\n",
    "    string = string.replace(' )', ')')\n",
    "    string = string.replace(']-[', '-')\n",
    "    string = string.replace('.]', '].')\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "arrays must all be same length",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-7926631d90f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0minit_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'lexicon.tsv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtext_dataframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlemma_posttag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tts-articles/txt/3.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m#     text_dataframe = lemma_posttag('test2.txt')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#     print(text_dataframe.head(60))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-cfd31946bc10>\u001b[0m in \u001b[0;36mlemma_posttag\u001b[0;34m(file)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m#     print(len(pos))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m#     print(len(new_pos))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'tokens'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0moriginal\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'tokens_lower'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lemma'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlemma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pos'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pattern'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnew_pos\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    433\u001b[0m             )\n\u001b[1;32m    434\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m             \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36minit_dict\u001b[0;34m(data, index, columns, dtype)\u001b[0m\n\u001b[1;32m    252\u001b[0m             \u001b[0marr\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_datetime64tz_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         ]\n\u001b[0;32m--> 254\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, arr_names, index, columns, dtype)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;31m# figure out the index, if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mextract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    363\u001b[0m             \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"arrays must all be same length\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhave_dicts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: arrays must all be same length"
=======
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tagging_IOB(string):\n",
    "    \"\"\"Tagging the terms into IOB and POS\"\"\"\n",
    "    is_term = False \n",
    "    string_tag = string.split(' ')\n",
    "    annotated = []\n",
    "    for k,l in enumerate(string_tag):\n",
    "        if '[' in l and ']' in l:\n",
    "            for i,j in enumerate(l):\n",
    "                if l[i] == ']':\n",
    "                    annotated.append(l[:i-1]+ ' (B)]'+l[i+1:])\n",
    "        else:\n",
    "            if '[' in l and is_term is False:\n",
    "                annotated.append(l+ ' (B)')\n",
    "                is_term = True\n",
    "            elif is_term and ']' not in l:\n",
    "                annotated.append(l+ ' (I)')\n",
    "            elif is_term and ']' in l:\n",
    "                for m,n in enumerate(l):\n",
    "                    if l[m] == ']':\n",
    "                        annotated.append(l[:m]+ ' (I)]'+l[m+1:])\n",
    "                is_term = False\n",
    "            else:\n",
    "                annotated.append(l)\n",
    "    iob_string = ' '.join(annotated)\n",
    "    print(iob_string)\n",
    "    return iob_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning to Speak Fluently in a [Foreign (B) Language (I)] : [Multilingual (B) Speech (I) Synthesis (I)] and cross [- (B) language (I) voice (I)] cloning \n",
      "\n",
      "\n",
      " Abstract \n",
      " We present a multispeaker, [multilingual (B) text-to-speech (I) (TTS) (I) synthesis (I) model (I)] based on [Tacotro (B)] that is able to produce [high (B) quality (I) speech (I)] in [multiple (B) languages (I)]. \n",
      " Moreover, the model is able to transfer voices across languages, e.g. synthesize fluent Spanish [speec (B)] using an [English (B) speaker (I)]âs voice, without training on any bilingual or parallel examples. \n",
      " Such transfer works across distantly [related (B) languages (I)], e.g. English and Mandarin. \n",
      " Critical to achieving this result are : using a [phonemic (B) input (I) representation (I)] to encourage sharing of model capacity across languages, and incorporating an [adversarial (B) loss (I)] term to encourage the model to disentangle its representation of [speaker (B) identity (I)] (which is perfectly correlated with language in the [training (B) data (I)]) from the [speech (B) content (I)]. \n",
      " Further scaling up the model by training on [multiple (B) speakers (I)] of each language, and incorporating an autoencoding input to help stabilize attention during training, results in a model which can be used to consistently synthesize [intelligible (B) speech (I)] for [training (B) speakers (I)] in all languages seen during training, and in native or foreign accents. \n",
      " Index Terms : [speech (B) synthesis (I)], [end-to-en (B)], [adversarial (B) loss (I)] \n",
      "\n",
      " Introduction \n",
      " Recent [end-to-end (B) neural (I) TTS (I) models (I)] have been extended to enable control of [speaker (B) identity (I)] as well as [unlabelled (B) speech (I) attributes (I)], e.g. [prosod (B)], by conditioning synthesis on latent representations in addition to text. \n",
      " Extending such models to support multiple, [unrelated (B) languages (I)] is nontrivial when using [language-dependent (B) input (I) representations (I)] or model components, especially when the amount of [training (B) data (I) per (I)] language is imbalanced. \n",
      " For example, there is no overlap in the text representation between languages like Mandarin and English. Furthermore, recordings from [bilingual (B) speakers (I)] are expensive to collect. It is therefore most common for each speaker in the training set to speak only one language, so [speaker (B) identity (I)] is perfectly correlated with language. \n",
      " This makes it difficult to transfer voices across [different (B) languages (I)], a [desirable (B) feature (I)] when the number of available [training (B) voices (I)] for a [particular (B) language (I)] is small. \n",
      " Moreover, for languages with borrowed or shared words, such as proper nouns in Spanish (ES) and English (EN), pronunciations of the same text might be different. \n",
      " This adds more ambiguity when a naively trained model sometimes generates [accented (B) speech (I)] for a [particular (B) speaker (I)]. \n",
      " Zen et al. proposed a speaker and [language (B) factorization (I)] for [HMM-based (B) parametric (I) TTS (I) system (I)], aiming to transfer a voice from one language to others. \n",
      " proposed a multilingual parametric [neural (B) TTS (I) system (I)], which used a unified [input (B) representation (I)] and shared parameters across languages, however the   voices used for each language were disjoint. \n",
      " described a similar bilingual Chinese and English [neural (B) TTS (I) system (I)] trained on [speec (B)] from a [bilingual (B) speaker (I)], allowing it to [synthesize (B) speech (I)] in both languages using the [same (B) voice (I)]. \n",
      " studied learning pronunciation from a [bilingual (B) TTS (I) model (I)]. \n",
      " Most recently, presented a [multilingual (B) neural (I) TTS (I) model (I)] which supports voice cloning across English, Spanish, and German. \n",
      "\n",
      "                                         \n",
      " Figure : Overview of the components of the proposed model. \n",
      " Dashed lines denote sampling via reparameterization   during training. \n",
      " The prior mean is always use during inference. \n",
      "                                   \n",
      " It used [language-specific (B) text (I)] and [speaker (B) encoders (I)], and incorporated a secondary fine-tuning step to optimize a [speaker (B) identity (I)]-preserving loss, ensuring that the model could output a [consistent (B) voice (I)] regardless of language. \n",
      " We also note that the sound quality is not on par with recent [neural (B) TTS (I) systems (I)], potentially because of its use of the [WORLD (B) vocoder (I)] for [waveform (B) synthesis (I)]. \n",
      " Our work is most similar to, which describes a [multilingual (B) TTS (I) model (I)] based on [Tacotron (B) 2 (I)] which uses a Unicode encoding â [byt (B)] â [input (B) representation (I)] to train a model on one speaker of each of English, Spanish, and Mandarin. \n",
      " In this paper, we evaluate different [input (B) representations (I)], scale up the number of [training (B) speakers (I)] for each language, and extend the model to support [cross-lingual (B) voice (I)] cloning. \n",
      " The model is trained in a single stage, with no [language-specific (B) components (I)], and obtains naturalness on par with baseline monolingual models. \n",
      " Our contributions include : \n",
      " Evaluating the effect of using different text [input (B) representations (I)] in a [multilingual (B) TTS (I) model (I)]. \n",
      " Introducing a [pe (B)]-input [token (B) speaker-adversarial (I) loss (I)] to enable [cross-lingual (B) voice (I) transfer (I)] when only one [training (B) speaker (I)] is available for each language. \n",
      " Incorporating an [explicit (B) language (I) embedding (I)] to the input, which enables moderate control of [speech (B) accent (I)], independent of [speaker (B) identity (I)], when the [training (B) data (I)] contains [multiple (B) speakers (I) per (I)] language. \n",
      " We evaluate the contribution of each component, and demonstrate the proposed modelâs ability to disentangle speakers from languages and consistently synthesize [high (B) quality (I) speech (I)] for all speakers, despite the perfect correlation to the [original (B) language (I)] in the [training (B) data (I)]. \n",
      "                                         \n",
      " Model Structure \n",
      " We base our [multilingual (B) TTS (I) model (I)] on [Tacotron (B) 2 (I)], which uses an [attention-based (B) sequence-to-sequence (I) model (I)] to generate a sequence of log-[mel (B) spectrogram (I)] frames based on an input text sequence. \n",
      " The architecture is illustrated in Figure. \n",
      " It augments the base [Tacotron (B) 2 (I) model (I)] with [additional (B) speaker (I)] and, optionally, [language (B) embedding (I)] inputs (bottom right), an adversarially-trained [speaker (B) classifier (I)] (top right), and a [variational (B) autoencoder (I)]-style [residual (B) encoder (I)] (top left) which conditions the [decode (B)] on a latent embedding computed from the [target (B) spectrogram (I)] during training (top left). \n",
      " Finally, similar to [Tacotron (B) 2 (I)], we separately train a [WaveRNN (B) neural (I) vocoder (I)]. \n",
      "\n",
      " [Input (B) representations (I)] \n",
      " [End-to-end (B) TTS (I) models (I)] have typically used character or [phonem (B)]   [input (B) representations (I)], or hybrids between them. \n",
      " Recently, proposed using inputs derived from the UTF-8 [byt (B)] encoding in [multilingual (B) settings (I)]. \n",
      " We evaluate the effects of using these representations for [multilingual (B) TTS (I)]. \n",
      "\n",
      " Characters / [Grapheme (B)] \n",
      " Embeddings corresponding to each character or [graphem (B)] are the default inputs for [end-to-end (B) TTS (I) models (I)], requiring the model to implicitly learn how to pronounce input words (i.e. [grapheme-to-phoneme (B) conversion (I)]) as part of the synthesis task. \n",
      " Extending a [grapheme-based (B) input (I) vocabulary (I)] to a [multilingual (B) setting (I)] is straightforward, by simply concatenating \n",
      " [grapheme (B) sets (I)] in the [training (B) corpus (I)] for each language. \n",
      " This can grow quickly for languages with large alphabets, e.g. our Mandarin vocabulary contains over 4.5k tokens. \n",
      " We simply concatenate all [grapheme (B)] appearing in the [training (B) corpus (I)], leading to a total of 4,619 tokens. \n",
      " Equivalent [grapheme (B)] are shared across languages. \n",
      " During inference all previously unseen characters are mapped to a special out-of-vocabulary (OOV) symbol. \n",
      "\n",
      " UTF-8 Encoded [Byte (B)] \n",
      " Following   we experiment with an [input (B) representation (I)] based on the UTF-8 text encoding, which uses 256 possible values as each input token where the mapping from [grapheme (B)] to [byte (B)] is [language-dependen (B)]. \n",
      " For languages with single-[byt (B)] characters (e.g., English), this representation is equivalent to the [grapheme (B) representation (I)]. \n",
      " However, for languages with multi-[byt (B)] characters (such as Mandarin) the [TTS (B) model (I)] must learn to attend to a consistent [sequence (B) of (I) bytes (I)] to correctly generate the corresponding [speec (B)]. \n",
      " On the other hand, using a UTF-8 [byte (B) representation (I)] may promote sharing of representations between languages due to the smaller number of input tokens. \n",
      "\n",
      " [Phoneme (B)] \n",
      " Using [phonem (B)] inputs simplifies the [TTS (B) task (I)], as the model no longer needs to learn complicated pronunciation rules for languages such as English. \n",
      " Similar to our [grapheme-based (B) model (I)], equivalent [phoneme (B)] are shared across languages. We concatenate all possible [phoneme (B) symbols (I)], for a total of 88 tokens. \n",
      " To support Mandarin, we include tone information by learning [phoneme-independent (B) embeddings (I)] for each of the 4 possible tones, and broadcast each tone embedding to all [phoneme (B) embeddings (I)] inside the corresponding syllable. \n",
      " For English and Spanish, tone embeddings are replaced by stress embeddings which include primary and secondary stresses. A special symbol is used when there is no tone or stress. \n",
      "\n",
      " [Residual (B) encoder (I)] \n",
      " Following, we augment the [TTS (B) model (I)] by incorporating a [variational (B) autoencoder (I)]-like [residual (B) encoder (I)] which encodes the latent factors in the [training (B) audio (I)], e.g. [prosod (B)] or background noise, which is not well-explained by the conditioning inputs : the text representation, speaker, and [language (B) embeddings (I)]. \n",
      " We follow the structure from, except we use a standard single [Gaussian (B) prior (I) distribution (I)] and reduce the latent dimension to 16. \n",
      " In our experiments, we observe that feeding in the prior mean (all zeros) during inference, significantly improves stability of [cross-lingual (B) speaker (I) transfer (I)] and leads to improved naturalness as shown by [MOS (B) evaluations (I)] in Section. \n",
      "\n",
      " Adversarial training \n",
      " One of the challenges for [multilingual (B) TTS (I)] is [data (B) sparsity (I)], where some languages may only have [training (B) data (I)] for a [few (B) speakers (I)]. \n",
      " In the extreme case where there is only one speaker [pe (B)] language in the [training (B) data (I)], the [speaker (B) identity (I)] is essentially the same as the language i d. \n",
      " To encourage the model to learn disentangled representations of the text and [speaker (B) identity (I)], we proactively discourage the text encoding ts from also capturing [speaker (B) information (I)]. \n",
      " We employ domain adversarial training to encourage ti to encode text in a speaker-independent manner by introducing a [speaker (B) classifier (I)] based on the text encoding and a [gradient (B) reversal (I) layer (I)]. \n",
      " Note that the [speaker (B) classifier (I)] is optimized with a different objective than the rest of the model : Lspeaker (Ïs ; ti) = N \n",
      " Ã log p(si | ti), where si is the [speaker (B) label (I)] i and Ïs are the parameters for [speaker (B) classifier (I)]. \n",
      " To train the full model, we insert a [gradient (B) reversal (I) layer (I)]   prior to this [speaker (B) classifier (I)], which scales the gradient by âÎ». \n",
      " Following, we also explore inserting another adversarial layer on top of the [variational (B) autoencoder (I)] to encourage it to learn speaker-independent representations. \n",
      " However, we found that this layer has no effect after decreasing the latent space dimension. \n",
      "                  \n",
      " We impose this [adversarial (B) loss (I)] separately on each element of the encoded text sequence, in order to encourage the model to learn a speaker and [language-independent (B) text (I)] embedding space. \n",
      " In contrast to, which disentangled [speaker (B) identity (I)] from background noise, some input tokens are highly [language-dependen (B)] which can lead to unstable adversarial classifier gradients. \n",
      " We address this by clipping gradients computed at the [reversal (B) layer (I)] to limit the impact of such outliers. \n",
      "\n",
      " Experiments \n",
      " We train models using a [proprietary (B) dataset (I)] composed of [high (B) quality (I) speech (I)] in three languages : 385 hours of English (EN) from 84 [professional (B) voice (I)] actors with accents from the United States, Great Britain, Australia, and Singapore ;   97 hours of Spanish (ES) from 3 [female (B) speakers (I)] include Castilian and US Spanish ; 68 hours of Mandarin (CN) from 5 speakers. \n",
      "\n",
      " Model and training setup \n",
      " The [synthesizer (B) network (I)] uses the [Tacotron (B) 2 (I) architecture (I)], with additional inputs consisting of learned speaker (64-dim) and [language (B) embeddings (I)] (3-dim), concatenated and passed to the [decode (B)] at each step. \n",
      " The generated [speec (B)] is represented as a sequence of 128-dim log-[mel (B) spectrogram (I)] frames, computed from 50ms windows shifted by 12.5ms. \n",
      " The [variational (B) residual (I) encoder (I) architecture (I)] closely follows the attribute [encode (B)] in. \n",
      " It maps a variable length [mel (B) spectrogram (I)] to two [vector (B)] parameterizing the mean and log variance of the [Gaussian (B) posterior (I)]. \n",
      " The [speaker (B) classifiers (I)] are fully-connected networks with one 256 unit hidden layer followed by a [softma (B)] predicting the [speaker (B) identity (I)]. \n",
      " The synthesizer and [speaker (B) classifier (I)] are trained with weight 1.0 and 0.02 respectively. \n",
      " As described in the previous section we apply gradient clipping with factor 0.5 to the [gradient (B) reversal (I) layer (I)]. \n",
      "\n",
      " Table : [Speaker (B) similarity (I) Mean (I) Opinion (I) Score (I)] ([MO (B)]) comparing [ground (B) truth (I) audio (I)] from speakers of [different (B) languages (I)]. \n",
      " Raters are [native (B) speakers (I)] of the [target (B) language (I)]. \n",
      "\n",
      " Table : Naturalness [MO (B)] of monolingual and [multilingual (B) models (I) synthesizing (I) speech (I)] of in [different (B) languages (I)]. \n",
      "\n",
      " Table : Naturalness and [speaker (B) similarity (I) MOS (I)] of cross [- (B) language (I) voice (I)] cloning of an EN [source (B) speaker (I)]. Models which use different [input (B) representations (I)] are compared, with and without the speaker-[adversarial (B) loss (I)]. fail : raters complained that too many utterances were spoken in the [wrong (B) language (I)]. \n",
      "                                                                           \n",
      "\n",
      " The entire model is trained jointly with a [batch (B) size (I)] of 256, using the [Adam (B) optimizer (I)] configured with an initial learning rate of 10â3, and an exponential decay that halves the learning rate every 12.5k steps, starting at 50k steps. \n",
      " [Waveform (B)] are synthesized using a [WaveRNN (B) vocoder (I)] which generates 16-bit signals sampled at 24 kHz conditioned on [spectrogram (B)] predicted by the [TTS (B) model (I)]. \n",
      " We synthesize 100 samples [per (B) model (I)], and have each one rated by 6 raters. \n",
      "\n",
      " Evaluation \n",
      " To evaluate [synthesized (B) speech (I)], we rely on crowdsourced [Mean (B) Opinion (I) Score (I) (MOS) (I) evaluations (I)] of [speech (B) naturalness (I)] via subjective listening tests. \n",
      " Ratings follow the Absolute Category Rating scale, with scores from 1 to 5 in 0.5 point increments. \n",
      " For cross [- (B) language (I) voice (I)] cloning, we also evaluate whether the [synthesized (B) speech (I)] resembles the identity of the [reference (B) speaker (I)] by pairing each synthesized utterance with a reference utterance from the [same (B) speaker (I)] for subjective [MOS (B) evaluation (I)] of [speaker (B) similarity (I)], as in. \n",
      " Although rater instructions explicitly asked for the content to be ignored, note that this similarity evaluation is more challenging than the one in because the reference and target examples are spoken in [different (B) languages (I)], and raters are not bilingual. \n",
      " We found that low fidelity [audi (B)] tended to result in high variance similarity [MO (B)] so we always use [WaveRNN (B) outputs (I)]. \n",
      " For each language, we chose one speaker to use for similarity tests. \n",
      " As shown in Table, the [EN (B) speaker (I)] is found to be dissimilar to the ES and [CN (B) speakers (I)] ([MO (B)] below 2.0), while the ES and [CN (B) speakers (I)] are slightly similar ([MO (B)] around 2.0). \n",
      " The [CN (B) speaker (I)] has more natural variability compared to EN and ES, leading to a lower self similarity. \n",
      " The scores are consistent when EN and CN raters evaluate the same EN and CN test set. \n",
      " The observation is consistent with : raters are able to discriminate between speakers across languages. \n",
      " However, when rating [synthetic (B) speech (I)], we observed that English speaking raters often considered â heavy accented â synthetic CN [speec (B)] to sound more similar to the target EN speaker, compared to more [fluent (B) speech (I)] from the [same (B) speaker (I)]. \n",
      " This indicates that accent and [speaker (B) identity (I)] are not fully disentangled. \n",
      " We encourage readers to listen to samples on the companion webpage. \n",
      "\n",
      " Comparing [input (B) representations (I)] \n",
      " We first build and evaluate models comparing the performance of different text [input (B) representations (I)]. \n",
      " For all three languages, [byt (B)]-based models always use a 256-dim [softmax (B) output (I)]. \n",
      " Monolingual character and [phoneme (B) models (I)] each use a different input vocabulary corresponding to the [training (B) language (I)]. \n",
      "\n",
      " Some raters gave low fidelity [audi (B)] lower scores, treating \" blurriness \" as a property of the speaker. Others gave higher scores because they recognized such [audi (B)] as synthetic and had lower expectations. \n",
      " ttp://google.github.io/tacotron/publications/multilingual \n",
      "\n",
      " Table compares monolingual and [multilingual (B) model (I)] performance using different [input (B) representations (I)]. \n",
      " For Mandarin, the [phoneme-based (B) model (I)] performs significantly better than char or [byt (B)]-based variants due to rare and OOV words. \n",
      " Compared to the monolingual system, [multilingual (B) phoneme-based (I) systems (I)] have similar performance on ES and CN but are slightly worse on EN. \n",
      " CN has a larger gap to ground truth (top) due to unseen word segmentation (for simplicity, we did nât add word boundary during training). \n",
      " The multispeaker model (bottom) performs about the same as the [single (B) speaker (I) per-language (I) variant (I)] (middle). \n",
      " Overall, when using [phoneme (B) inputs (I)] all the languages obtain [MOS (B) scores (I)] above 4.0. \n",
      "\n",
      " Cross [- (B) language (I) voice (I)] cloning \n",
      " We evaluate how well the multispeaker models can be used to clone a speakerâs voice into a [new (B) language (I)] by simply passing in [speaker (B) embeddings (I)] corresponding to a [different (B) language (I)] from the input text. \n",
      " Table shows voice cloning performance from an [EN (B) speaker (I)] in the [most (B) data (I)]-poor scenario (129 hours), where only a [single (B) speaker (I)] is available for each [training (B) language (I)] (1EN 1ES 1CN) without using the speaker-[adversarial (B) loss (I)]. \n",
      " Using [byte (B) inputs (I)] 3 it was possible to clone the [EN (B) speaker (I)] to ES with high similarity [MO (B)], albeit with significantly reduced naturalness. \n",
      " However, cloning the [EN (B) voice (I)] to CN failed4, as did cloning to ES and CN using [phonem (B)] inputs. \n",
      "   \n",
      " Using character or [byte (B) inputs (I)] led to similar results. \n",
      " We did nât run listening tests because it was clear that synthesizing EN text using the [CN (B) speaker (I) embedding (I)] did nât affect the model output. \n",
      "\n",
      "\n",
      " Table : Naturalness and [speaker (B) similarity (I) MOS (I)] of cross [- (B) language (I) voice (I)] cloning of the full [multilingual (B) model (I)] using [phonem (B)] inputs. \n",
      "\n",
      "\n",
      " Table : Effect of [EN (B) speaker (I)] cloning with no [residual (B) encoder (I)]. \n",
      "\n",
      "\n",
      " Figure : Visualizing the effect of [voice (B) cloning (I)] and accent control, using 2D PCA of [speaker (B) embeddings (I)] computed from [speec (B)] synthesized with [different (B) speaker (I)], text, and language i d combinations. \n",
      " Embeddings cluster together (bottom left and right), implying high similarity, when the speakerâs [original (B) language (I)] matches the [language (B) embedding (I)], regardless of the [text (B) language (I)]. However, using language i d from the text (squares), modifying the speakerâs accent to speak fluently, hurts similarity compared to the [native (B) language (I)] and accent (circles). \n",
      "     \n",
      " Adding the [adversarial (B) speaker (I) classifier (I)] enabled crosslanguage cloning of the [EN (B) speaker (I)] to CN with very high similarity [MO (B)] for both [byt (B)] and [phoneme (B) models (I)]. \n",
      " However, naturalness [MO (B)] remains much lower than using the [native (B) speaker (I)] identity, with the naturalness listening test failing entirely in the CN case with [byte (B) inputs (I)] as a result of rater comments that the [speec (B)] sounded like a [foreign (B) language (I)]. \n",
      " According to rater comments on the [phoneme (B) system (I)], most of the degradation came from mismatched accent and pronunciation, not fidelity. \n",
      " CN raters commented that it sounded like â a foreigner speaking Chinese â. \n",
      " More interestingly, few ES raters commented that â The voice does not sound robotic but instead sounds like an English [native (B) speaker (I)] who is learning to pronounce the words in Spanish. â \n",
      " Based on these results, we only use [phoneme (B) inputs (I)] in the following experiments since this guarantees that pronunciations are correct and results in more [fluent (B) speech (I)]. \n",
      " Table evaluates voice cloning performance of the full [multilingual (B) model (I)] (84EN 3ES 5CN), which is trained on the [full (B) dataset (I)] with increased [speaker (B) coverage (I)], and uses the speaker-[adversarial (B) loss (I)] and speaker / [language (B) embeddings (I)]. \n",
      " Incorporating the [adversarial (B) loss (I)] forces the text representation to be [less (B) language-specific (I)], instead relying on the [language (B) embedding (I)] to capture [language-dependent (B) information (I)]. \n",
      " Across all [language (B) pairs (I)], the model [synthesizes (B) speech (I)] in all voices with naturalness [MO (B)] above 3.85, demonstrating that increasing [training (B) speaker (I)] diversity improves generalization. \n",
      " In most cases synthesizing EN and ES [speec (B)] (except EN-to-ES) approaches the ground truth scores. In contrast, naturalness of CN [speec (B)] is consistently lower than the ground truth. \n",
      "\n",
      " The high naturalness and similarity [MOS (B) scores (I)] in the top row of Table indicate that the model is able to successfully transfer the [EN (B) voice (I)] to both ES and CN almost without accent.             \n",
      " When consistently conditioning on the [EN (B) language (I) embedding (I)] regardless of the [target (B) language (I)] (second row), the model produces more English accented ES and CN [speec (B)], which leads to lower naturalness but higher similarity [MOS (B) scores (I)]. \n",
      " Also see Figure and the demo for accent transfer [audi (B)] examples.                                                                                            \n",
      " We see that cloning the [CN (B) voice (I)] to [other (B) languages (I)] (bottom row) has the lowest similarity [MO (B)], although the scores are still much higher than different-[speaker (B) similarity (I) MOS (I)] in the off-diagonals of Table indicating that there is some degree of transfer. \n",
      " This is a consequence of the [low (B) speaker (I)] coverage of CN compared to EN in the [training (B) data (I)], as well as the large distance between CN and [other (B) languages (I)].                                                                        \n",
      " Finally, Table demonstrates the importance of training using a [variational (B) residual (I) encoder (I)] to stabilize the model output. \n",
      " Naturalness [MO (B)] decreases by 0.4 points for EN-to-CN cloning without the [residual (B) encoder (I)] (bottom row). \n",
      " In informal comparisons of the outputs of the two models we find that the model without the [residual (B) encoder (I)] tends to skip rare words or inserts unnatural pauses in the output [speec (B)]. \n",
      " This indicates the VAE prior learns a mode which helps stabilize attention. \n",
      "\n",
      " Conclusions \n",
      " We describe extensions to the [Tacotron (B) 2 (I) neural (I) TTS (I) model (I)] which allow training of a [multilingual (B) model (I)] trained only on [monolingual (B) speakers (I)], which is able to synthesize [high (B) quality (I) speech (I)] in three languages, and transfer [training (B) voices (I)] across languages. \n",
      " Furthermore, the model learns to speak [foreign (B) languages (I)] with moderate control of accent, and, as demonstrated on the companion webpage, has rudimentary support for code switching. \n",
      " In future work we plan to investigate methods for scaling up to leverage large amounts of low quality [training (B) data (I)], and support many [more (B) speakers (I)] and languages. \n",
      "\n",
      " Acknowledgements \n",
      " We thank Ami Patel, Amanda Ritchart-Scott, Ryan Li, Siamak Tazari, Yutian Chen, Paul McCartney, Eric Battenberg, Toby Hawker, and Rob Clark for discussions and helpful feedback. \n",
      "\n"
>>>>>>> 277df5128ba97fff0bc6433c27da671025a97b13
     ]
    }
   ],
   "source": [
    "if __name__== \"__main__\":\n",
    "    \"\"\"Main -> to modify by putting all steps in one fonction\"\"\"\n",
    "    init_data = read_data('lexicon.tsv')\n",
    "    data = select_data(init_data)\n",
    "    text_dataframe = lemma_posttag('tts-articles/txt/3.txt')\n",
    "#     text_dataframe = lemma_posttag('test2.txt')\n",
    "#     print(text_dataframe.head(60))\n",
    "    text_dataframe.to_csv(r'terms.txt', header=None, index=None, sep=' ', mode='w')\n",
    "    annotate(data, text_dataframe)\n",
    "#     print(construct_annotated_text(text_dataframe))\n",
    "    annotation = construct_annotated_text(text_dataframe)\n",
    "    tagging_IOB(annotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
