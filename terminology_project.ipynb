{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terminology - Project\n",
    "Authors: CÃ©cile MACAIRE & Ludivine ROBERT "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librairies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "spacy_nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from lexicon\n",
    "def read_data(file):\n",
    "    \"\"\"Read data file with pandas dataframe\"\"\"\n",
    "    return pd.read_csv(file, sep='\\t')\n",
    "\n",
    "def lemma_lexicon(dataframe):\n",
    "    \"\"\"Lemmatization of lexicon with scapy\"\"\"\n",
    "    terms = dataframe['pilot']\n",
    "    lemma = []\n",
    "    for el in terms:\n",
    "        doc = spacy_nlp(el.lower())\n",
    "        tmp = [token.lemma_ for token in doc]\n",
    "        lemma.append(' '.join(tmp))\n",
    "    dataframe['lemma'].replace(lemma)\n",
    "    return dataframe\n",
    "  \n",
    "def select_data(dataframe):\n",
    "    \"\"\"We keep only columns pattern, pilot and lemma\"\"\"\n",
    "    return dataframe[['pattern', 'pilot', 'lemma']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text\n",
    "def read_file(file):\n",
    "    with open(file, 'r') as f:\n",
    "        return f.read()\n",
    "        \n",
    "def lemma_posttag(file):\n",
    "    \"\"\"Convert post-tag scapy into corresponding pattern from lexicon\"\"\"\n",
    "    text = read_file(file)\n",
    "    doc_a = spacy_nlp(text)\n",
    "    doc = spacy_nlp(text.lower())\n",
    "    new_pos = []\n",
    "    pos = []\n",
    "    lemma = []\n",
    "    t = []\n",
    "    original = [token.text for token in doc_a]\n",
    "#     original.append('')\n",
    "    for token in doc:\n",
    "        t.append(token.text)\n",
    "        lemma.append(token.lemma_)\n",
    "        pos.append(token.pos_)\n",
    "        if token.pos_ == 'NOUN' or token.pos_ == 'PROPN':\n",
    "            new_pos.append('N')\n",
    "        elif token.pos_ == 'VERB':\n",
    "            new_pos.append('V')\n",
    "        elif token.pos_ == 'ADJ':\n",
    "            new_pos.append('A')\n",
    "        elif token.pos_ == 'CCONJ' or token.pos_ == 'SCONJ':\n",
    "            new_pos.append('C')\n",
    "        elif token.pos_ == 'PART' or token.pos_ == 'ADP':\n",
    "            new_pos.append('P')\n",
    "        else:\n",
    "            new_pos.append('')\n",
    "    frame = pd.DataFrame({'tokens': original,'tokens_lower':t, 'lemma':lemma, 'pos':pos, 'pattern':new_pos})\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rules(terms_dataframe, text_dataframe):\n",
    "    \"\"\"Define rules from terms according to their pattern\"\"\"\n",
    "    new_terms = []\n",
    "    for terms in terms_dataframe['lemma']:\n",
    "        # Get the same structure of terms as in text dataframe\n",
    "        tmp = ' '.join(terms.split('-'))\n",
    "        new_terms.append(tmp.split(' '))\n",
    "    for i, token in enumerate(text_dataframe['lemma']):\n",
    "        for j, t in enumerate(new_terms):\n",
    "            # Case 1: term of size 3 seperated by dashes (ex: text-to-speech) and followed by 1 or 2 Nouns is a term \n",
    "            if len(t) == 3 and len(text_dataframe['lemma']) >= i+3:\n",
    "                if token == t[0] and text_dataframe['lemma'][i+1] == '-' and text_dataframe['lemma'][i+2] == 'to' or text_dataframe['lemma'][i+2] == 'of' and text_dataframe['lemma'][i+3] == '-' and text_dataframe['lemma'][i+4] == t[2]:\n",
    "                    # followed by 2 nouns (ex: text-to-speech modal synthesis)\n",
    "                    if text_dataframe['pattern'][i+5] == 'N':\n",
    "                        text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                        text_dataframe['tokens'][i+5] = text_dataframe['tokens'][i+5]+']'\n",
    "                    else:\n",
    "                        # followed by 1 noun (ex: text-to-speech system)\n",
    "                        text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                        text_dataframe['tokens'][i+4] = text_dataframe['tokens'][i+4]+']' \n",
    "            # Case 2: term of size 2 separated by dashes (ex: encoder-decoder) and followed by 0,1,2 or 3 nouns is a term\n",
    "            elif len(t) >= 2 and len(text_dataframe['lemma']) >= i+3 and i != 0:\n",
    "                if token == 'front' and text_dataframe['lemma'][i+1] == '-' and text_dataframe['lemma'][i+2] == 'end':\n",
    "                    if text_dataframe['pattern'][i-1] == 'N':\n",
    "                        text_dataframe['tokens'][i-1] = '['+text_dataframe['tokens'][i-1]\n",
    "                        text_dataframe['tokens'][i+2] = text_dataframe['tokens'][i+2]+']'\n",
    "                if token == t[0] and text_dataframe['lemma'][i+1] == '-' and text_dataframe['lemma'][i+2] == t[1]:\n",
    "                    # followed by 3 nouns (ex: HMM-based generation synthesis approach)\n",
    "                    if len(t) == 5:\n",
    "                        if text_dataframe['pattern'][i+3] == 'N' and text_dataframe['pattern'][i+4] == 'N' and text_dataframe['pattern'][i+5] == 'N':\n",
    "                            text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                            text_dataframe['tokens'][i+5] = text_dataframe['tokens'][i+5]+']'\n",
    "                    # followed by 2 nouns (ex: HMM-based generation synthesis)\n",
    "                    elif len(t) == 4:\n",
    "                        if text_dataframe['pattern'][i+3] == 'N' and text_dataframe['pattern'][i+4] == 'N':\n",
    "                            text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                            text_dataframe['tokens'][i+4] = text_dataframe['tokens'][i+4]+']'\n",
    "                    # followed by 1 noun (ex: cross-lingual adaptation)\n",
    "                    elif len(t) == 3:\n",
    "                        if text_dataframe['pattern'][i+3] == 'N':\n",
    "                            text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                            text_dataframe['tokens'][i+3] = text_dataframe['tokens'][i+3]+']'\n",
    "                    # followed by nothing (ex: mel-spectrogram)\n",
    "                    elif len(t) == 2:\n",
    "                        text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                        text_dataframe['tokens'][i+2] = text_dataframe['tokens'][i+2]+']'\n",
    "        if token == 'data' or token == 'datum' or token == 'speaker' or token == 'dataset' or token == 'database' or token == 'feature' or token == 'corpus' or token == 'language' and i != 0 and len(text_dataframe['lemma']) >= i+1:\n",
    "            if text_dataframe['pattern'][i-1] == 'N' or text_dataframe['pattern'][i-1] == 'A':\n",
    "                text_dataframe['tokens'][i-1] = '['+text_dataframe['tokens'][i-1]\n",
    "                text_dataframe['tokens'][i] = text_dataframe['tokens'][i]+']'\n",
    "            elif text_dataframe['pattern'][i+1] == 'N':\n",
    "                text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                text_dataframe['tokens'][i+1] = text_dataframe['tokens'][i+1]+']'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_4 = ['system', 'model', 'synthesis', 'translation', 'recognition', 'signal', 'research', 'processing', 'conversion', 'technique', 'accuracy', 'synthesizer']\n",
    "def annotate(terms_dataframe, text_dataframe):\n",
    "    \"\"\"Annotate the terms of the text thanks to list of terms + applied rules\"\"\"\n",
    "    rules(terms_dataframe, text_dataframe)  # apply rules\n",
    "    for i, token in enumerate(text_dataframe['lemma']):\n",
    "        for term in terms_dataframe['lemma']:\n",
    "            term = term.split(' ')\n",
    "            # Case 1: if terms of length 4, we check if each word from text corresponds to each word in the term\n",
    "            if len(term) == 4:\n",
    "                term_1 = term[0]\n",
    "                if token == term_1 and len(text_dataframe['lemma']) > i+4:\n",
    "                    if text_dataframe['lemma'][i+1] == term[1] and text_dataframe['lemma'][i+2] == term[2] and text_dataframe['lemma'][i+3] == term[3]:\n",
    "                        text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                        text_dataframe['tokens'][i+3] = text_dataframe['tokens'][i+3]+']'\n",
    "                        i += 3\n",
    "            # Case 2: terms of length 3\n",
    "            elif len(term) == 3:\n",
    "                term_1 = term[0]\n",
    "                if token == term_1 and len(text_dataframe['lemma']) > i+3:\n",
    "                    if text_dataframe['lemma'][i+1] == term[1] and text_dataframe['lemma'][i+2] == term[2]:\n",
    "                        text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                        text_dataframe['tokens'][i+2] = text_dataframe['tokens'][i+2]+']'\n",
    "                        i += 2\n",
    "            # Case 3: terms of length 2\n",
    "            elif len(term) == 2:\n",
    "                if token == term[0] and len(text_dataframe['lemma']) > i+2:\n",
    "                    if text_dataframe['lemma'][i+1] == term[1]:\n",
    "                        text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                        text_dataframe['tokens'][i+1] = text_dataframe['tokens'][i+1]+']'\n",
    "                        i += 1\n",
    "            # Case 4: term of length 1\n",
    "            elif token == term[0] and i > 1 and text_dataframe['lemma'][i-1] == 'of' and text_dataframe['lemma'][i-2] == 'sequence':\n",
    "                text_dataframe['tokens'][i-2] = '['+text_dataframe['tokens'][i-2]\n",
    "                text_dataframe['tokens'][i] = text_dataframe['tokens'][i]+']'\n",
    "            elif token == term[0] and len(term) == 1 and len(text_dataframe['lemma']) >= i+1:\n",
    "                if text_dataframe['lemma'][i+1] in rule_4:\n",
    "                    text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]\n",
    "                    text_dataframe['tokens'][i+1] = text_dataframe['tokens'][i+1]+']'\n",
    "                else:\n",
    "                    text_dataframe['tokens'][i] = '['+text_dataframe['tokens'][i]+']'\n",
    "    return text_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_annotated_text(text_dataframe):\n",
    "    \"\"\"Return the text from the annotated text dataframe with the correct annotation of brackets\"\"\"\n",
    "    content = ' '.join(text_dataframe['tokens'].to_list())\n",
    "    compt = 0\n",
    "    compt2 = 0\n",
    "    string = ''\n",
    "    for i in content:\n",
    "        if i == '[':\n",
    "            if compt == 0:\n",
    "                compt += 1\n",
    "                string += i\n",
    "            elif compt >= 1:\n",
    "                compt += 1\n",
    "        elif i == ']':\n",
    "            if compt-1 != compt2:\n",
    "                compt2 += 1\n",
    "            else:\n",
    "                string += i\n",
    "                compt = 0\n",
    "                compt2 = 0\n",
    "        else:\n",
    "            string += i\n",
    "    string2 = ''\n",
    "    string = string.replace('] [', ' ')\n",
    "    string = string.replace(' .', '.')\n",
    "    string = string.replace(' â', 'â')\n",
    "    string = string.replace(' ,', ',')\n",
    "    string = string.replace(' - ', '-')\n",
    "    string = string.replace('( ', '(')\n",
    "    string = string.replace(' )', ')')\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Grapheme-to-Phoneme Conversion] with [Convolutional Neural Networks] \n",
      "          \n",
      "\n",
      "\n",
      " Abstract : [Grapheme-to-phoneme] ([G2P]) conversion is the process of generating pronunciation for words based on their written form. \n",
      " It has a highly essential role for [natural language processing], [text-to-speech synthesis] and [automatic speech recognition] systems. \n",
      " In this paper, we investigate [convolutional neural networks] (CNN) for [G2P conversion]. \n",
      " We propose a novel CNN-based [sequence-to-sequence] ([seq2seq]) architecture for [G2P conversion]. \n",
      " Our approach includes an [end-to-end CNN G2P conversion] with [residual connections] and, furthermore, a model that utilizes a [convolutional neural network] (with and without [residual connections]) as encoder and [Bi-LSTM] as a decoder. \n",
      " We compare our approach with state-of-the-art methods, including [Encoder-Decoder LSTM] and [Encoder-Decoder Bi-LSTM]. \n",
      " Training and inference times, [phoneme] and [word error rates] were evaluated on the public [CMUDict dataset] for US English, and the best performing [convolutional neural network]-based architecture was also evaluated on the [NetTalk dataset]. \n",
      " Our method approaches the accuracy of previous state-of-the-art results in terms of [phoneme error rate]. \n",
      " Keywords : [grapheme-to-phoneme] ([G2P]) ; [encoder-decoder] ; [LSTM] ; 1D convolution ; [Bi-LSTM] ; residual architecture \n",
      "\n",
      "\n",
      "\n",
      " Introduction \n",
      "     \n",
      " The process of [grapheme-to-phoneme] ([G2P]) conversion generates a [phonetic] transcription from the written form of words. \n",
      " The spelling of a word is called a [grapheme sequence] (or [graphemes]), the [phonetic] form is called a [phoneme sequence] (or [phonemes]). \n",
      " It is essential to develop a phonemic lexicon in [text-to-speech] ([TTS]) and [automatic speech recognition] ([ASR]) systems. \n",
      " For this purpose, [G2P techniques] are used, and getting state-of-the-art performance in these systems depends on the accuracy of [G2P conversion]. \n",
      " For instance, in [ASR acoustic models], the pronunciation lexicons and [language models] are critical components. \n",
      " Acoustic and [language models] are built automatically from large corpora. \n",
      " Pronunciation lexicons are the middle layer between acoustic and [language models]. \n",
      " For a new [speech recognition] task, the performance of the overall system depends on the quality of the pronunciation component. \n",
      " In other words, the systemâs performance depends on [G2P accuracy]. \n",
      " For example, the [G2P conversion] of word â speakerâ is âS P IY K ERâ. \n",
      " In [TTS] systems, a high-quality [G2P model] is also an essential part and has a great influence on the overall quality. \n",
      " Inaccurate [G2P conversion] results in unnatural pronunciation or even incomprehensible [synthetic speech]. \n",
      "\n",
      " Previous Works \n",
      "\n",
      " [G2P conversion] has been studied for a long time. \n",
      " Rule-based [G2P systems] use a wide set of [grapheme-to-phoneme rules]. \n",
      " Developing such a [G2P system] requires linguistic expertise. \n",
      " Additionally, some languages (such as Chinese and Japanese) have complex writing systems, and building the rules is labor-intensive and it is extremely difficult to cover most possible situations. \n",
      " Furthermore, these systems are sensitive to out of vocabulary (OOV) events. \n",
      " Other previous solutions used joint sequence models. \n",
      " These models create an initial [grapheme]-[phoneme sequence] alignment, and by using this alignment, it calculates a joint [n-gram language model] over sequences. \n",
      " The method proposed by is implemented in the publicly available tool Sequitur (https://www-i6.informatik.rwth-aachen.de / web / Software / g2p.html, Access date : 9th August 2018). \n",
      " In [one-to-one alignment], each [grapheme] corresponds to only one [phoneme], and vice versa. \n",
      " An â empty â symbol is introduced to match [grapheme and phoneme sequences]. \n",
      " For example, the [grapheme sequence] of â CAKEâ matches the [phoneme sequence] of â K EY Kâ, and [one-to-one alignment] of these sequences is C â K, A â EY, K â K, and the last [grapheme] â Eâ matches the â empty â symbol. \n",
      " Conditional and joint maximum entropy models use this approach. \n",
      " Later, Hidden Conditional Random Field (HCRF) models were introduced in which the alignment between [grapheme and phoneme sequence] is modelled with hidden variables. \n",
      " The HCRF models usually lead to very competitive results ; however, the training of such models is very memory and computationally intensive. \n",
      " A further approach utilizes conditional random fields (CRF) and Segmentation / Tagging models (such as linear finite-state automata or transducers, FSTs), then use them in two different compositions. \n",
      " The first composition is a joint-multigram combined with CRF ; the second one is a joint-multigram combined with Segmentation / Tagging. \n",
      " The first approach achieved 5.5 % [phoneme error rate] (PER) on [CMUDict]. \n",
      " Recently, [neural networks] have been applied for [G2P conversion]. \n",
      " [Neural network]-based [G2P conversion] is robust against spelling mistakes and OOV words ; it generalizes well. \n",
      " Also, it can be seamlessly integrated into [end-to-end TTS] / [ASR] systems (that are constructed entirely of [deep neural networks]). \n",
      " In this paper, a [TTS system] ([Deep Voice]) is presented which was constructed entirely from [deep neural networks]. \n",
      " [Deep Voice] lays the groundwork for truly [end-to-end] neural [speech synthesis]. \n",
      " Thus, the [G2P model] is jointly trained with further essential parts of the [speech synthesizer] and recognizer, which increase the overall quality of the system. \n",
      " [LSTM] has shown competitive performance in various fields, like acoustic modelling and [language understanding]. \n",
      " One of the early neural approaches investigates unidirectional Long Short-Term Memory (ULSTM) with full output delays, which achieved 9.1 % [phoneme error rate]. \n",
      " In the same paper, a deep [bidirectional LSTM] (DBLSTM) was combined with connectionist temporal classification (CTC) and joint n-gram models for better accuracy (21.3 % [word error rate]). \n",
      " Please note that CTC objective function was introduced to infer [speech]-[label] alignments automatically without any intermediate process, leading to an [end-to-end approach] for [ASR]. \n",
      " CTC technique has combined with CNN, [LSTM] for the various [speech]-related tasks. \n",
      " Due to utilizing an [encoder-decoder] approach for the [G2P] task, a separate alignment between [grapheme] sequences and [phoneme sequences] became unnecessary. \n",
      " Alignment-based models of unidirectional [LSTM] with one layer and bi-directional [LSTM] ([Bi-LSTM]) with one, two and three layers have also previously been investigated. \n",
      " In this work, alignment was explicitly modelled in the [G2P conversion] process by the context of the [grapheme]. \n",
      " A further work, which applies deep bi-directional [LSTM] with hyperparameter optimization (including the number of hidden layers, optional linear projection layers, optional splicing window at the input) considered various alignment schemes. \n",
      " The best model with hyperparameter optimization achieved a 5.37 % [phoneme error rate] (PER) and a 23.23 % [word error rate] (WER). \n",
      " Multi-layer bidirectional encoder with gated recurrent units (GRU) and deep unidirectional GRU as a decoder achieved 5.8 % PER and 28.7 % WER on [CMUDict]. \n",
      " [Convolutional neural networks] have achieved superior performance compared to previous methods in large-scale image recognition. \n",
      " Recently, these architectures were also applied to [Natural Language Processing] (NLP) tasks, including sentence classifications and [neural machine translation]. \n",
      " Nowadays, completely [convolutional neural networks] may achieve superior results compared to recurrent solutions. \n",
      " [Sequence-to-sequence] ([seq2seq]) learning, or [encoder-decoder] type [neural networks] have achieved remarkable success in various tasks, such as [speech recognition], [text-to-speech synthesis], [machine translation]. \n",
      " This type of network is used for several tasks, and its performance has also been enhanced with [attention mechanisms]. \n",
      " In this structure, the encoder computes a representation of each input sequence, and the decoder generates an output sequence based on the learned representation. \n",
      " In [ 28 ], bidirectional multi-layer [recurrent neural network]-based [seq2seq] learning was investigated in two architectures : a single [Bi-LSTM] / Bidirectional Gated Recurrent Unit (Bi-GRU) layer and two [Bi-LSTM] / Bi-GRU layers. \n",
      " Both [Bi-LSTM] and Bi-GRU uses both past and future contexts. \n",
      " Moreover, a [bidirectional decoder] was proposed for [neural machine translation] (NMT) in. \n",
      " Both encoder and decoder are Bi-GRU, but this model is applicable to other RNNs, such as [LSTM]. \n",
      " By introducing a backward decoder, the purpose of which is to exploit reverse target-side contexts, the results of NMT task was improved. \n",
      " For [speech recognition], several [sequence-to-sequence models], including connectionist temporal classification (CTC), the [recurrent neural network] (RNN) transducer, and an attention-based model, have been analyzed. \n",
      " The basics of sequence modelling with [convolutional networks] are summarized in. \n",
      " Furthermore, the key components of the temporal convolution network (TCN) have also been introduced, and some vital advantages and disadvantages of using TCN for sequence predictions instead of [RNNs] were analyzed as well. \n",
      " The [encoder-decoder] structure has been studied for the [G2P] task before, but usually, [LSTM] and GRU networks have been involved. \n",
      " For example, Baiduâs [end-to-end text-to-speech synthesizer], called [Deep Voice], uses the multi-layer bidirectional encoder with GRUâs non-linearity and an equally [deep unidirectional GRU decoder]. \n",
      " Until now, the best result for [G2P conversion] was introduced by, which applied an attention-enabled [encoder-decoder] model and achieved 4.69 % PER and 20.24 % WER on [CMUDict]. \n",
      " Furthermore, [G2P]-[seq2seq] (https://github.com/cmusphinx/g2p-seq2seq, Access date : 9th August 2018) is based on [neural networks] implemented in the TensorFlow framework with 20.6 % WER. \n",
      " To the best of our knowledge, our approach is the first that uses [convolutional neural networks] for [G2P conversion]. \n",
      " In this paper, we present one general [sequence-to-sequence] and four [encoder-decoder] models. \n",
      " These are introduced in Section. \n",
      " Our goal was to achieve and surpass (if possible) the accuracy of previous models and to reduce the training times (which is quite high in the case of [LSTM] / GRU). \n",
      " The remaining parts of this paper are structured as follows : Section discusses the possibility of applying [convolutional neural networks] for [sequence-to-sequence]-based [grapheme-to-phoneme conversion]. \n",
      " Datasets, training processes, and evaluation of the proposed models are presented in Section. \n",
      " Section analyzes the results of the models, and finally, the conclusion is drawn in Section. \n",
      "\n",
      " [Convolutional Neural Networks] for [Grapheme] to [Phoneme Conversion] \n",
      "\n",
      " [Convolutional neural networks] are used in various fields, including image, object and handwriting recognition, face verification, [natural language processing] and [machine translation]. \n",
      " The architecture of an ordinary CNN is composed of many layer types (such as the [convolutional layers], pooling layers, fully connecting layers, etc.), where each layer carries out a specific function. \n",
      " The convolutional and pooling layers are for representation learning, while the fully connected layers on the top of the network are for modelling a classification or regression problem. \n",
      " One of the main reasons that make [convolutional neural networks] superior to previous methods is that CNNs perform representation learning and modelling jointly ; thus, a quasi-optimal representation is extracted from the [input data] for the machine learning model. \n",
      " Weight sharing in the [convolutional layers] is also a key element. \n",
      " Thus, the model becomes spatially tolerant ; similar representations are learned in different regions of the input, and the total number of parameters can also be reduced drastically. \n",
      " [Deep Learning] refers to the increased depth of [neural networks]. \n",
      " Intuitively, it is expected that [neural networks] with many hidden layers are more powerful than shallow ones with a single hidden layer. \n",
      " However, as the number of layers increases, the training may become surprisingly hard, partly because the gradients are unstable. \n",
      " [Batch normalization] is a technique to overcome this problem ; it reduces internal covariance shift and helps to smooth learning. \n",
      " The main idea of [batch normalization] is to bring back the benefits of normalization at each layer. \n",
      " [Batch normalization] results in faster convergence as well. \n",
      " For example, with [batch normalization], 7 % of the training steps were enough to achieve similar accuracy in an image classification task. \n",
      " Moreover, an additional advantage of [batch normalization] is that it regularizes the training and thus reduces the need for dropout and other regularization techniques. \n",
      " However, [batch normalization] and dropout are often simultaneously applied. \n",
      " [Convolutional neural networks] have been successfully applied to various NLP tasks. \n",
      " These results suggest investigating the possibility of applying CNN-based [sequence-to-sequence models] for [G2P]. \n",
      " We expected that the advantage of [convolutional neural networks] enhances the performance of [G2P conversion]. \n",
      " As is known, [LSTMs] read input sequentially, and the output for further inputs depends on the previous ones. \n",
      " Thus, we can not parallelize these networks. \n",
      " Applying CNN also reduces computational load by using large receptive fields. \n",
      " [Deep neural networks] with a sequential architecture have many typical building blocks, such as convolutional or fully connected layers, stacked on each other. \n",
      " Increasing the number of layers in these kinds of networks does not implicitly mean improved accuracy (in our case PER or WER), and some issues, such as vanishing gradient and degradation problems, can arise as well. \n",
      " Introducing residual and highway connections can improve performance significantly. \n",
      " These connection alternatives allow the information to flow more into the deeper layers, increase the convergence speed and decrease the vanishing gradient problem. \n",
      "\n",
      " Models \n",
      "\n",
      " [Encoder-decoder] structures have shown state-of-the-art results in different NLP tasks. \n",
      " The main idea of these approaches has two steps : the first step is mapping the input sequence to a [vector] ; the second step is to generate the output sequence based on the learned [vector] representation. \n",
      " [Encoder-decoder] models generate an output after the complete input sequence is processed by the encoder, which enables the decoder to learn from any part of the input without being limited to fixed context windows. \n",
      " Figure shows an example of an [encoder-decoder] architecture. \n",
      "\n",
      " Figure. The input of the encoder is the â CAKE â [grapheme sequence], and the decoder produces â KEY K â as the [phoneme sequences]. \n",
      " The left side is the encoder ; the right side is the decoder. \n",
      " The model stops making predictions after generating the end-of-[phonemes] tag. \n",
      " As distinct from, [input data] for the encoder is not reversed in any of our models. \n",
      "\n",
      " In our experiments, we used [encoder-decoder] architectures. \n",
      " Several models with different hyperparameters were developed and tested. \n",
      " From a large number of experiments, the five models with the highest accuracy and diverse architectures were selected. \n",
      " Our first two models were based on existing solutions for comparison purposes. \n",
      " We used these models as a baseline. \n",
      " In the following paragraphs, the five models are introduced :      \n",
      " The first model uses [LSTMs] for both the encoder and the decoder. \n",
      " The [LSTM] encoder reads the input sequence and creates a fixed-dimensional [vector] representation. \n",
      " The second [LSTM] is the decoder, and it generates the output. \n",
      " [Figurea] shows the structure of the first model. \n",
      " It can be seen that both [LSTMs] have 1024 units ; [softmax] activation function is used to obtain model predictions. \n",
      " This architecture is the same as a previous solution, while the parameters of training (optimization method, regularization, etc.) are identical to the settings used in case of the other four models. \n",
      " In this way we try to ensure a fair comparison among the models. \n",
      "\n",
      " Figure. [G2P conversion] model based on [encoder-decoder] (a) [LSTMs] (first model) ; (b) Bi-[LSTMs] (second model) ; (c) encoder CNN, decoder [Bi-LSTM] (third model). \n",
      " f, d, s are the number of the filters, length of the filters and stride, respectively, in the [convolutional layer]. \n",
      "\n",
      " Although the [encoder-decoder] architecture achieves competitive results on a wide range of problems, it suffers from the constraint that all input sequences are forced to be encoded to a fixed-size latent space. \n",
      " To overcome this limitation, we investigated the effects of the [attention mechanism] proposed by in Model 1 and Model 2. \n",
      " We applied an [attention layer] between the encoder and decoder [LSTMs] in the case of Model 1, and Bi-[LSTMs] for Model 2. \n",
      " The introduced [attention layers] are based on global attention. \n",
      " In the second model, both the encoder and the decoder are Bi-[LSTMs]. \n",
      " The structure of this model is presented in Figureb. \n",
      " The input is fed to the first [Bi-LSTM] (encoder), which combines two unidirectional [LSTM layers] that process the input from left-to-right and right-to-left. \n",
      " The output of the encoder is given as the input for the second [Bi-LSTM] (decoder). \n",
      " Finally, the [softmax] function is applied to generate the output of one-hot [vectors] ([phonemes]). \n",
      " During the inference, the complete input sequence is processed by the encoder, and after that, the decoder generates the output. \n",
      " For predicting a [phoneme], both the left and the right contexts are considered. \n",
      " This model was also inspired by an existing solution. \n",
      " In the third model, a [convolutional neural network] is introduced as the encoder, and a [Bi-LSTM] as the decoder. \n",
      " This architecture is presented in Figurec. \n",
      " As this figure shows, the number of filters is 524, the length of the filter is 23, the stride is 1, and the number of cells in the [Bi-LSTM] is 1024. \n",
      " In this model, the CNN layer takes [graphemes] as input and performs convolution operations. \n",
      " For regularization purpose, we also introduced [batch normalization] in this model. \n",
      " The fourth model contains [convolutional layers] only, with [residual connections] (blocks). \n",
      " These [residual connections] have two rules : \n",
      " if [feature maps] have the same size, then the blocks share the same hyperparameters. \n",
      " each time when the [feature map] is halved, the number of filters is doubled. \n",
      " First, we apply one [convolutional layer] with 64 filters to the input layer, followed by a stack of residual blocks. \n",
      " Through hyperparameter optimization, the best result was achieved by 4 residual blocks, as shown in [Figurea], and the number of filters in each residual block is 64, 128, 256, 512, respectively. \n",
      " Each residual block contains a sequence of two [convolutional layers] followed by a [batch normalization] layer and [ReLU] activation. \n",
      " The filter size of all [convolutional layers] is three. \n",
      " After these blocks, one more [batch normalization] layer and [ReLU] activation are applied. \n",
      " The architecture ends with a fully connected layer, which uses the [softmax] activation function. \n",
      "\n",
      " Figure. [G2P conversion] based on (a) [convolutional neural network] with [residual connections] (fourth model) ; and (b) encoder [convolutional neural network] with [residual connections] and decoder [Bi-LSTM] (fifth model). \n",
      " f, d, s are the number of the filters, length of the filters and stride, respectively. \n",
      "\n",
      " We carried out experiments with the same fully convolutional models without [residual connections] ; however, the [phoneme] and [word error rates] were worse than with [residual connections], as expected. \n",
      " The fifth model combines Models 3 and 4 : the encoder has the same [convolutional neural network architecture] with [residual connections] and [batch normalization] that was introduced in model four. \n",
      " The decoder is a [Bi-LSTM], as in model three. \n",
      " The structure of this model is presented in Figureb. \n",
      " In all models except Model 4, we used stateless [LSTM] (or [Bi-LSTM]) configurations ; the internal state is reset after each batch for predictions. \n",
      "\n",
      " Details of the [Bidirectional Decoder] \n",
      "\n",
      " The details of the [bidirectional decoder], which was used in Model 2, are presented in this section. \n",
      " Given an input sequence x = (x1, x2,..., x N), the [LSTM] network computes the hidden [vector] sequence h = (h1, h2,..., h N) and output [vector] sequence y = (y1, y2,..., y N). \n",
      " Initially, one-hot character [vectors] for [graphemes and phonemes sequences] were created. \n",
      " Character vocabularies, which contain all the elements that are present in the input and [output data], are separately calculated. \n",
      " In other words, neither a [grapheme vector] in the output vocabulary, nor a [phoneme vector] in the input vocabulary, was used. \n",
      " These were the inputs to the encoder and the decoder. \n",
      " Padding was applied to make all input and output sequences to have the same length, which was set to 22. \n",
      " This number (22) was chosen based on the maximum length in the [training database]. \n",
      " For [G2P], x = (x1, x2,..., x N) are one-hot character [vectors] of [grapheme] sequences ; y = (y1, y2,..., y N) are one-hot character [vectors] of [phoneme sequences]. \n",
      " In the proposed Model 2, [Bi-LSTM] was used as an encoder, and it consists of two [LSTMs] : one that processes the sequence from left-to-right (forward encoder), and one that does it in reverse (backward encoder). \n",
      " This was applied to learn the semantic representation of the input sequences in both directions. \n",
      " One [LSTM] looks at the sequence from left-to-right (forward encoder), and so reads an input sequence in left-to-right order ; and another [LSTM] looks at it in reverse (backward encoder), and so reads an input sequence in a right-to-left order. \n",
      " At each of the time steps, the forward hidden â sequence ~h and the backward hidden sequence h are iterated by the following equations : \n",
      " In Equation, the forward layer is iterated from t = 1 to N ; in Equation, the backward layer is iterated from t = N to 1 ; H is an element-wise [sigmoid function]. \n",
      " As the next step, the hidden states of these two [LSTMs] were concatenated to form an annotation â â sequence h = { h1, h2,..., h N }, where ht = [ ht, ht ] encodes information about the t â th [grapheme] with respect to all the other surrounding [graphemes] in the input. \n",
      "  W â, W â, Wââ and Wââ are weight xh      xh        hh       hh matrixes ; bâ, bâ denotes the bias [vectors]. \n",
      " Generally, in all parameters, the arrows pointing left to right             h     h and right to left refer to the forward and backward layers, respectively. \n",
      " The forward [LSTM] unrolls the sequences until it reaches the end of sequence for that input. \n",
      " The backward [LSTM] unrolls the sequences until it reaches the start of the sequence. \n",
      " For the decoder, we used [bidirectional LSTM]. \n",
      " These [LSTMs] can be called forward and backward â (decoders, and described as d, d. \n",
      " After concatenating the forward and backward encoder [LSTMs], the backward decoder performs decoding in a right-to-left way. \n",
      " It was initialized with a final encoded state and a reversed output ([phonemes]). \n",
      " The forward decoder is trained to sequentially predict the next [phoneme] given the [phoneme sequence]. \n",
      " This part was initialized with the final state of the encoder and all [phoneme sequences]. \n",
      " Each decoder output is passed through the [softmax] layer that will learn to classify the correct [phonemes]. \n",
      " For training, given the previous [phonemes], the model factorizes the conditional into a summation of individual log conditional probabilities from both directions, \n",
      " where log P yt y[1:(tâ1) ] and log P yt y[(t+1):N ] are the left-to-right (forward), the right-to-left (backward) conditional probability in Equation, and calculated as per the equations below : \n",
      "\n",
      " The prediction is performed on [test data] as follows : \n",
      " According to Equation, future output is not used during inference. \n",
      " The architecture is shown in Figure. \n",
      "\n",
      " Figure. The architecture of the proposed [bidirectional decoder] model for [G2P] task. \n",
      " \n",
      " Experiments \n",
      "\n",
      " Datasets \n",
      "\n",
      " We used the CMU pronunciation (http://www.speech.cs.cmu.edu / cgi-bin / [cmudict]) and [NetTalk] (we are grateful to Stan Chen for providing the data) datasets, which have frequently been chosen by various researchers [ 3,16,32 ]. \n",
      " The training and testing splits are the same as those found in [ 4,5,8,12 ] ;   thus, the results are comparable. \n",
      " [CMUDict] contains a 106,837-word training set and a 12,000-word test set ([reference data]). \n",
      " 2670 words are used as development set. \n",
      " There are 27 [graphemes] (uppercase alphabet symbols plus the apostrophe) and 41 [phonemes] (AA, AE, AH, AO, AW, AY, B, CH, D, DH,   EH, ER, EY, F, G, HH, IH, IY, JH, K, L, M, N, NG, OW, OY, P, R, S, SH, T, TH, UH, UW, V, W, Y, Z, ZH, < EP >, < /EP >) in this dataset. \n",
      " [NetTalk] contains 14,851 words for training, 4,951 words for testing and does not have a predefined validation set. \n",
      " There are 26 [graphemes] (lowercase alphabet symbols) and 52 [phonemes] (â !â, â #â, â *â, â +â, â @â, â Aâ, â Câ, â Dâ, â Eâ, â Gâ, â Iâ, â Jâ, â Kâ, â Lâ, â Mâ, â Nâ, â Oâ, â Râ, â Sâ, â Tâ, â Uâ, â Wâ, â Xâ, â Yâ, â Zâ, â Ëâ, â aâ, â bâ, â câ, â dâ, â eâ, â fâ, â gâ, â hâ, â iâ, â kâ, â lâ, â mâ, â nâ, â oâ, â pâ, â râ, â sâ, â tâ, â uâ, â vâ, â wâ, â xâ, â yâ, â zâ,<EP >, < /EP >) in this dataset. \n",
      " We use < EP > and < /EP > tokens as beginning-of-[graphemes] and end-of-[graphemes] tokens in both datasets. \n",
      " For inference, the decoder uses the past [phoneme sequence] to predict the next [phoneme], and it stops predicting after token < /EP >. \n",
      "\n",
      " Training \n",
      "     \n",
      " For the [CMUDict] experiments, in all models, the size of the input layers is equal to the input : { length of the longest input (22) Ã number of [graphemes] (27) } and the size of the [output layers] is equal to the output : { length of the longest output (22) Ã number of [phonemes] (41) }. \n",
      " To transform [graphemes] and [phonemes] for [neural networks], we convert inputs into 27-dimensional and outputs to 41-dimensional one-hot [vector] representations. \n",
      " For example, the [phoneme sequences] of the word â ARRESTâ is â ER EH S Tâ ; the input and output [vectors] of the [grapheme and phoneme sequences] are as below : \n",
      "\n",
      "\n",
      " In the case of [LSTMs], we applied the [Adam optimization algorithm] with a starting learning rate of 0.001, and with baseline values of Î²1, Î²2 and Îµ (0.9, 0.999 and 1 Ã 10â8, respectively). \n",
      " For [batch size], 128 was chosen. \n",
      " Weights were saved when the PER on the [validation dataset] achieved a lower value than before. \n",
      " When the PER did not decrease further for 100 epochs, the best model was chosen, and it was trained with stochastic gradient descent (SGD) further. \n",
      " In the case of the first, second and third models for SGD, we used 0.005 as the learning rate, and 0.8 for momentum. \n",
      " For the fourth (convolutional with [residual connections]) model, 0.05 (learning rate) and 0.8 (momentum) were applied, and it was trained for 142 when early stopping was called. \n",
      " In the fifth model, 0.5 (learning rate) of SGD and 0.8 (momentum) was set, and when PER stopped improving for about 50 epochs, the learning rate was multiplied by 4/5. \n",
      " The numbers of epochs for this model reached 147 and 135 for [CMUDict] and [NetTalk], respectively. \n",
      " In all proposed models, the patience of early stopping was set to 50 in the [Adam optimizer] and 30 in the SGD optimizer. \n",
      " For [NetTalk] experiments, the sizes of the input and [output layers] are as follows : inputâ{(length of the longest input (19) Ã number of [graphemes] (26) }) ; and outputâ{length of the longest output (19) Ã number of [phonemes] (52) }. \n",
      " We converted inputs to 26-dimensional and outputs to 52-dimensional one-hot [vector] representations as in case of [CMUDict]. \n",
      " The same model structure was used as with the [CMUDict] experiments. \n",
      " Moreover, the implementation of a single [convolutional layer] on [input data] is presented in Figure. \n",
      " The input is a one-hot [vector] of â ARRESTâ ; 64 filters of (input length) Ã3 are applied to the input. \n",
      " In other words, the input is convolved with 64 [feature maps], which produce the output of the [convolutional layer]. \n",
      " Zero padding was used to ensure that the output of the convolution layer has the same dimension as the input. \n",
      " During training, the filter weights are optimized to produce lower loss values. \n",
      "\n",
      " Figure. Implementation of a single [convolutional layer] with 64 filters of size (input length) Ã3 to the [input data]. \n",
      "\n",
      " During inference, prediction of the graphemes sequence is decoded until < /EP >, and the length of input and output are not considered. \n",
      "\n",
      " Evaluation and Results \n",
      "      \n",
      " NVidia Titan Xp (12 GB) and NVidia Titan X (12 GB) GPU cards hosted in two i7 workstations with 32 GB RAM served for training and inference. \n",
      " Ubuntu 14.04 with Cuda 8.0 and cuDNN 5.0 was used as a general software architecture. \n",
      " For training and evaluation, the Keras deep learning framework with Theano backend was our environment. \n",
      " For evaluation, the standard and commonly used measurements of [phoneme error rate] (PER) and [word error rate] (WER) were calculated. \n",
      " PER was used to measure the distance between the predicted [phoneme sequence] and reference pronunciation divided by the number of [phonemes] in the reference pronunciation. \n",
      " Edit distance (also known as Levenshtein distance) is the minimum number of insertions (I), deletions (D) and substitutions (S), that are required to transform one sequence into the other. \n",
      " If there are multiple pronunciation variants for a word in the [reference data], the variant that has the smallest Levenshtein distance to the candidate is used. \n",
      " Levenshtein distance can be calculated by dynamic programming method. \n",
      " For WER computation, which is only counted if the predicted pronunciation does not match any reference pronunciation, the number of word errors is divided by the total number of unique words in the reference. \n",
      " After training the model, predictions were run on the [test dataset]. \n",
      " The results of evaluation on the [CMUDict dataset] are shown in Table. \n",
      " The first and second columns show the model number and the applied architecture, respectively. \n",
      " The third and fourth columns show the PER and WER values. \n",
      " The fifth column of Table contains the average sum of training and validation time of one [epoch]. \n",
      " The last two columns present information about the size of models, which shows the number of parameters (weights) and the number of epochs to reach minimum validation loss. \n",
      " According to the results, the [encoder-decoder Bi-LSTM] architecture (Model 2) outperforms the first model, as expected. \n",
      " However, attention-based Model 1 (called Model 1A in Table) outperforms Model 2 in terms of PER. \n",
      " The best WER and PER values are achieved by the fifth model : PER is 4.81 %, and WER is 25.13 %. \n",
      " Attention-based Model 2 (called Model 2A in Table) approaches the best results in terms of both PER and WER. \n",
      " However, the number of parameters of Model 2A is twice as high as for Model 5. \n",
      " Although the fourth model was faster than all of the other models, both the PER and WER of this model were the highest ; however, they are still competitive. \n",
      " Moreover, this model also has the fewest parameters. \n",
      "\n",
      " Table. Results on the [CMUDict dataset]. \n",
      "\n",
      "\n",
      "\n",
      " We compared the performance of the fifth model on both [CMUDict] and [NetTalk] with previously achieved state-of-the-art results. \n",
      " These comparisons are presented in Table. \n",
      " The first column shows the dataset, the second column presents the method used in previous solutions with references, PER and WER columns tell the results of the referred models. \n",
      " Table clearly shows that our fifth model outperforms the previous solutions by PER on each dataset, except for. \n",
      " For [NetTalk], we were able to significantly surpass the previous state-of-the-art, but a better WER was obtained by with an [encoder-decoder] network based on an [attention mechanism]. \n",
      " We should point out that the results of the fifth model are very close to those obtained by. \n",
      "\n",
      " Table. Comparison of best previous results of [G2P models] with our fifth model (encoder is a CNN with [residual connections], [Bi-LSTM] decoder) on [CMUDict] and [NetTalk]. \n",
      "\n",
      "\n",
      " The proposed best model in consists of the combination of the sequitur [G2P] (model order 8) and [seq2seq-attention] ([Bi-LSTM] 512 Ã 3) and multitask learning (ARPAbet / IPA), and although the WER in their case is better, Model 5 has a smaller PER. \n",
      " Although the [encoder-decoder LSTM] by is similar to our first model, the PER is better in our case ; the WER of both models is almost the same. \n",
      " Our second model is comparable with, in which the [Bi-LSTM] method was implemented, alignment was also applied. \n",
      "\n",
      " Discussion \n",
      "\n",
      " In this section, we discuss the results of the previous section and analyze the connection between PER values and word length, furthermore the position of the error within the word. \n",
      "\n",
      " We categorize the word length into 3 classes : short (shorter than 6 characters), medium (between 6 and 10 characters), long (more than 10 characters). \n",
      " According to this categorization, there were 4306 short, 5993 medium and 1028 long words in the [CMUDict dataset]. \n",
      " In this analysis, we ignored approximately 600 words that have multiple pronunciation variants in the [reference data]. \n",
      " The results of this comparison are presented in [Figurea]. \n",
      " For short words, all models show similar PERs ; for medium length words, except the [end-to-end CNN] model (fourth model), the other models resulted in similar error ; for long words, encoder CNN with [residual connection], decoder [Bi-LSTM] (fourth model) and encoder CNN, decoder [Bi-LSTM] (third model) got similar minimum errors. \n",
      " The fourth model showed the highest error in both medium and long length words. \n",
      " According to [Figurea], the advantage of [Bi-LSTM]-based models is clearly shown for learning long sequences. \n",
      " Moreover, errors occurring in the first half of the pronunciation (in the reference) increases the probability of predicting incorrect [phonemes] in the second half. \n",
      " Still, a correctly predicted first half can not guarantee a correctly predicted second half. \n",
      " In our experiments, convolutional architectures also performed well on short and on long-range dependencies. \n",
      " Our intuition is that the [residual connections] enable the network to consider features learned by lower and higher layers â which represents shorter and longer dependencies. \n",
      " We also analyzed the position of the errors in the reference pronunciation : we investigated whether the error occurred in the first or in the second half of the word. \n",
      " The type of error can be insertion (I), deletion (D) and substitution (S). \n",
      " By using this position information, we can analyze the distribution of these errors across the first or second half of the word. \n",
      " The position of error was calculated by enumerating [graphemes] in the reference. \n",
      " For insertion error (I), the position of the previous [grapheme] was taken into account. \n",
      " The example below describes the process details : \n",
      " Word : Acknowledgement \n",
      " Enumeration : 0 1 2 3 4 5 6 7 8 9 10 11 12 \n",
      " Reference : [ EP AE K N AA L IH JH M AH N T /EP ] \n",
      " Prediction : [ EP IH K N AA L IH JH IH JH AH N T /EP ] \n",
      " Types of errors : S S S I S I \n",
      " Position : [ 1,8,8 ] \n",
      " As example the example shows, two substitutions substitutions (S) and (I) occurred in our fifth model output. \n",
      "\n",
      " Figure shows position errors calculated the models models on on the the reference [reference dataset]. \n",
      "\n",
      " Furthermore, in all models presented here, PER is better than the previous results on [CMUDict] except the first four models in, while WER is still reasonable. \n",
      " This means that even most of the incorrect predictions are very close to the reference ; therefore, they have small PER. \n",
      " Accordingly, we need to analyze the incorrect predictions (outputs) for each model to see how many [phonemes] are correct in the reference. \n",
      " In the fifth model, 25.3 % of the [test data] are not correct (about 3000 test samples). \n",
      " After the analysis of these predictions, more than half of them have 1 incorrect [phoneme]. \n",
      " In particular, the PER for 59 test samples is higher than 50 % (11 test samples are greater than 60 %, and only 1 test sample is more than 70 %). \n",
      " These percentages in the other presented models are more or less the same. \n",
      " Generally, the same 1000 words are incorrectly predicted by all presented models. \n",
      " We can see different types of error when generating [phoneme sequences]. \n",
      " One of these errors is that some [phonemes] are unnecessarily generated multiple times. \n",
      " For example, for the word YELLOWKNIFE, reference is [ Y EH L OWN AY F ], the prediction of Model 5 for this word is [ Y EH L OW K N N F ], where the character N was generated twice. \n",
      " Another error type regards [sequences of graphemes] that are rarely represented in the training process. \n",
      " For example, for the word ZANGHI Model 5 output is [ Z AE N G ], while the reference is [ Z AA N G IY ]. \n",
      " The [graphemes] â NGHIâ appeared only 7 times in the [training data]. \n",
      " Furthermore, many words are of foreign origin, for example, GDANSK is Polish a city, SCICCHITANO is an Italian name, KOVACIK is a Turkish surname. \n",
      " Generating [phoneme sequences] of abbreviations is one of the hard challenges. \n",
      " For example, LPN, INES are shown with their references and the prediction form of Model 5 in Table :     \n",
      "\n",
      " Table. Examples of errors predicted by Model 5. \n",
      "\n",
      " In the proposed models, we were able to achieve smaller PERs with different hyperparameter settings, but WERs showed different behavior, in contrast with what we expected. \n",
      " To calculate WER, the number of word errors is divided by the total number of unique words in the reference. \n",
      " These word errors are counted only if the predicted pronunciation does not match any reference pronunciation. \n",
      " Therefore, in the generated [phoneme sequences] of words that contained errors, there is at least one [phoneme] error. \n",
      " For that reason, we calculated the number of word errors depending on the number of [phoneme] errors for all proposed models on [CMUDict], as presented in Figure. \n",
      "\n",
      " Figure. Number of word errors depending on the number of [phoneme] errors for all models. \n",
      "\n",
      " In the case of each model, there are twice as many words with only one [phoneme] error than words with two [phoneme] errors. \n",
      " Words with one [phoneme] error significantly effect the WER. \n",
      " The number of words with two [phoneme] errors was the greatest in Model 4 (908), and the lowest in Model 5 (739). \n",
      " The number of words with three [phoneme] errors was the lowest (230) in Model 5. \n",
      " There was approximately the same number of words with four [phoneme] errors in Model 2 and Model 5 (84 in Model 2 and 86 in Model 5). \n",
      " There were very few words with five or more [phoneme] errors in any of the models. \n",
      " Model 1 and Model 3 have only 1 word which has seven [phoneme] errors ; Model 5 has 2 words ; Model 4 has 6 words. \n",
      " The number of words with eight [phoneme] errors was 0 in Model 3 and Model 5 ; 1 in Model 4. \n",
      " Figure helps to understand why PER in our models can be smaller while WER is higher.    \n",
      "\n",
      " Conclusions \n",
      "     \n",
      " In this paper, [convolutional neural networks] for [grapheme-to-phoneme conversion] are introduced. \n",
      " Five different models for the [G2P] task are described, and the results are compared to previously reported state-of-the-art research. \n",
      " Our models are based on the [seq2seq] architecture, and in the fourth and fifth models, we applied CNNs with [residual connections]. \n",
      " The fifth model, which uses [convolutional layers] with [residual connections] as encoder and [Bi-LSTM] as decoder outperformed most the previous solutions on the [CMUDict] and [NetTalk datasets] in terms of PER. \n",
      " Furthermore, the fourth model, which contains [convolutional layers] only, is significantly faster than other models and still has competitive accuracy. \n",
      " Our solution achieved these results without explicit alignments. \n",
      " The experiments were conducted on a test set corresponding to 9.8 % and 24.9 % of the whole [CMUDict] and [NetTalk databases], respectively. \n",
      " The same test set was used in all cases, so we consider the results to be comparable. \n",
      " To draw conclusions on whether one model is better than another, the goal must be defined. \n",
      " If inference time is crucial, then smaller model sizes are favorable (e.g., Model 4), but if lower WER and PER are the main factors, then Model 5 outperforms the others. \n",
      " The results presented in this paper can be applied in [TTS systems] ; however, because of the rapid development of deep learning further aspects will be investigated, like dilated [convolutional networks] and neural architecture search. \n",
      " These are possible further extensions of the current research. \n",
      " Author Contributions : All authors have read and approved the final manuscript. \n",
      "\n",
      " Acknowledgments : The research presented in this paper has been supported by the European Union, co-financed by the European Social Fund (EFOP-3.6.2-16-2017-00013), by the BME-Artificial Intelligence FIKP grant of Ministry of Human Resources (BME FIKP-MI / SC), by Doctoral Research Scholarship of Ministry of Human Resources (ÃNKP-18-4-BME-394) in the scope of New National Excellence Program, by JÃ¡nos Bolyai Research Scholarship of the Hungarian Academy of Sciences, by the VUK project (AAL 2014-183), and the DANSPLAT project (Eureka 9944). \n",
      " We gratefully acknowledge the support of NVIDIA Corporation with the donation of the Titan Xp GPU used for this research. \n",
      "\n",
      " Conflicts of Interest : The authors declare that they have no competing interests. \n",
      "\n",
      " Abbreviations \n",
      " [G2P]             [Grapheme-to-phoneme] \n",
      " [ASR]             [Automatic Speech Recognition] \n",
      " CNN             [Convolutional neural network] \n",
      " PER             [Phoneme error rate] \n",
      " WER             [Word error rate] \n",
      " [Bi-LSTM]         bi-directional Long Short Term Memory \n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__== \"__main__\":\n",
    "    \"\"\"Main -> to modify by putting all steps in one fonction\"\"\"\n",
    "    init_data = read_data('tts-lexicon4.tsv')\n",
    "    change_lemma = lemma_lexicon(init_data)\n",
    "    data = select_data(change_lemma)\n",
    "    text_dataframe = lemma_posttag('/home/macaire/Bureau/M2_NLP/Terminology/terminology_project/tts-articles/txt/11.txt')\n",
    "#     text_dataframe = lemma_posttag('test2.txt')\n",
    "    annotate(data, text_dataframe)\n",
    "#     text_dataframe.to_csv(r'pandatext.txt', header=None, index=None, sep=' ', mode='a')\n",
    "#     print(text_dataframe.tail(50))\n",
    "#     print(data.head(50))\n",
    "#     print(' '.join(text_dataframe['tokens'].to_list()))\n",
    "    print(construct_annotated_text(text_dataframe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
