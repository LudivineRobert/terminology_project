[Grapheme PROPN - to ADP - Phoneme PROPN (B) Conversion NOUN (I)] with ADP (O) [Convolutional PROPN (B) Neural PROPN (I) Networks NOUN (I)] 
      


 Abstract PROPN (O) : [Grapheme PROPN - to ADP - phoneme NOUN (B) (G2P PROPN) (I) conversion NOUN (I)] is AUX (O) the DET (O) process NOUN (O) of ADP (O) generating NOUN (O) pronunciation NOUN (O) for ADP (O) words NOUN (O) based VERB (O) on ADP (O) their DET (O) written VERB (O) form NOUN . (O) 
It PRON (O) has AUX (O) a NOUN (O) highly ADV (O) essential ADJ (O) role NOUN (O) for ADP (O) [natural ADJ (B) language NOUN (I) processing NOUN (I)] , [text NOUN - to ADP - speech NOUN (B) synthesis NOUN (I)] and CCONJ (O) [automatic PROPN (B) speech NOUN (I) recognition NOUN (I) systems NOUN (I)] . 
In ADP (O) this DET (O) paper NOUN , (O) we PRON (O) investigate VERB (O) [convolutional NOUN (B) neural NOUN (I) networks NOUN (I)] ([CNN PROPN (B)]) for ADP (O) [G2P NOUN (B) conversion NOUN (I)] . 
We PRON (O) propose NOUN (O) a NOUN (O) novel NOUN (O) [CNN PROPN - based VERB (B) sequence NOUN - to ADP - sequence NOUN (I) (seq2seq NOUN) (I) architecture NOUN (I)] for ADP (O) [G2P NOUN (B) conversion NOUN (I)] . 
Our DET (O) approach NOUN (O) includes VERB (O) an DET (O) [end NOUN - to ADP - end NOUN (B) CNN PROPN (I) G2P PROPN (I) conversion NOUN (I)] with ADP (O) [residual ADJ (B) connections NOUN (I)] and CCONJ , (O) furthermore ADV , (O) a NOUN (O) model NOUN (O) that SCONJ (O) utilizes VERB (O) a NOUN (O) [convolutional NOUN (B) neural NOUN (I) network NOUN (I)] (with ADP (O) and CCONJ (O) without ADP (O) [residual ADJ (B) connections NOUN (I)]) as SCONJ (O) [encoder NOUN (B)] and CCONJ (O) [Bi PROPN - LSTM PROPN (B)] as SCONJ (O) a NOUN (O) [decoder NOUN (B)] . 
We PRON (O) compare VERB (O) our DET (O) approach NOUN (O) with ADP (O) state NOUN - of ADP - the DET - art NOUN (O) methods NOUN , (O) including VERB (O) [Encoder PROPN - Decoder PROPN (B) LSTM PROPN (I)] and CCONJ (O) [Encoder PROPN - Decoder PROPN (B) Bi PROPN - LSTM PROPN (I)] . 
Training NOUN (O) and CCONJ (O) inference NOUN (O) times NOUN , (O) [phoneme NOUN (B)] and CCONJ (O) [word NOUN (B) error NOUN (I) rates NOUN (I)] were AUX (O) evaluated VERB (O) on ADP (O) the DET (O) public NOUN (O) [CMUDict NOUN (B) dataset NOUN (I)] for ADP (O) US PROPN (O) English PROPN , (O) and CCONJ (O) the DET (O) best ADJ (O) performing VERB (O) [convolutional NOUN (B) neural NOUN (I) network NOUN - based VERB (I) architecture NOUN (I)] was AUX (O) also ADV (O) evaluated VERB (O) on ADP (O) the DET (O) [NetTalk PROPN (B) dataset NOUN (I)] . 
Our DET (O) method NOUN (O) approaches VERB (O) the DET (O) accuracy NOUN (O) of ADP (O) previous ADJ (O) state NOUN - of ADP - the DET - art NOUN (O) results VERB (O) in ADP (O) terms NOUN (O) of ADP (O) [phoneme NOUN (B) error NOUN (I) rate NOUN (I)] . 
Keywords PROPN (O) : [grapheme NOUN - to ADP - phoneme NOUN (B)] ([G2P NOUN (B)]) ; [encoder NOUN - decoder NOUN (B)] ; [LSTM PROPN (B)] ; [1D PROPN (B) convolution NOUN (I)] ; [Bi PROPN - LSTM PROPN (B)] ; residual ADJ (O) architecture NOUN (O) 



 Introduction NOUN (O) 
 
 The DET (O) process NOUN (O) of ADP (O) [grapheme NOUN - to ADP - phoneme NOUN (B) (G2P PROPN) (I) conversion NOUN (I)] generates VERB (O) a NOUN (O) [phonetic NOUN (B) transcription NOUN (I)] from ADP (O) the DET (O) written VERB (O) form NOUN (O) of ADP (O) words NOUN . (O) 
The DET (O) spelling NOUN (O) of ADP (O) a NOUN (O) word NOUN (O) is AUX (O) called VERB (O) a NOUN (O) [grapheme NOUN (B) sequence NOUN (I)] (or CCONJ (O) [graphemes NOUN (B)]) , the DET (O) [phonetic NOUN (B) form NOUN (I)] is AUX (O) called VERB (O) a NOUN (O) [phoneme NOUN (B) sequence NOUN (I)] (or CCONJ (O) [phonemes NOUN (B)]) . 
It PRON (O) is AUX (O) essential ADJ (O) to ADP (O) develop VERB (O) a NOUN (O) [phonemic NOUN (B) lexicon PROPN (I)] in ADP (O) [text NOUN - to ADP - speech NOUN (B)] ([TTS PROPN (B)]) and CCONJ (O) [automatic PROPN (B) speech NOUN (I) recognition NOUN (I) (ASR PROPN) (I) systems NOUN (I)] . 
For ADP (O) this DET (O) purpose NOUN , (O) [G2P NOUN (B) techniques NOUN (I)] are AUX (O) used VERB , (O) and CCONJ (O) getting VERB (O) state NOUN - of ADP - the DET - art NOUN (O) performance NOUN (O) in ADP (O) these DET (O) systems NOUN (O) depends VERB (O) on ADP (O) the DET (O) accuracy NOUN (O) of ADP (O) [G2P NOUN (B) conversion NOUN (I)] . 
For ADP (O) instance NOUN , (O) in ADP (O) [ASR PROPN (B) acoustic ADJ (I) models NOUN (I)] , the DET (O) [pronunciation NOUN (B) lexicons NOUN (I)] and CCONJ (O) language NOUN (O) models NOUN (O) are AUX (O) critical ADJ (O) components NOUN . (O) 
Acoustic PROPN (O) and CCONJ (O) language NOUN (O) models NOUN (O) are AUX (O) built VERB (O) automatically ADV (O) from ADP (O) large ADJ (O) corpora PROPN . (O) 
[Pronunciation NOUN (B) lexicons NOUN (I)] are AUX (O) the DET (O) middle NOUN (O) layer NOUN (O) between ADP (O) acoustic ADJ (O) and CCONJ (O) language NOUN (O) models NOUN . (O) 
For ADP (O) a NOUN (O) new ADJ (O) [speech NOUN (B) recognition NOUN (I) task NOUN (I)] , the DET (O) performance NOUN (O) of ADP (O) the DET (O) overall NOUN (O) system NOUN (O) depends VERB (O) on ADP (O) the DET (O) quality NOUN (O) of ADP (O) the DET (O) pronunciation NOUN (O) component NOUN . (O) 
In ADP (O) other ADJ (O) words NOUN , (O) the DET (O) system NOUN ’s PUNCT (O) performance NOUN (O) depends VERB (O) on ADP (O) [G2P NOUN (B) accuracy NOUN (I)] . 
For ADP (O) example NOUN , (O) the DET (O) [G2P NOUN (B) conversion NOUN (I)] of ADP (O) word NOUN (O) ‘ PUNCT (O) speaker NOUN ’ PUNCT (O) is AUX (O) ‘S NOUN (O) P NOUN (O) IY NOUN (O) K NOUN (O) ER PROPN ’ PUNCT . (O) 
In ADP (O) [TTS PROPN (B)] systems NOUN , (O) a NOUN (O) [high ADJ - quality NOUN (B) G2P PROPN (I) model NOUN (I)] is AUX (O) also ADV (O) an DET (O) essential ADJ (O) part NOUN (O) and CCONJ (O) has AUX (O) a NOUN (O) great ADJ (O) influence NOUN (O) on ADP (O) the DET (O) overall NOUN (O) quality NOUN . (O) 
Inaccurate PROPN (O) [G2P NOUN (B) conversion NOUN (I)] results VERB (O) in ADP (O) unnatural NOUN (O) pronunciation NOUN (O) or CCONJ (O) even ADV (O) incomprehensible NOUN (O) [synthetic NOUN (B) speech NOUN (I)] . 

 Previous ADJ (O) Works PROPN (O) 

 [G2P NOUN (B) conversion NOUN (I)] has AUX (O) been AUX (O) studied VERB (O) for ADP (O) a NOUN (O) long ADJ (O) time NOUN . (O) 
[Rule NOUN - based VERB (B) G2P PROPN (I) systems NOUN (I)] use VERB (O) a NOUN (O) wide ADJ (O) set NOUN (O) of ADP (O) [grapheme NOUN - to ADP - phoneme NOUN (B) rules NOUN (I)] . 
Developing VERB (O) such ADJ (O) a NOUN (O) [G2P NOUN (B) system NOUN (I)] requires VERB (O) linguistic ADJ (O) expertise NOUN . (O) 
Additionally ADV , (O) some DET (O) languages NOUN (O) (such ADJ (O) as SCONJ (O) Chinese PROPN (O) and CCONJ (O) Japanese PROPN) (O) have AUX (O) complex NOUN (O) writing NOUN (O) systems NOUN , (O) and CCONJ (O) building NOUN (O) the DET (O) rules VERB (O) is AUX (O) labor NOUN - intensive ADJ (O) and CCONJ (O) it PRON (O) is AUX (O) extremely ADV (O) difficult ADJ (O) to ADP (O) cover NOUN (O) most ADJ (O) possible ADJ (O) situations NOUN . (O) 
Furthermore ADV , (O) these DET (O) systems NOUN (O) are AUX (O) sensitive ADJ (O) to ADP (O) out NOUN (O) of ADP (O) vocabulary NOUN (O) (OOV PROPN) (O) events NOUN . (O) 
Other PROPN (O) previous ADJ (O) solutions NOUN (O) used VERB (O) joint NOUN (O) sequence NOUN (O) models NOUN . (O) 
These DET (O) models NOUN (O) create VERB (O) an DET (O) initial NOUN (O) [grapheme PROPN - phoneme PROPN (B) sequence NOUN (I) alignment NOUN (I)] , and CCONJ (O) by ADP (O) using VERB (O) this DET (O) alignment NOUN , (O) it PRON (O) calculates NOUN (O) a NOUN (O) joint NOUN (O) [n PROPN - gram PROPN (B) language NOUN (I) model NOUN (I)] over ADP (O) sequences NOUN . (O) 
The DET (O) method NOUN (O) proposed VERB (O) by ADP (O) is AUX (O) implemented VERB (O) in ADP (O) the DET (O) publicly ADV (O) available ADJ (O) tool NOUN (O) Sequitur PROPN (O) (https://www VERB - i6.informatik.rwth PROPN - aachen.de PROPN (O) / web NOUN (O) / Software NOUN (O) / g2p.html PROPN , (O) Access NOUN (O) date NOUN (O) : 9th NOUN (O) August PROPN (O) 2018 NUM) . (O) 
In ADP (O) one NUM - to ADP - one NUM (O) alignment NOUN , (O) each DET (O) [grapheme NOUN (B)] corresponds VERB (O) to ADP (O) only ADV (O) one NUM (O) [phoneme NOUN (B)] , and CCONJ (O) vice NOUN (O) versa NOUN . (O) 
An PROPN (O) “ PUNCT (O) empty ADJ (O) ” PUNCT (O) symbol NOUN (O) is AUX (O) introduced VERB (O) to ADP (O) match NOUN (O) [grapheme NOUN (B) and CCONJ (I) phoneme PROPN (I) sequences NOUN (I)] . 
For ADP (O) example NOUN , (O) the DET (O) [grapheme NOUN (B) sequence NOUN (I)] of ADP (O) ‘ PUNCT (O) CAKE PROPN ’ PUNCT (O) matches VERB (O) the DET (O) [phoneme NOUN (B) sequence NOUN (I)] of ADP (O) ‘ PUNCT (O) K PROPN (O) EY PROPN (O) K NOUN ’ PUNCT , (O) and CCONJ (O) one NUM - to ADP - one NUM (O) alignment NOUN (O) of ADP (O) these DET (O) sequences NOUN (O) is AUX (O) C NOUN (O) → NOUN (O) K PROPN , (O) A NOUN (O) → NOUN (O) EY PROPN , (O) K NOUN (O) → NOUN (O) K PROPN , (O) and CCONJ (O) the DET (O) last ADJ (O) [grapheme NOUN (B)] ‘ PUNCT (O) E NOUN ’ PUNCT (O) matches VERB (O) the DET (O) “ PUNCT (O) empty ADJ (O) ” PUNCT (O) symbol NOUN . (O) 
Conditional PROPN (O) and CCONJ (O) joint NOUN (O) maximum NOUN (O) entropy NOUN (O) models NOUN (O) use NOUN (O) this DET (O) approach NOUN . (O) 
Later ADV , (O) Hidden PROPN (O) [Conditional PROPN (B) Random PROPN (I) Field NOUN (I) (HCRF PROPN) (I) models NOUN (I)] were AUX (O) introduced VERB (O) in ADP (O) which DET (O) the DET (O) alignment NOUN (O) between ADP (O) [grapheme NOUN (B) and CCONJ (I) phoneme PROPN (I) sequence NOUN (I)] is AUX (O) modelled VERB (O) with ADP (O) hidden VERB (O) variables NOUN . (O) 
The DET (O) [HCRF PROPN (B) models NOUN (I)] usually ADV (O) lead NOUN (O) to ADP (O) very ADV (O) competitive ADJ (O) results NOUN (O) ; however ADV , (O) the DET (O) training NOUN (O) of ADP (O) such ADJ (O) models NOUN (O) is AUX (O) very ADV (O) memory NOUN (O) and CCONJ (O) computationally ADV (O) intensive ADJ . (O) 
A NOUN (O) further NOUN (O) approach NOUN (O) utilizes VERB (O) [conditional ADJ (B) random ADJ (I) fields NOUN (I)] ([CRF NOUN (B)]) and CCONJ (O) Segmentation NOUN (O) / Tagging NOUN (O) models NOUN (O) (such ADJ (O) as SCONJ (O) linear NOUN (O) [finite ADJ - state NOUN (B) automata NOUN (I)] or CCONJ (O) [transducers NOUN (B)] , [FSTs NOUN (B)]) , then ADV (O) use NOUN (O) them PRON (O) in ADP (O) two NUM (O) different ADJ (O) compositions NOUN . (O) 
The DET (O) first ADJ (O) composition NOUN (O) is AUX (O) a NOUN (O) joint NOUN - multigram NOUN (O) combined VERB (O) with ADP (O) [CRF NOUN (B)] ; the DET (O) second NOUN (O) one NUM (O) is AUX (O) a NOUN (O) joint NOUN - multigram NOUN (O) combined VERB (O) with ADP (O) Segmentation NOUN (O) / Tagging VERB . (O) 
The DET (O) first ADJ (O) approach NOUN (O) achieved VERB (O) 5.5 NUM (O) % [phoneme NOUN (B) error NOUN (I) rate NOUN (I)] (PER PROPN) (O) on ADP (O) [CMUDict NOUN (B)] . 
Recently ADV , (O) [neural NOUN (B) networks NOUN (I)] have AUX (O) been AUX (O) applied VERB (O) for ADP (O) [G2P NOUN (B) conversion NOUN (I)] . 
[Neural PROPN (B) network NOUN - based VERB (I) G2P PROPN (I) conversion NOUN (I)] is AUX (O) robust ADJ (O) against ADP (O) spelling NOUN (O) mistakes NOUN (O) and CCONJ (O) OOV PROPN (O) words NOUN (O) ; it PRON (O) generalizes VERB (O) well INTJ . (O) 
Also ADV , (O) it PRON (O) can VERB (O) be AUX (O) seamlessly ADV (O) integrated VERB (O) into ADP (O) [end NOUN - to ADP - end NOUN (B) TTS PROPN (I)] / [ASR PROPN (B)] systems NOUN (O) (that SCONJ (O) are AUX (O) constructed VERB (O) entirely ADV (O) of ADP (O) [deep ADJ (B) neural NOUN (I) networks NOUN (I)]) . 
In ADP (O) this DET (O) paper NOUN , (O) a NOUN (O) [TTS PROPN (B) system NOUN (I)] ([Deep ADJ (B) Voice PROPN (I)]) is AUX (O) presented VERB (O) which DET (O) was AUX (O) constructed VERB (O) entirely ADV (O) from ADP (O) [deep ADJ (B) neural NOUN (I) networks NOUN (I)] . 
[Deep ADJ (B) Voice PROPN (I)] lays VERB (O) the DET (O) groundwork NOUN (O) for ADP (O) truly ADV (O) [end NOUN - to ADP - end NOUN (B) neural NOUN (I) speech NOUN (I) synthesis NOUN (I)] . 
Thus ADV , (O) the DET (O) [G2P NOUN (B) model NOUN (I)] is AUX (O) jointly ADV (O) trained VERB (O) with ADP (O) further NOUN (O) essential ADJ (O) parts NOUN (O) of ADP (O) the DET (O) [speech NOUN (B) synthesizer NOUN (I)] and CCONJ (O) recognizer PROPN , (O) which DET (O) increase NOUN (O) the DET (O) overall NOUN (O) quality NOUN (O) of ADP (O) the DET (O) system NOUN . (O) 
[LSTM PROPN (B)] has AUX (O) shown VERB (O) competitive ADJ (O) performance NOUN (O) in ADP (O) various ADJ (O) fields NOUN , (O) like INTJ (O) acoustic ADJ (O) modelling NOUN (O) and CCONJ (O) language NOUN (O) understanding NOUN . (O) 
One NUM (O) of ADP (O) the DET (O) early ADV (O) neural NOUN (O) approaches VERB (O) investigates VERB (O) unidirectional ADJ (O) Long ADV (O) Short ADJ - Term NOUN (O) Memory NOUN (O) (ULSTM PROPN) (O) with ADP (O) full ADJ (O) output NOUN (O) delays NOUN , (O) which DET (O) achieved VERB (O) 9.1 NUM (O) % [phoneme NOUN (B) error NOUN (I) rate NOUN (I)] . 
In ADP (O) the DET (O) same ADJ (O) paper NOUN , (O) a NOUN (O) deep ADJ (O) [bidirectional ADJ (B) LSTM PROPN (I)] ([DBLSTM NOUN (B)]) was AUX (O) combined VERB (O) with ADP (O) [connectionist NOUN (B) temporal NOUN (I) classification NOUN (I)] (CTC PROPN) (O) and CCONJ (O) joint NOUN (O) [n PROPN - gram PROPN (B) models NOUN (I)] for ADP (O) better ADJ (O) accuracy NOUN (O) (21.3 NUM (O) % [word NOUN (B) error NOUN (I) rate NOUN (I)]) . 
Please INTJ (O) note NOUN (O) that SCONJ (O) CTC PROPN (O) objective NOUN (O) function NOUN (O) was AUX (O) introduced VERB (O) to ADP (O) infer NOUN (O) [speech NOUN (B)]-label PROPN alignments NOUN (O) automatically ADV (O) without ADP (O) any DET (O) intermediate ADJ (O) process NOUN , (O) leading VERB (O) to ADP (O) an DET (O) [end NOUN - to ADP - end NOUN (B) approach NOUN (I)] for ADP (O) [ASR PROPN (B)] . 
CTC PROPN (O) technique NOUN (O) has AUX (O) combined VERB (O) with ADP (O) [CNN PROPN (B)] , [LSTM PROPN (B)] for ADP (O) the DET (O) various ADJ (O) [speech NOUN - related VERB (B) tasks NOUN (I)] . 
Due PROPN (O) to ADP (O) utilizing VERB (O) an DET (O) [encoder NOUN - decoder NOUN (B) approach NOUN (I)] for ADP (O) the DET (O) [G2P NOUN (B) task NOUN (I)] , a DET (O) separate ADJ (O) alignment NOUN (O) between ADP (O) [grapheme NOUN (B)] sequences NOUN (O) and CCONJ (O) [phoneme NOUN (B) sequences NOUN (I)] became VERB (O) unnecessary ADJ . (O) 
Alignment PROPN - based VERB (O) models NOUN (O) of ADP (O) unidirectional ADJ (O) [LSTM PROPN (B)] with ADP (O) one NUM (O) layer NOUN (O) and CCONJ (O) [bi ADJ - directional ADJ (B) LSTM PROPN (I)] ([Bi PROPN - LSTM PROPN (B)]) with ADP (O) one NUM , (O) two NUM (O) and CCONJ (O) three NUM (O) layers NOUN (O) have AUX (O) also ADV (O) previously ADV (O) been AUX (O) investigated VERB . (O) 
In ADP (O) this DET (O) work NOUN , (O) alignment NOUN (O) was AUX (O) explicitly ADV (O) modelled VERB (O) in ADP (O) the DET (O) [G2P NOUN (B) conversion NOUN (I)] process NOUN (O) by ADP (O) the DET (O) context NOUN (O) of ADP (O) the DET (O) [grapheme NOUN (B)] . 
A NOUN (O) further NOUN (O) work NOUN , (O) which DET (O) applies VERB (O) deep ADJ (O) [bi ADJ - directional ADJ (B) LSTM PROPN (I)] with ADP (O) [hyperparameter NOUN (B) optimization NOUN (I)] (including VERB (O) the DET (O) number NOUN (O) of ADP (O) hidden VERB (O) layers NOUN , (O) optional PROPN (O) linear NOUN (O) projection NOUN (O) layers NOUN , (O) optional PROPN (O) splicing NOUN (O) window NOUN (O) at ADP (O) the DET (O) input NOUN) (O) considered VERB (O) various ADJ (O) alignment NOUN (O) schemes NOUN . (O) 
The DET (O) best ADJ (O) model NOUN (O) with ADP (O) [hyperparameter NOUN (B) optimization NOUN (I)] achieved VERB (O) a NOUN (O) 5.37 NUM (O) % [phoneme NOUN (B) error NOUN (I) rate NOUN (I)] (PER PROPN) (O) and CCONJ (O) a NOUN (O) 23.23 NUM (O) % [word NOUN (B) error NOUN (I) rate NOUN (I)] ([WER NOUN (B)]) . 
[Multi ADJ - layer NOUN (B) bidirectional ADJ (I) encoder NOUN (I)] with ADP (O) [gated NOUN (B) recurrent NOUN (I) units NOUN (I)] ([GRU PROPN (B)]) and CCONJ (O) deep ADJ (O) unidirectional ADJ (O) [GRU PROPN (B)] as SCONJ (O) a NOUN (O) [decoder NOUN (B)] achieved VERB (O) 5.8 NUM (O) % PER PROPN (O) and CCONJ (O) 28.7 NUM (O) % [WER PROPN (B)] on ADP (O) [CMUDict NOUN (B)] . 
[Convolutional PROPN (B) neural NOUN (I) networks NOUN (I)] have AUX (O) achieved VERB (O) superior PROPN (O) performance NOUN (O) compared VERB (O) to ADP (O) previous ADJ (O) methods NOUN (O) in ADP (O) large ADJ - scale NOUN (O) image NOUN (O) recognition NOUN . (O) 
Recently ADV , (O) these DET (O) architectures VERB (O) were AUX (O) also ADV (O) applied VERB (O) to ADP (O) [Natural PROPN (B) Language NOUN (I) Processing NOUN (I) (NLP PROPN) (I) tasks NOUN (I)] , including VERB (O) sentence NOUN (O) classifications NOUN (O) and CCONJ (O) [neural NOUN (B) machine NOUN (I) translation NOUN (I)] . 
Nowadays ADV , (O) completely ADV (O) [convolutional NOUN (B) neural NOUN (I) networks NOUN (I)] may VERB (O) achieve VERB (O) superior PROPN (O) results NOUN (O) compared VERB (O) to ADP (O) recurrent NOUN (O) solutions NOUN . (O) 
[Sequence NOUN - to ADP - sequence NOUN (B) (seq2seq NOUN) (I) learning VERB (I)] , or CCONJ (O) [encoder NOUN - decoder NOUN (B) type NOUN (I) neural PROPN (I) networks NOUN (I)] have AUX (O) achieved VERB (O) remarkable ADJ (O) success NOUN (O) in ADP (O) various ADJ (O) tasks NOUN , (O) such ADJ (O) as SCONJ (O) [speech NOUN (B) recognition NOUN (I)] , [text NOUN - to ADP - speech NOUN (B) synthesis NOUN (I)] , [machine NOUN (B) translation NOUN (I)] . 
This DET (O) type NOUN (O) of ADP (O) network NOUN (O) is AUX (O) used VERB (O) for ADP (O) several ADJ (O) tasks NOUN , (O) and CCONJ (O) its DET (O) performance NOUN (O) has AUX (O) also ADV (O) been AUX (O) enhanced VERB (O) with ADP (O) [attention NOUN (B) mechanisms NOUN (I)] . 
In ADP (O) this DET (O) structure NOUN , (O) the DET (O) [encoder NOUN (B)] computes VERB (O) a NOUN (O) representation NOUN (O) of ADP (O) each DET (O) input NOUN (O) sequence NOUN , (O) and CCONJ (O) the DET (O) [decoder NOUN (B)] generates VERB (O) an DET (O) output NOUN (O) sequence NOUN (O) based VERB (O) on ADP (O) the DET (O) learned VERB (O) representation NOUN . (O) 
In ADP (O) (28 NUM) , (O) [bidirectional ADJ (B) multi ADJ - layer ADJ (I) recurrent ADJ (I) neural PROPN (I) network NOUN - based VERB (I) seq2seq NOUN (I) learning VERB (I)] was AUX (O) investigated VERB (O) in ADP (O) two NUM (O) architectures VERB (O) : a DET (O) single ADJ (O) [Bi PROPN - LSTM PROPN (B)] / [Bidirectional ADJ (B) Gated PROPN (I) Recurrent ADJ (I) Unit NOUN (I)] ([Bi PROPN - GRU PROPN) (B) layer NOUN (I)] and CCONJ (O) two NUM (O) [Bi PROPN - LSTM PROPN (B)] / [Bi PROPN - GRU PROPN (B) layers NOUN (I)] . 
Both DET (O) [Bi PROPN - LSTM PROPN (B)] and CCONJ (O) [Bi PROPN - GRU PROPN (B)] uses VERB (O) both DET (O) past ADJ (O) and CCONJ (O) future NOUN (O) contexts PROPN . (O) 
Moreover ADV , (O) a NOUN (O) [bidirectional ADJ (B) decoder NOUN (I)] was AUX (O) proposed VERB (O) for ADP (O) [neural NOUN (B) machine NOUN (I) translation NOUN (I)] ([NMT PROPN (B)]) in ADP . (O) 
Both DET (O) [encoder NOUN (B)] and CCONJ (O) [decoder NOUN (B)] are AUX (O) [Bi PROPN - GRU PROPN (B)] , but CCONJ (O) this DET (O) model NOUN (O) is AUX (O) applicable ADJ (O) to ADP (O) other ADJ (O) [RNNs PROPN (B)] , such ADJ (O) as SCONJ (O) [LSTM PROPN (B)] . 
By ADP (O) introducing VERB (O) a NOUN (O) [backward NOUN (B) decoder NOUN (I)] , the DET (O) purpose NOUN (O) of ADP (O) which DET (O) is AUX (O) to ADP (O) exploit VERB (O) reverse NOUN (O) target NOUN - side NOUN (O) contexts PROPN , (O) the DET (O) results VERB (O) of ADP (O) [NMT PROPN (B) task NOUN (I)] was AUX (O) improved VERB . (O) 
For ADP (O) [speech NOUN (B) recognition NOUN (I)] , several ADJ (O) [sequence NOUN - to ADP - sequence NOUN (B) models NOUN (I)] , including VERB (O) [connectionist NOUN (B) temporal NOUN (I) classification NOUN (I)] (CTC PROPN) , (O) the DET (O) [recurrent NOUN (B) neural NOUN (I) network NOUN (I)] ([RNN PROPN (B)]) [transducer NOUN (B)] , and CCONJ (O) an DET (O) [attention NOUN - based VERB (B) model NOUN (I)] , have AUX (O) been AUX (O) analyzed VERB . (O) 
The DET (O) basics NOUN (O) of ADP (O) sequence NOUN (O) modelling NOUN (O) with ADP (O) [convolutional NOUN (B) networks NOUN (I)] are AUX (O) summarized VERB (O) in ADP . (O) 
Furthermore ADV , (O) the DET (O) key NOUN (O) components NOUN (O) of ADP (O) the DET (O) [temporal NOUN (B) convolution NOUN (I) network NOUN (I)] ([TCN PROPN (B)]) have AUX (O) also ADV (O) been AUX (O) introduced VERB , (O) and CCONJ (O) some DET (O) vital ADJ (O) advantages NOUN (O) and CCONJ (O) disadvantages NOUN (O) of ADP (O) using VERB (O) [TCN PROPN (B)] for ADP (O) sequence NOUN (O) predictions NOUN (O) instead ADV (O) of ADP (O) [RNNs PROPN (B)] were AUX (O) analyzed VERB (O) as SCONJ (O) well INTJ . (O) 
The DET (O) [encoder NOUN - decoder NOUN (B) structure NOUN (I)] has AUX (O) been AUX (O) studied VERB (O) for ADP (O) the DET (O) [G2P NOUN (B) task NOUN (I)] before ADP , (O) but CCONJ (O) usually ADV , (O) [LSTM PROPN (B)] and CCONJ (O) [GRU PROPN (B) networks NOUN (I)] have AUX (O) been AUX (O) involved VERB . (O) 
For ADP (O) example NOUN , (O) Baidu PROPN ’s PART (O) [end NOUN - to ADP - end NOUN (B) text NOUN - to ADP - speech NOUN (I) synthesizer PROPN (I)] , called VERB (O) [Deep ADJ (B) Voice PROPN (I)] , uses VERB (O) the DET (O) [multi ADJ - layer ADJ (B) bidirectional ADJ (I) encoder NOUN (I)] with ADP (O) [GRU PROPN (B)] ’s PART non ADJ - linearity ADJ (O) and CCONJ (O) an DET (O) equally ADV (O) [deep ADJ (B) unidirectional NOUN (I) GRU PROPN (I) decoder NOUN (I)] . 
Until ADP (O) now ADV , (O) the DET (O) best ADJ (O) result NOUN (O) for ADP (O) [G2P NOUN (B) conversion NOUN (I)] was AUX (O) introduced VERB (O) by ADP , (O) which DET (O) applied VERB (O) an DET (O) [attention NOUN - enabled VERB (B) encoder NOUN - decoder NOUN (I) model NOUN (I)] and CCONJ (O) achieved VERB (O) 4.69 NUM (O) % PER PROPN (O) and CCONJ (O) 20.24 NUM (O) % [WER PROPN (B)] on ADP (O) [CMUDict NOUN (B)] . 
Furthermore ADV , (O) [G2P NOUN - seq2seq NOUN (B)] (https://github.com/cmusphinx/g2p-seq2seq NUM , (O) Access NOUN (O) date NOUN (O) : 9th NOUN (O) August PROPN (O) 2018 NUM) (O) is AUX (O) based VERB (O) on ADP (O) [neural NOUN (B) networks NOUN (I)] implemented VERB (O) in ADP (O) the DET (O) TensorFlow PROPN (O) framework NOUN (O) with ADP (O) 20.6 NUM (O) % [WER PROPN (B)] . 
To PART (O) the DET (O) best ADJ (O) of ADP (O) our DET (O) knowledge NOUN , (O) our DET (O) approach NOUN (O) is AUX (O) the DET (O) first ADJ (O) that SCONJ (O) uses VERB (O) [convolutional NOUN (B) neural NOUN (I) networks NOUN (I)] for ADP (O) [G2P NOUN (B) conversion NOUN (I)] . 
In ADP (O) this DET (O) paper NOUN , (O) we PRON (O) present NOUN (O) one NUM (O) general NOUN (O) [sequence NOUN - to ADP - sequence NOUN (B)] and CCONJ (O) four NUM (O) [encoder NOUN - decoder NOUN (B) models NOUN (I)] . 
These DET (O) are AUX (O) introduced VERB (O) in ADP (O) Section NOUN . (O) 
Our DET (O) goal NOUN (O) was AUX (O) to ADP (O) achieve VERB (O) and CCONJ (O) surpass NOUN (O) (if SCONJ (O) possible ADJ) (O) the DET (O) accuracy NOUN (O) of ADP (O) previous ADJ (O) models NOUN (O) and CCONJ (O) to ADP (O) reduce VERB (O) the DET (O) training NOUN (O) times NOUN (O) (which DET (O) is AUX (O) quite ADV (O) high ADJ (O) in ADP (O) the DET (O) case NOUN (O) of ADP (O) [LSTM PROPN (B)] / [GRU PROPN (B)]) . 
The DET (O) remaining VERB (O) parts NOUN (O) of ADP (O) this DET (O) paper NOUN (O) are AUX (O) structured ADJ (O) as SCONJ (O) follows VERB (O) : Section NOUN (O) discusses VERB (O) the DET (O) possibility NOUN (O) of ADP (O) applying VERB (O) [convolutional NOUN (B) neural NOUN (I) networks NOUN (I)] for ADP (O) [sequence NOUN - to ADP - sequence NOUN (B) grapheme NOUN - to ADP - phoneme NOUN (I) conversion NOUN (I)] . 
Datasets PROPN , (O) training NOUN (O) processes NOUN , (O) and CCONJ (O) evaluation NOUN (O) of ADP (O) the DET (O) proposed VERB (O) models NOUN (O) are AUX (O) presented VERB (O) in ADP (O) Section NOUN . (O) 
Section NOUN (O) analyzes VERB (O) the DET (O) results VERB (O) of ADP (O) the DET (O) models NOUN , (O) and CCONJ (O) finally ADV , (O) the DET (O) conclusion NOUN (O) is AUX (O) drawn VERB (O) in ADP (O) Section NOUN . (O) 

 [Convolutional PROPN (B) Neural PROPN (I) Networks NOUN (I)] for ADP (O) [Grapheme PROPN (B) to ADP (I) Phoneme PROPN (I) Conversion NOUN (I)] 

 [Convolutional PROPN (B) neural NOUN (I) networks NOUN (I)] are AUX (O) used VERB (O) in ADP (O) various ADJ (O) fields NOUN , (O) including VERB (O) image NOUN , (O) object NOUN (O) and CCONJ (O) handwriting NOUN (O) recognition NOUN , (O) face NOUN (O) verification NOUN , (O) [natural ADJ (B) language NOUN (I) processing NOUN (I)] and CCONJ (O) [machine NOUN (B) translation NOUN (I)] . 
The DET (O) architecture NOUN (O) of ADP (O) an DET (O) ordinary ADJ (O) [CNN PROPN (B)] is AUX (O) composed VERB (O) of ADP (O) many ADJ (O) layer NOUN (O) types NOUN (O) (such ADJ (O) as SCONJ (O) the DET (O) [convolutional NOUN (B) layers NOUN (I)] , pooling VERB (O) layers NOUN , (O) fully ADV (O) connecting VERB (O) layers NOUN , (O) etc X .) , (O) where ADV (O) each DET (O) layer NOUN (O) carries VERB (O) out NOUN (O) a NOUN (O) specific ADJ (O) function NOUN . (O) 
The DET (O) convolutional ADJ (O) and CCONJ (O) pooling VERB (O) layers NOUN (O) are AUX (O) for ADP (O) representation NOUN (O) learning NOUN , (O) while SCONJ (O) the DET (O) fully ADV (O) connected VERB (O) layers NOUN (O) on ADP (O) the DET (O) top NOUN (O) of ADP (O) the DET (O) network NOUN (O) are AUX (O) for ADP (O) modelling NOUN (O) a NOUN (O) classification NOUN (O) or CCONJ (O) regression NOUN (O) problem NOUN . (O) 
One NUM (O) of ADP (O) the DET (O) main ADJ (O) reasons NOUN (O) that SCONJ (O) make VERB (O) [convolutional NOUN (B) neural NOUN (I) networks NOUN (I)] superior ADJ (O) to ADP (O) previous ADJ (O) methods NOUN (O) is AUX (O) that SCONJ (O) [CNNs PROPN (B)] perform VERB (O) representation NOUN (O) learning NOUN (O) and CCONJ (O) modelling NOUN (O) jointly ADV (O) ; thus ADV , (O) a NOUN (O) quasi PROPN - optimal ADJ (O) representation NOUN (O) is AUX (O) extracted VERB (O) from ADP (O) the DET (O) [input NOUN (B) data NOUN (I)] for ADP (O) the DET (O) [machine NOUN (B) learning NOUN (I) model NOUN (I)] . 
Weight NOUN (O) sharing NOUN (O) in ADP (O) the DET (O) [convolutional NOUN (B) layers NOUN (I)] is AUX (O) also ADV (O) a NOUN (O) key NOUN (O) element NOUN . (O) 
Thus ADV , (O) the DET (O) model NOUN (O) becomes VERB (O) spatially ADV (O) tolerant ADJ (O) ; similar ADJ (O) representations NOUN (O) are AUX (O) learned VERB (O) in ADP (O) different ADJ (O) regions NOUN (O) of ADP (O) the DET (O) input NOUN , (O) and CCONJ (O) the DET (O) total NOUN (O) number NOUN (O) of ADP (O) parameters NOUN (O) can VERB (O) also ADV (O) be AUX (O) reduced VERB (O) drastically ADV . (O) 
[Deep ADJ (B) Learning NOUN (I)] refers VERB (O) to ADP (O) the DET (O) increased VERB (O) depth NOUN (O) of ADP (O) [neural NOUN (B) networks NOUN (I)] . 
Intuitively ADV , (O) it PRON (O) is AUX (O) expected VERB (O) that SCONJ (O) [neural NOUN (B) networks NOUN (I)] with ADP (O) many ADJ (O) hidden VERB (O) layers NOUN (O) are AUX (O) more ADJ (O) powerful ADJ (O) than SCONJ (O) shallow NOUN (O) ones NOUN (O) with ADP (O) a NOUN (O) single ADJ (O) hidden VERB (O) layer NOUN . (O) 
However ADV , (O) as SCONJ (O) the DET (O) number NOUN (O) of ADP (O) layers NOUN (O) increases VERB , (O) the DET (O) training NOUN (O) may VERB (O) become VERB (O) surprisingly ADV (O) hard ADV , (O) partly ADV (O) because SCONJ (O) the DET (O) gradients VERB (O) are AUX (O) unstable ADJ . (O) 
[Batch PROPN (B) normalization NOUN (I)] is AUX (O) a NOUN (O) technique NOUN (O) to ADP (O) overcome NOUN (O) this DET (O) problem NOUN (O) ; it PRON (O) reduces VERB (O) internal ADJ (O) covariance NOUN (O) shift NOUN (O) and CCONJ (O) helps VERB (O) to ADP (O) smooth VERB (O) learning NOUN . (O) 
The DET (O) main ADJ (O) idea NOUN (O) of ADP (O) [batch NOUN (B) normalization NOUN (I)] is AUX (O) to ADP (O) bring VERB (O) back NOUN (O) the DET (O) benefits NOUN (O) of ADP (O) normalization NOUN (O) at ADP (O) each DET (O) layer NOUN . (O) 
[Batch PROPN (B) normalization NOUN (I) results VERB (I)] in ADP (O) faster ADV (O) convergence NOUN (O) as SCONJ (O) well INTJ . (O) 
For ADP (O) example NOUN , (O) with ADP (O) [batch NOUN (B) normalization NOUN (I)] , 7 NUM (O) % of ADP (O) the DET (O) training NOUN (O) steps VERB (O) were AUX (O) enough ADJ (O) to ADP (O) achieve VERB (O) similar ADJ (O) accuracy NOUN (O) in ADP (O) an DET (O) image NOUN (O) classification NOUN (O) task NOUN . (O) 
Moreover ADV , (O) an DET (O) additional ADJ (O) advantage NOUN (O) of ADP (O) [batch NOUN (B) normalization NOUN (I)] is AUX (O) that SCONJ (O) it PRON (O) regularizes VERB (O) the DET (O) training NOUN (O) and CCONJ (O) thus ADV (O) reduces VERB (O) the DET (O) need NOUN (O) for ADP (O) [dropout NOUN (B)] and CCONJ (O) other ADJ (O) regularization NOUN (O) techniques NOUN . (O) 
However ADV , (O) [batch NOUN (B) normalization NOUN (I)] and CCONJ (O) [dropout NOUN (B)] are AUX (O) often ADV (O) simultaneously ADV (O) applied VERB . (O) 
[Convolutional PROPN (B) neural NOUN (I) networks NOUN (I)] have AUX (O) been AUX (O) successfully ADV (O) applied VERB (O) to ADP (O) various ADJ (O) [NLP PROPN (B) tasks NOUN (I)] . 
These DET (O) results VERB (O) suggest VERB (O) investigating VERB (O) the DET (O) possibility NOUN (O) of ADP (O) applying VERB (O) [CNN PROPN - based VERB (B) sequence NOUN - to ADP - sequence NOUN (I) models NOUN (I)] for ADP (O) [G2P NOUN (B)] . 
We PRON (O) expected VERB (O) that SCONJ (O) the DET (O) advantage NOUN (O) of ADP (O) [convolutional NOUN (B) neural NOUN (I) networks NOUN (I)] enhances VERB (O) the DET (O) performance NOUN (O) of ADP (O) [G2P NOUN (B) conversion NOUN (I)] . 
As SCONJ (O) is AUX (O) known VERB , (O) [LSTMs NOUN (B)] read VERB (O) input NOUN (O) sequentially ADV , (O) and CCONJ (O) the DET (O) output NOUN (O) for ADP (O) further NOUN (O) inputs VERB (O) depends VERB (O) on ADP (O) the DET (O) previous ADJ (O) ones NOUN . (O) 
Thus ADV , (O) we PRON (O) can VERB (O) not PART (O) parallelize NOUN (O) these DET (O) networks NOUN . (O) 
Applying VERB (O) [CNN PROPN (B)] also ADV (O) reduces VERB (O) computational ADJ (O) load NOUN (O) by ADP (O) using VERB (O) large ADJ (O) receptive ADJ (O) fields NOUN . (O) 
[Deep ADJ (B) neural NOUN (I) networks NOUN (I)] with ADP (O) a NOUN (O) sequential ADJ (O) architecture NOUN (O) have AUX (O) many ADJ (O) typical ADJ (O) building NOUN (O) blocks PROPN , (O) such ADJ (O) as SCONJ (O) convolutional ADJ (O) or CCONJ (O) fully ADV (O) connected VERB (O) layers NOUN , (O) stacked VERB (O) on ADP (O) each DET (O) other ADJ . (O) 
Increasing VERB (O) the DET (O) number NOUN (O) of ADP (O) layers NOUN (O) in ADP (O) these DET (O) kinds NOUN (O) of ADP (O) networks NOUN (O) does AUX (O) not PART (O) implicitly ADV (O) mean VERB (O) improved VERB (O) accuracy NOUN (O) (in ADP (O) our DET (O) case NOUN (O) PER PROPN (O) or CCONJ (O) [WER NOUN (B)]) , and CCONJ (O) some DET (O) issues NOUN , (O) such ADJ (O) as SCONJ (O) vanishing VERB (O) gradient NOUN (O) and CCONJ (O) degradation NOUN (O) problems NOUN , (O) can VERB (O) arise VERB (O) as SCONJ (O) well INTJ . (O) 
Introducing PROPN (O) residual ADJ (O) and CCONJ (O) highway NOUN (O) connections NOUN (O) can VERB (O) improve VERB (O) performance NOUN (O) significantly ADV . (O) 
These DET (O) connection NOUN (O) alternatives NOUN (O) allow VERB (O) the DET (O) information NOUN (O) to ADP (O) flow NOUN (O) more ADJ (O) into ADP (O) the DET (O) deeper NOUN (O) layers NOUN , (O) increase NOUN (O) the DET (O) convergence NOUN (O) speed NOUN (O) and CCONJ (O) decrease NOUN (O) the DET (O) vanishing VERB (O) gradient NOUN (O) problem NOUN . (O) 

 Models NOUN (O) 

 [Encoder NOUN - decoder NOUN (B) structures NOUN (I)] have AUX (O) shown VERB (O) state NOUN - of ADP - the DET - art NOUN (O) results VERB (O) in ADP (O) different ADJ (O) [NLP PROPN (B) tasks NOUN (I)] . 
The DET (O) main ADJ (O) idea NOUN (O) of ADP (O) these DET (O) approaches VERB (O) has AUX (O) two NUM (O) steps NOUN (O) : the DET (O) first ADJ (O) step NOUN (O) is AUX (O) mapping NOUN (O) the DET (O) input NOUN (O) sequence NOUN (O) to ADP (O) a NOUN (O) [vector NOUN (B)] ; the DET (O) second NOUN (O) step NOUN (O) is AUX (O) to ADP (O) generate NOUN (O) the DET (O) output NOUN (O) sequence NOUN (O) based VERB (O) on ADP (O) the DET (O) learned VERB (O) [vector NOUN (B) representation NOUN (I)] . 
[Encoder PROPN - decoder NOUN (B) models NOUN (I)] generate VERB (O) an DET (O) output NOUN (O) after ADP (O) the DET (O) complete ADJ (O) input NOUN (O) sequence NOUN (O) is AUX (O) processed VERB (O) by ADP (O) the DET (O) [encoder NOUN (B)] , which DET (O) enables VERB (O) the DET (O) [decoder NOUN (B)] to PART (O) learn VERB (O) from ADP (O) any DET (O) part NOUN (O) of ADP (O) the DET (O) input NOUN (O) without ADP (O) being NOUN (O) limited ADJ (O) to ADP (O) fixed VERB (O) context NOUN (O) windows PROPN . (O) 
Figure NOUN (O) shows VERB (O) an DET (O) example NOUN (O) of ADP (O) an DET (O) [encoder NOUN - decoder NOUN (B) architecture NOUN (I)] . 

 Figure NOUN . (O) The DET (O) input NOUN (O) of ADP (O) the DET (O) [encoder NOUN (B)] is AUX (O) the DET (O) “ PUNCT (O) CAKE PROPN (O) ” PUNCT (O) [grapheme NOUN (B) sequence NOUN (I)] , and CCONJ (O) the DET (O) [decoder NOUN (B)] produces VERB (O) “ PUNCT (O) KEY PROPN (O) K NOUN (O) ” PUNCT (O) as SCONJ (O) the DET (O) [phoneme NOUN (B) sequences NOUN (I)] . 
The DET (O) left VERB (O) side NOUN (O) is AUX (O) the DET (O) [encoder NOUN (B)] ; the DET (O) right ADV (O) side NOUN (O) is AUX (O) the DET (O) [decoder NOUN (B)] . 
The DET (O) model NOUN (O) stops VERB (O) making NOUN (O) predictions NOUN (O) after ADP (O) generating NOUN (O) the DET (O) [end NOUN - of ADP - phonemes NOUN (B)] tag NOUN . (O) 
As SCONJ (O) distinct ADJ (O) from ADP , (O) [input NOUN (B) data NOUN (I)] for ADP (O) the DET (O) [encoder NOUN (B)] is AUX (O) not PART (O) reversed VERB (O) in ADP (O) any DET (O) of ADP (O) our DET (O) models NOUN . (O) 

 In ADP (O) our DET (O) experiments NOUN , (O) we PRON (O) used VERB (O) [encoder NOUN - decoder NOUN (B) architectures NOUN (I)] . 
Several ADJ (O) models NOUN (O) with ADP (O) different ADJ (O) [hyperparameters NOUN (B)] were AUX (O) developed VERB (O) and CCONJ (O) tested VERB . (O) 
From ADP (O) a NOUN (O) large ADJ (O) number NOUN (O) of ADP (O) experiments NOUN , (O) the DET (O) five NUM (O) models NOUN (O) with ADP (O) the DET (O) highest ADJ (O) accuracy NOUN (O) and CCONJ (O) diverse NOUN (O) architectures VERB (O) were AUX (O) selected VERB . (O) 
Our DET (O) first ADJ (O) two NUM (O) models NOUN (O) were AUX (O) based VERB (O) on ADP (O) existing VERB (O) solutions NOUN (O) for ADP (O) comparison NOUN (O) purposes NOUN . (O) 
We PRON (O) used VERB (O) these DET (O) models NOUN (O) as SCONJ (O) a NOUN (O) baseline PROPN . (O) 
In ADP (O) the DET (O) following VERB (O) paragraphs NOUN , (O) the DET (O) five NUM (O) models NOUN (O) are AUX (O) introduced VERB (O) :      
 The DET (O) first ADJ (O) model NOUN (O) uses VERB (O) [LSTMs NOUN (B)] for ADP (O) both DET (O) the DET (O) [encoder NOUN (B)] and CCONJ (O) the DET (O) [decoder NOUN (B)] . 
The DET (O) [LSTM PROPN (B) encoder NOUN (I)] reads VERB (O) the DET (O) input NOUN (O) sequence NOUN (O) and CCONJ (O) creates VERB (O) a NOUN (O) fixed VERB - dimensional ADJ (O) [vector NOUN (B) representation NOUN (I)] . 
The DET (O) second NOUN (O) [LSTM PROPN (B)] is AUX (O) the DET (O) [decoder NOUN (B)] , and CCONJ (O) it PRON (O) generates VERB (O) the DET (O) output NOUN . (O) 
Figurea PROPN (O) shows VERB (O) the DET (O) structure NOUN (O) of ADP (O) the DET (O) first ADJ (O) model NOUN . (O) 
It PRON (O) can VERB (O) be AUX (O) seen VERB (O) that SCONJ (O) both DET (O) [LSTMs NOUN (B)] have AUX (O) 1024 NUM (O) units NOUN (O) ; [softmax PROPN (B) activation NOUN (I)] function NOUN (O) is AUX (O) used VERB (O) to ADP (O) obtain VERB (O) model NOUN (O) predictions NOUN . (O) 
This DET (O) architecture NOUN (O) is AUX (O) the DET (O) same ADJ (O) as SCONJ (O) a NOUN (O) previous ADJ (O) solution NOUN , (O) while SCONJ (O) the DET (O) parameters VERB (O) of ADP (O) training NOUN (O) (optimization NOUN (O) method NOUN , (O) regularization NOUN , (O) etc X .) (O) are AUX (O) identical NOUN (O) to ADP (O) the DET (O) settings NOUN (O) used VERB (O) in ADP (O) case NOUN (O) of ADP (O) the DET (O) other ADJ (O) four NUM (O) models NOUN . (O) 
In ADP (O) this DET (O) way NOUN (O) we PRON (O) try VERB (O) to ADP (O) ensure VERB (O) a NOUN (O) fair NOUN (O) comparison NOUN (O) among ADP (O) the DET (O) models NOUN . (O) 

 Figure NOUN . (O) [G2P NOUN (B) conversion NOUN (I)] model NOUN (O) based VERB (O) on ADP (O) [encoder NOUN - decoder NOUN (B)] (a X) (O) [LSTMs NOUN (B)] (first ADJ (O) model NOUN) (O) ; (b NOUN) (O) [Bi PROPN - LSTMs PROPN (B)] (second ADJ (O) model NOUN) (O) ; (c NOUN) (O) [encoder NOUN (B) CNN PROPN (I)] , [decoder NOUN (B) Bi PROPN - LSTM PROPN (I)] (third ADJ (O) model NOUN) . (O) 
f PROPN , (O) d NOUN , (O) s NOUN (O) are AUX (O) the DET (O) number NOUN (O) of ADP (O) the DET (O) filters NOUN , (O) length NOUN (O) of ADP (O) the DET (O) filters NOUN (O) and CCONJ (O) stride NOUN , (O) respectively ADV , (O) in ADP (O) the DET (O) [convolutional NOUN (B) layer NOUN (I)] . 

 Although SCONJ (O) the DET (O) [encoder NOUN - decoder NOUN (B) architecture NOUN (I)] achieves VERB (O) competitive ADJ (O) results NOUN (O) on ADP (O) a NOUN (O) wide ADJ (O) range NOUN (O) of ADP (O) problems NOUN , (O) it PRON (O) suffers VERB (O) from ADP (O) the DET (O) constraint NOUN (O) that SCONJ (O) all DET (O) input NOUN (O) sequences NOUN (O) are AUX (O) forced VERB (O) to ADP (O) be AUX (O) encoded VERB (O) to ADP (O) a NOUN (O) fixed VERB - size NOUN (O) latent NOUN (O) space NOUN . (O) 
To NOUN (O) overcome NOUN (O) this DET (O) limitation NOUN , (O) we PRON (O) investigated VERB (O) the DET (O) effects NOUN (O) of ADP (O) the DET (O) [attention NOUN (B) mechanism NOUN (I)] proposed VERB (O) by ADP (O) in ADP (O) Model NOUN (O) 1 NUM (O) and CCONJ (O) Model NOUN (O) 2 NUM . (O) 
We PRON (O) applied VERB (O) an DET (O) [attention NOUN (B) layer NOUN (I)] between ADP (O) the DET (O) [encoder NOUN (B)] and CCONJ (O) [decoder NOUN (B) LSTMs PROPN (I)] in ADP (O) the DET (O) case NOUN (O) of ADP (O) Model NOUN (O) 1 NUM , (O) and CCONJ (O) [Bi PROPN - LSTMs PROPN (B)] for ADP (O) Model NOUN (O) 2 NUM . (O) 
The DET (O) introduced VERB (O) [attention NOUN (B) layers NOUN (I)] are AUX (O) based VERB (O) on ADP (O) global ADJ (O) attention NOUN . (O) 
In ADP (O) the DET (O) second NOUN (O) model NOUN , (O) both DET (O) the DET (O) [encoder NOUN (B)] and CCONJ (O) the DET (O) [decoder NOUN (B)] are AUX (O) [Bi PROPN - LSTMs PROPN (B)] . 
The DET (O) structure NOUN (O) of ADP (O) this DET (O) model NOUN (O) is AUX (O) presented VERB (O) in ADP (O) Figureb PROPN . (O) 
The DET (O) input NOUN (O) is AUX (O) fed PROPN (O) to ADP (O) the DET (O) first ADJ (O) [Bi PROPN - LSTM PROPN (B)] ([encoder NOUN (B)]) , which DET (O) combines VERB (O) two NUM (O) unidirectional ADJ (O) [LSTM PROPN (B) layers NOUN (I)] that DET (O) process NOUN (O) the DET (O) input NOUN (O) from ADP (O) left VERB - to ADP - right NOUN (O) and CCONJ (O) right NOUN - to NOUN - left NOUN . (O) 
The DET (O) output NOUN (O) of ADP (O) the DET (O) [encoder NOUN (B)] is AUX (O) given VERB (O) as SCONJ (O) the DET (O) input NOUN (O) for ADP (O) the DET (O) second NOUN (O) [Bi PROPN - LSTM PROPN (B)] ([decoder NOUN (B)]) . 
Finally ADV , (O) the DET (O) [softmax PROPN (B) function NOUN (I)] is AUX (O) applied VERB (O) to ADP (O) generate NOUN (O) the DET (O) output NOUN (O) of ADP (O) [one NUM - hot ADJ (B) vectors NOUN (I)] ([phonemes NOUN (B)]) . 
During ADP (O) the DET (O) inference NOUN , (O) the DET (O) complete ADJ (O) input NOUN (O) sequence NOUN (O) is AUX (O) processed VERB (O) by ADP (O) the DET (O) [encoder NOUN (B)] , and CCONJ (O) after ADP (O) that SCONJ , (O) the DET (O) [decoder NOUN (B)] generates VERB (O) the DET (O) output NOUN . (O) 
For ADP (O) predicting VERB (O) a NOUN (O) [phoneme NOUN (B)] , both DET (O) the DET (O) left VERB (O) and CCONJ (O) the DET (O) right ADV (O) contexts NOUN (O) are AUX (O) considered VERB . (O) 
This DET (O) model NOUN (O) was AUX (O) also ADV (O) inspired VERB (O) by ADP (O) an DET (O) existing VERB (O) solution NOUN . (O) 
In ADP (O) the DET (O) third NOUN (O) model NOUN , (O) a NOUN (O) [convolutional NOUN (B) neural NOUN (I) network NOUN (I)] is AUX (O) introduced VERB (O) as SCONJ (O) the DET (O) [encoder NOUN (B)] , and CCONJ (O) a NOUN (O) [Bi PROPN - LSTM PROPN (B)] as SCONJ (O) the DET (O) [decoder NOUN (B)] . 
This DET (O) architecture NOUN (O) is AUX (O) presented VERB (O) in ADP (O) Figurec PROPN . (O) 
As SCONJ (O) this DET (O) figure NOUN (O) shows VERB , (O) the DET (O) number NOUN (O) of ADP (O) filters NOUN (O) is AUX (O) 524 NUM , (O) the DET (O) length NOUN (O) of ADP (O) the DET (O) filter NOUN (O) is AUX (O) 23 NUM , (O) the DET (O) stride NOUN (O) is AUX (O) 1 NUM , (O) and CCONJ (O) the DET (O) number NOUN (O) of ADP (O) cells NOUN (O) in ADP (O) the DET (O) [Bi PROPN - LSTM PROPN (B)] is AUX (O) 1024 NUM . (O) 
In ADP (O) this DET (O) model NOUN , (O) the DET (O) [CNN PROPN (B) layer NOUN (I)] takes VERB (O) [graphemes NOUN (B)] as SCONJ (O) input NOUN (O) and CCONJ (O) performs VERB (O) convolution NOUN (O) operations NOUN . (O) 
For ADP (O) regularization NOUN (O) purpose NOUN , (O) we PRON (O) also ADV (O) introduced VERB (O) [batch NOUN (B) normalization NOUN (I)] in ADP (O) this DET (O) model NOUN . (O) 
The DET (O) fourth ADJ (O) model NOUN (O) contains VERB (O) [convolutional NOUN (B) layers NOUN (I)] only ADV , (O) with ADP (O) [residual ADJ (B) connections NOUN (I)] (blocks PROPN) . (O) 
These DET (O) [residual ADJ (B) connections NOUN (I)] have AUX (O) two NUM (O) rules VERB (O) : 
if SCONJ (O) [feature NOUN (B) maps NOUN (I)] have AUX (O) the DET (O) same ADJ (O) size NOUN , (O) then ADV (O) the DET (O) blocks PROPN (O) share NOUN (O) the DET (O) same ADJ (O) [hyperparameters NOUN (B)] . 
each DET (O) time NOUN (O) when ADV (O) the DET (O) [feature NOUN (B) map NOUN (I)] is AUX (O) halved VERB , (O) the DET (O) number NOUN (O) of ADP (O) filters NOUN (O) is AUX (O) doubled VERB . (O) 
First ADV , (O) we PRON (O) apply VERB (O) one NUM (O) [convolutional NOUN (B) layer NOUN (I)] with ADP (O) 64 NUM (O) filters NOUN (O) to ADP (O) the DET (O) input NOUN (O) layer NOUN , (O) followed VERB (O) by ADP (O) a NOUN (O) stack NOUN (O) of ADP (O) residual ADJ (O) blocks PROPN . (O) 
Through ADP (O) [hyperparameter NOUN (B) optimization NOUN (I)] , the DET (O) best ADJ (O) result NOUN (O) was AUX (O) achieved VERB (O) by ADP (O) 4 NUM (O) residual ADJ (O) blocks PROPN , (O) as SCONJ (O) shown VERB (O) in ADP (O) Figurea PROPN , (O) and CCONJ (O) the DET (O) number NOUN (O) of ADP (O) filters NOUN (O) in ADP (O) each DET (O) residual ADJ (O) block NOUN (O) is AUX (O) 64 NUM , (O) 128 NUM , (O) 256 NUM , (O) 512 NUM , (O) respectively ADV . (O) 
Each DET (O) residual ADJ (O) block NOUN (O) contains VERB (O) a NOUN (O) sequence NOUN (O) of ADP (O) two NUM (O) [convolutional NOUN (B) layers NOUN (I)] followed VERB (O) by ADP (O) a NOUN (O) [batch NOUN (B) normalization NOUN (I) layer NOUN (I)] and CCONJ (O) [ReLU NOUN (B) activation NOUN (I)] . 
The DET (O) filter NOUN (O) size NOUN (O) of ADP (O) all DET (O) [convolutional NOUN (B) layers NOUN (I)] is AUX (O) three NUM . (O) 
After ADP (O) these DET (O) blocks PROPN , (O) one NUM (O) more ADJ (O) [batch NOUN (B) normalization NOUN (I) layer NOUN (I)] and CCONJ (O) [ReLU NOUN (B) activation NOUN (I)] are AUX (O) applied VERB . (O) 
The DET (O) architecture NOUN (O) ends VERB (O) with ADP (O) a NOUN (O) fully ADV (O) connected VERB (O) layer NOUN , (O) which DET (O) uses VERB (O) the DET (O) [softmax PROPN (B) activation NOUN (I)] function NOUN . (O) 

 Figure NOUN . (O) [G2P NOUN (B) conversion NOUN (I)] based VERB (O) on ADP (O) (a X) (O) [convolutional NOUN (B) neural NOUN (I) network NOUN (I)] with ADP (O) [residual ADJ (B) connections NOUN (I)] (fourth ADJ (O) model NOUN) (O) ; and CCONJ (O) (b NOUN) (O) [encoder NOUN (B) convolutional ADJ (I) neural PROPN (I) network NOUN (I)] with ADP (O) [residual ADJ (B) connections NOUN (I)] and CCONJ (O) [decoder NOUN (B) Bi PROPN - LSTM PROPN (I)] (fifth ADJ (O) model NOUN) . (O) 
f PROPN , (O) d NOUN , (O) s NOUN (O) are AUX (O) the DET (O) number NOUN (O) of ADP (O) the DET (O) filters NOUN , (O) length NOUN (O) of ADP (O) the DET (O) filters NOUN (O) and CCONJ (O) stride NOUN , (O) respectively ADV . (O) 

 We PRON (O) carried VERB (O) out NOUN (O) experiments VERB (O) with ADP (O) the DET (O) same ADJ (O) fully ADV (O) convolutional ADJ (O) models NOUN (O) without ADP (O) [residual ADJ (B) connections NOUN (I)] ; however ADV , (O) the DET (O) [phoneme NOUN (B)] and CCONJ (O) [word NOUN (B) error NOUN (I) rates NOUN (I)] were AUX (O) worse ADJ (O) than SCONJ (O) with ADP (O) [residual ADJ (B) connections NOUN (I)] , as SCONJ (O) expected VERB . (O) 
The DET (O) fifth NOUN (O) model NOUN (O) combines VERB (O) Models PROPN (O) 3 NUM (O) and CCONJ (O) 4 NUM (O) : the DET (O) [encoder NOUN (B)] has AUX (O) the DET (O) same ADJ (O) [convolutional NOUN (B) neural NOUN (I) network NOUN (I) architecture NOUN (I)] with ADP (O) [residual ADJ (B) connections NOUN (I)] and CCONJ (O) [batch NOUN (B) normalization NOUN (I)] that DET (O) was AUX (O) introduced VERB (O) in ADP (O) model NOUN (O) four NUM . (O) 
The DET (O) [decoder NOUN (B)] is AUX (O) a NOUN (O) [Bi PROPN - LSTM PROPN (B)] , as SCONJ (O) in ADP (O) model NOUN (O) three NUM . (O) 
The DET (O) structure NOUN (O) of ADP (O) this DET (O) model NOUN (O) is AUX (O) presented VERB (O) in ADP (O) Figureb PROPN . (O) 
In ADP (O) all DET (O) models NOUN (O) except SCONJ (O) Model NOUN (O) 4 NUM , (O) we PRON (O) used VERB (O) stateless NOUN (O) [LSTM PROPN (B)] (or CCONJ (O) [Bi PROPN - LSTM PROPN (B)]) configurations NOUN (O) ; the DET (O) internal ADJ (O) state NOUN (O) is AUX (O) reset VERB (O) after ADP (O) each DET (O) batch NOUN (O) for ADP (O) predictions NOUN . (O) 

 Details NOUN (O) of ADP (O) the DET (O) [Bidirectional PROPN (B) Decoder PROPN (I)] 

 The DET (O) details NOUN (O) of ADP (O) the DET (O) [bidirectional ADJ (B) decoder NOUN (I)] , which DET (O) was AUX (O) used VERB (O) in ADP (O) Model NOUN (O) 2 NUM , (O) are AUX (O) presented VERB (O) in ADP (O) this DET (O) section NOUN . (O) 
Given VERB (O) an DET (O) input NOUN (O) sequence NOUN (O) x NOUN (O) = (x1 PROPN , (O) x2 PROPN , ... PUNCT , (O) x NOUN (O) N NUM) , (O) the DET (O) [LSTM PROPN (B) network NOUN (I)] computes VERB (O) the DET (O) hidden VERB (O) [vector NOUN (B) sequence NOUN (I)] h NOUN (O) = (h1 PROPN , (O) h2 PROPN , ... PUNCT , (O) h NOUN (O) N NUM) (O) and CCONJ (O) output NOUN (O) [vector NOUN (B) sequence NOUN (I)] y NOUN (O) = (y1 PROPN , (O) y2 PROPN , ... PUNCT , (O) y NOUN (O) N PROPN) . (O) 
Initially ADV , (O) [one NUM - hot ADJ (B) character NOUN (I) vectors NOUN (I)] for ADP (O) [graphemes NOUN (B) and CCONJ (I) phonemes PROPN (I) sequences NOUN (I)] were AUX (O) created VERB . (O) 
Character NOUN (O) vocabularies NOUN , (O) which DET (O) contain NOUN (O) all DET (O) the DET (O) elements NOUN (O) that SCONJ (O) are AUX (O) present NOUN (O) in ADP (O) the DET (O) input NOUN (O) and CCONJ (O) [output NOUN (B) data NOUN (I)] , are AUX (O) separately ADV (O) calculated VERB . (O) 
In ADP (O) other ADJ (O) words NOUN , (O) neither CCONJ (O) a NOUN (O) [grapheme NOUN (B) vector NOUN (I)] in ADP (O) the DET (O) output NOUN (O) vocabulary NOUN , (O) nor CCONJ (O) a NOUN (O) [phoneme NOUN (B) vector NOUN (I)] in ADP (O) the DET (O) input NOUN (O) vocabulary NOUN , (O) was AUX (O) used VERB . (O) 
These DET (O) were AUX (O) the DET (O) inputs VERB (O) to ADP (O) the DET (O) [encoder NOUN (B)] and CCONJ (O) the DET (O) [decoder NOUN (B)] . 
Padding NOUN (O) was AUX (O) applied VERB (O) to ADP (O) make VERB (O) all DET (O) input NOUN (O) and CCONJ (O) output NOUN (O) sequences NOUN (O) to ADP (O) have AUX (O) the DET (O) same ADJ (O) length NOUN , (O) which DET (O) was AUX (O) set NOUN (O) to ADP (O) 22 NUM . (O) 
This DET (O) number NOUN (O) (22 NUM) (O) was AUX (O) chosen VERB (O) based VERB (O) on ADP (O) the DET (O) maximum NOUN (O) length NOUN (O) in ADP (O) the DET (O) [training NOUN (B) database NOUN (I)] . 
For ADP (O) [G2P NOUN (B)] , x SYM (O) = (x1 PROPN , (O) x2 PROPN , ... PUNCT , (O) x NOUN (O) N NUM) (O) are AUX (O) [one NUM - hot ADJ (B) character NOUN (I) vectors NOUN (I)] of ADP (O) [grapheme NOUN (B)] sequences NOUN (O) ; y NOUN (O) = (y1 PROPN , (O) y2 PROPN , ... PUNCT , (O) y NOUN (O) N PROPN) (O) are AUX (O) [one NUM - hot ADJ (B) character NOUN (I) vectors NOUN (I)] of ADP (O) [phoneme NOUN (B) sequences NOUN (I)] . 
In ADP (O) the DET (O) proposed VERB (O) Model NOUN (O) 2 NUM , (O) [Bi PROPN - LSTM PROPN (B)] was AUX (O) used VERB (O) as SCONJ (O) an DET (O) [encoder NOUN (B)] , and CCONJ (O) it PRON (O) consists VERB (O) of ADP (O) two NUM (O) [LSTMs NOUN (B)] : one NUM (O) that SCONJ (O) processes VERB (O) the DET (O) sequence NOUN (O) from ADP (O) left VERB - to ADP - right NOUN (O) (forward ADV (O) [encoder NOUN (B)]) , and CCONJ (O) one NUM (O) that SCONJ (O) does AUX (O) it PRON (O) in ADP (O) reverse NOUN (O) ([backward ADJ (B) encoder NOUN (I)]) . 
This DET (O) was AUX (O) applied VERB (O) to ADP (O) learn VERB (O) the DET (O) semantic ADJ (O) representation NOUN (O) of ADP (O) the DET (O) input NOUN (O) sequences NOUN (O) in ADP (O) both DET (O) directions NOUN . (O) 
One NUM (O) [LSTM PROPN (B)] looks VERB (O) at ADP (O) the DET (O) sequence NOUN (O) from ADP (O) left VERB - to ADP - right NOUN (O) (forward ADV (O) [encoder NOUN (B)]) , and CCONJ (O) so CCONJ (O) reads VERB (O) an DET (O) input NOUN (O) sequence NOUN (O) in ADP (O) left VERB - to ADP - right NOUN (O) order NOUN (O) ; and CCONJ (O) another DET (O) [LSTM PROPN (B)] looks VERB (O) at ADP (O) it PRON (O) in ADP (O) reverse NOUN (O) ([backward ADJ (B) encoder NOUN (I)]) , and CCONJ (O) so CCONJ (O) reads VERB (O) an DET (O) input NOUN (O) sequence NOUN (O) in ADP (O) a NOUN (O) right NOUN - to ADP - left NOUN (O) order NOUN . (O) 
At ADP (O) each DET (O) of ADP (O) the DET (O) time NOUN (O) steps NOUN , (O) the DET (O) forward NOUN (O) hidden VERB (O) ← PROPN (O) sequence NOUN (O) ~h PUNCT (O) and CCONJ (O) the DET (O) backward NOUN (O) hidden VERB (O) sequence NOUN (O) h NOUN (O) are AUX (O) iterated ADJ (O) by ADP (O) the DET (O) following VERB (O) equations NOUN (O) : 
In ADP (O) Equation NOUN , (O) the DET (O) [forward ADV (B) layer NOUN (I)] is AUX (O) iterated ADJ (O) from ADP (O) t NOUN (O) = 1 NUM (O) to ADP (O) N NOUN (O) ; in ADP (O) Equation NOUN , (O) the DET (O) backward NOUN (O) layer NOUN (O) is AUX (O) iterated ADJ (O) from ADP (O) t NOUN (O) = N NOUN (O) to ADP (O) 1 NUM (O) ; H NOUN (O) is AUX (O) an DET (O) element NOUN - wise ADJ (O) [sigmoid NOUN (B) function NOUN (I)] . 
As SCONJ (O) the DET (O) next ADJ (O) step NOUN , (O) the DET (O) [hidden VERB (B) states NOUN (I)] of ADP (O) these DET (O) two NUM (O) [LSTMs NOUN (B)] were AUX (O) concatenated VERB (O) to ADP (O) form NOUN (O) an DET (O) annotation NOUN (O) → NOUN (O) ← PROPN (O) sequence NOUN (O) h NOUN (O) = { h1 PROPN , (O) h2 PROPN , ... PUNCT , (O) h NOUN (O) N NOUN (O) } , (O) where ADV (O) ht X (O) = (ht INTJ , (O) ht PROPN) (O) encodes VERB (O) information NOUN (O) about ADP (O) the DET (O) t NOUN (O) − PROPN (O) th X (O) [grapheme NOUN (B)] with ADP (O) respect NOUN (O) to ADP (O) all DET (O) the DET (O) other ADJ (O) surrounding VERB (O) [graphemes NOUN (B)] in ADP (O) the DET (O) input NOUN . (O) 
W NOUN (O) → INTJ , (O) W NOUN (O) ← PROPN , (O) W←← PROPN (O) and CCONJ (O) W→→ PUNCT (O) are AUX (O) weight NOUN (O) xh PROPN (O)        xh NOUN (O)          hh PRON (O)         hh PART (O) matrixes NOUN (O) ; b→ PROPN , (O) b← PROPN (O) denotes VERB (O) the DET (O) bias NOUN (O) [vectors NOUN (B)] . 
Generally ADV , (O) in ADP (O) all DET (O) parameters NOUN , (O) the DET (O) arrows VERB (O) pointing VERB (O) left VERB (O) to ADP (O) right ADV (O)               h NOUN (O)       h PROPN (O) and CCONJ (O) right ADV (O) to ADP (O) left VERB (O) refer NOUN (O) to ADP (O) the DET (O) forward NOUN (O) and CCONJ (O) backward NOUN (O) layers NOUN , (O) respectively ADV . (O) 
The DET (O) forward NOUN (O) [LSTM PROPN (B)] unrolls VERB (O) the DET (O) sequences NOUN (O) until ADP (O) it PRON (O) reaches VERB (O) the DET (O) end NOUN (O) of ADP (O) sequence NOUN (O) for ADP (O) that SCONJ (O) input NOUN . (O) 
The DET (O) backward NOUN (O) [LSTM PROPN (B)] unrolls VERB (O) the DET (O) sequences NOUN (O) until ADP (O) it PRON (O) reaches VERB (O) the DET (O) start VERB (O) of ADP (O) the DET (O) sequence NOUN . (O) 
For ADP (O) the DET (O) [decoder NOUN (B)] , we PRON (O) used VERB (O) [bidirectional ADJ (B) LSTM PROPN (I)] . 
These DET (O) [LSTMs NOUN (B)] can VERB (O) be AUX (O) called VERB (O) forward NOUN (O) and CCONJ (O) backward NOUN (O) → NOUN (O) ([decoders NOUN (B)] , and CCONJ (O) described VERB (O) as SCONJ (O) d NOUN , (O) d. PROPN (O) 
After ADP (O) concatenating VERB (O) the DET (O) forward NOUN (O) and CCONJ (O) [backward NOUN (B) encoder NOUN (I) LSTMs PROPN (I)] , the DET (O) [backward NOUN (B) decoder NOUN (I)] performs VERB (O) decoding VERB (O) in ADP (O) a NOUN (O) right NOUN - to ADP - left NOUN (O) way NOUN . (O) 
It PRON (O) was AUX (O) initialized VERB (O) with ADP (O) a NOUN (O) final ADJ (O) encoded VERB (O) state NOUN (O) and CCONJ (O) a NOUN (O) reversed VERB (O) output NOUN (O) ([phonemes NOUN (B)]) . 
The DET (O) [forward ADV (B) decoder NOUN (I)] is AUX (O) trained VERB (O) to ADP (O) sequentially ADV (O) predict VERB (O) the DET (O) next ADJ (O) [phoneme NOUN (B)] given VERB (O) the DET (O) [phoneme NOUN (B) sequence NOUN (I)] . 
This DET (O) part NOUN (O) was AUX (O) initialized VERB (O) with ADP (O) the DET (O) final ADJ (O) state NOUN (O) of ADP (O) the DET (O) [encoder NOUN (B)] and CCONJ (O) all DET (O) [phoneme NOUN (B) sequences NOUN (I)] . 
Each DET (O) [decoder NOUN (B) output NOUN (I)] is AUX (O) passed VERB (O) through ADP (O) the DET (O) [softmax PROPN (B) layer NOUN (I)] that DET (O) will VERB (O) learn VERB (O) to ADP (O) classify NOUN (O) the DET (O) correct ADJ (O) [phonemes NOUN (B)] . 
For ADP (O) training NOUN , (O) given VERB (O) the DET (O) previous ADJ (O) [phonemes NOUN (B)] , the DET (O) model NOUN (O) factorizes NOUN (O) the DET (O) conditional ADJ (O) into ADP (O) a NOUN (O) summation NOUN (O) of ADP (O) individual NOUN (O) log NOUN (O) conditional ADJ (O) probabilities NOUN (O) from ADP (O) both DET (O) directions NOUN , (O) 
where ADV (O) log NOUN (O) P NOUN (O) yt NOUN (O) y(1:(t−1 NOUN)) (O) and CCONJ (O) log NOUN (O) P NOUN (O) yt NOUN (O) y((t+1):N PROPN) (O) are AUX (O) the DET (O) left VERB - to ADP - right NOUN (O) (forward ADV) , (O) the DET (O) right NOUN - to ADP - left NOUN (O) (backward ADJ) (O) conditional ADJ (O) probability NOUN (O) in ADP (O) Equation NOUN , (O) and CCONJ (O) calculated VERB (O) as SCONJ (O) per ADP (O) the DET (O) equations NOUN (O) below ADP (O) : 

 The DET (O) prediction NOUN (O) is AUX (O) performed VERB (O) on ADP (O) [test NOUN (B) data NOUN (I)] as SCONJ (O) follows VERB (O) : 
According VERB (O) to ADP (O) Equation NOUN , (O) future NOUN (O) output NOUN (O) is AUX (O) not PART (O) used VERB (O) during ADP (O) inference NOUN . (O) 
The DET (O) architecture NOUN (O) is AUX (O) shown VERB (O) in ADP (O) Figure NOUN . (O) 

 Figure NOUN . (O) The DET (O) architecture NOUN (O) of ADP (O) the DET (O) proposed VERB (O) [bidirectional ADJ (B) decoder NOUN (I) model NOUN (I)] for ADP (O) [G2P NOUN (B) task NOUN (I)] . 

 Experiments NOUN (O) 

 Datasets VERB (O) 

 We PRON (O) used VERB (O) the DET (O) CMU PROPN (O) pronunciation NOUN (O) (http://www.speech.cs.cmu.edu NOUN (O) / cgi PROPN - bin PROPN (O) / [cmudict NOUN (B)]) and CCONJ (O) [NetTalk PROPN (B)] (we PRON (O) are AUX (O) grateful ADJ (O) to ADP (O) Stan PROPN (O) Chen PROPN (O) for ADP (O) providing VERB (O) the DET (O) data NOUN) (O) datasets VERB , (O) which DET (O) have AUX (O) frequently ADV (O) been AUX (O) chosen VERB (O) by ADP (O) various ADJ (O) researchers NOUN (O) (3,16,32 NOUN) . (O) 
The DET (O) training NOUN (O) and CCONJ (O) testing NOUN (O) splits VERB (O) are AUX (O) the DET (O) same ADJ (O) as SCONJ (O) those DET (O) found VERB (O) in ADP (O) (4,5,8,12 NUM) (O) ;     thus ADV , (O) the DET (O) results VERB (O) are AUX (O) comparable ADJ . (O) 
[CMUDict NOUN (B)] contains VERB (O) a NOUN (O) 106,837-word NUM (O) training NOUN (O) set NOUN (O) and CCONJ (O) a NOUN (O) 12,000-word NUM (O) test NOUN (O) set NOUN (O) ([reference NOUN (B) data NOUN (I)]) . 
2670 NUM (O) words NOUN (O) are AUX (O) used VERB (O) as SCONJ (O) development NOUN (O) set VERB . (O) 
There PRON (O) are AUX (O) 27 NUM (O) [graphemes NOUN (B)] (uppercase NOUN (O) alphabet PROPN (O) symbols NOUN (O) plus CCONJ (O) the DET (O) apostrophe PROPN) (O) and CCONJ (O) 41 NUM (O) [phonemes NOUN (B)] (AA PROPN , (O) AE NOUN , (O) AH PROPN , (O) AO PROPN , (O) AW PROPN , (O) AY INTJ , (O) B , (O) CH PROPN , (O) D NOUN , (O) DH PROPN , (O)     EH INTJ , (O) ER PROPN , (O) EY PROPN , (O) F PROPN , (O) G NOUN , (O) HH PROPN , (O) IH PROPN , (O) IY PROPN , (O) JH PROPN , (O) K PROPN , (O) L NOUN , (O) M PROPN , (O) N NOUN , (O) NG NOUN , (O) OW PROPN , (O) OY NOUN , (O) P NOUN , (O) R NOUN , (O) S PROPN , (O) SH PROPN , (O) T NOUN , (O) TH NOUN , (O) UH PROPN , (O) UW PROPN , (O) V PROPN , (O) W PROPN , (O) Y INTJ , (O) Z NOUN , (O) ZH PROPN , (O) < EP PROPN (O) > , (O) < /EP NUM (O) >) (O) in ADP (O) this DET (O) dataset NOUN . (O) 
[NetTalk PROPN (B)] contains VERB (O) 14,851 NUM (O) words NOUN (O) for ADP (O) training NOUN , (O) 4,951 NUM (O) words NOUN (O) for ADP (O) testing NOUN (O) and CCONJ (O) does AUX (O) not PART (O) have AUX (O) a NOUN (O) predefined VERB (O) validation NOUN (O) set VERB . (O) 
There PRON (O) are AUX (O) 26 NUM (O) [graphemes NOUN (B)] (lowercase NOUN (O) alphabet PROPN (O) symbols NOUN) (O) and CCONJ (O) 52 NUM (O) [phonemes NOUN (B)] (‘ PUNCT (O) ! ’ PUNCT , (O) ‘ PUNCT (O) # ’ PUNCT , (O) ‘ PUNCT (O) * ’ PUNCT , (O) ‘ PUNCT (O) + ’ PUNCT , (O) ‘ PUNCT (O) @ ’ PUNCT , (O) ‘ PUNCT (O) A NOUN ’ PUNCT , (O) ‘ PUNCT (O) C NOUN ’ PUNCT , (O) ‘ PUNCT (O) D PROPN ’ PUNCT , (O) ‘ PUNCT (O) E NOUN ’ PUNCT , (O) ‘ PUNCT (O) G NOUN ’ PUNCT , (O) ‘ PUNCT (O) I ’ PUNCT , (O) ‘ PUNCT (O) J PROPN ’ PUNCT , (O) ‘ PUNCT (O) K PROPN ’ PUNCT , (O) ‘ PUNCT (O) L NOUN ’ PUNCT , (O) ‘ PUNCT (O) M PROPN ’ PUNCT , (O) ‘ PUNCT (O) N PROPN ’ PUNCT , (O) ‘ PUNCT (O) O ’ PUNCT , (O) ‘ PUNCT (O) R NOUN ’ PUNCT , (O) ‘ PUNCT (O) S PROPN ’ PUNCT , (O) ‘ PUNCT (O) T PROPN ’ PUNCT , (O) ‘ PUNCT (O) U NOUN ’ PUNCT , (O) ‘ PUNCT (O) W NOUN ’ PUNCT , (O) ‘ PUNCT (O) X NOUN ’ PUNCT , (O) ‘ PUNCT (O) Y NOUN ’ PUNCT , (O) ‘ PUNCT (O) Z NOUN ’ PUNCT , (O) ‘ PUNCT (O) ˆ NOUN ’ PUNCT , (O) ‘ PUNCT (O) a DET ’ PUNCT , (O) ‘ PUNCT (O) b NOUN ’ PUNCT , (O) ‘ PUNCT (O) c NOUN ’ PUNCT , (O) ‘ PUNCT (O) d NOUN ’ PUNCT , (O) ‘ PUNCT (O) e NOUN ’ PUNCT , (O) ‘ PUNCT (O) f PROPN ’ PUNCT , (O) ‘ PUNCT (O) g PROPN ’ PUNCT , (O) ‘ PUNCT (O) h NOUN ’ PUNCT , (O) ‘ PUNCT (O) i PRON ’ PUNCT , (O) ‘ PUNCT (O) k NOUN ’ PUNCT , (O) ‘ PUNCT (O) l PROPN ’ PUNCT , (O) ‘ PUNCT (O) m NOUN ’ PUNCT , (O) ‘ PUNCT (O) n PROPN ’ PUNCT , (O) ‘ PUNCT (O) o NOUN ’ PUNCT , (O) ‘ PUNCT (O) p NOUN ’ PUNCT , (O) ‘ PUNCT (O) r NOUN ’ PUNCT , (O) ‘ PUNCT (O) s PROPN ’ PUNCT , (O) ‘ PUNCT (O) t NOUN ’ PUNCT , (O) ‘ PUNCT (O) u PROPN ’ PUNCT , (O) ‘ PUNCT (O) v NOUN ’ PUNCT , (O) ‘ PUNCT (O) w NOUN ’ PUNCT , (O) ‘ PUNCT (O) x X ’ PUNCT , (O) ‘ PUNCT (O) y NOUN ’ PUNCT , (O) ‘ PUNCT (O) z’,<EP NOUN (O) > , (O) < /EP NUM (O) >) (O) in ADP (O) this DET (O) dataset NOUN . (O) 
We PRON (O) use NOUN (O) < EP PROPN (O) > and CCONJ (O) < /EP NUM (O) > tokens NOUN (O) as SCONJ (O) [beginning NOUN - of ADP - graphemes NOUN (B)] and CCONJ (O) [end NOUN - of ADP - graphemes NOUN (B) tokens NOUN (I)] in ADP (O) both DET (O) datasets VERB . (O) 
For ADP (O) inference NOUN , (O) the DET (O) [decoder NOUN (B)] uses VERB (O) the DET (O) past ADJ (O) [phoneme NOUN (B) sequence NOUN (I)] to PART (O) predict VERB (O) the DET (O) next ADJ (O) [phoneme NOUN (B)] , and CCONJ (O) it PRON (O) stops VERB (O) predicting VERB (O) after ADP (O) token PROPN (O) < /EP NUM (O) > . (O) 

 Training NOUN (O) 
 
 For ADP (O) the DET (O) [CMUDict NOUN (B) experiments NOUN (I)] , in ADP (O) all DET (O) models NOUN , (O) the DET (O) size NOUN (O) of ADP (O) the DET (O) input NOUN (O) layers NOUN (O) is AUX (O) equal ADJ (O) to ADP (O) the DET (O) input NOUN (O) : { length NOUN (O) of ADP (O) the DET (O) longest ADJ (O) input NOUN (O) (22 NUM) (O) × PROPN (O) number NOUN (O) of ADP (O) [graphemes NOUN (B)] (27 NUM) (O) } and CCONJ (O) the DET (O) size NOUN (O) of ADP (O) the DET (O) [output NOUN (B) layers NOUN (I)] is AUX (O) equal ADJ (O) to ADP (O) the DET (O) output NOUN (O) : { length NOUN (O) of ADP (O) the DET (O) longest ADJ (O) output NOUN (O) (22 NUM) (O) × PROPN (O) number NOUN (O) of ADP (O) [phonemes NOUN (B)] (41 NUM) (O) } . (O) 
To NOUN (O) transform NOUN (O) [graphemes NOUN (B)] and CCONJ (O) [phonemes NOUN (B)] for ADP (O) [neural NOUN (B) networks NOUN (I)] , we PRON (O) convert NOUN (O) inputs VERB (O) into ADP (O) 27-dimensional NUM (O) and CCONJ (O) outputs NOUN (O) to ADP (O) 41-dimensional NUM (O) [one NUM - hot ADJ (B) vector NOUN (I) representations NOUN (I)] . 
For ADP (O) example NOUN , (O) the DET (O) [phoneme NOUN (B) sequences NOUN (I)] of ADP (O) the DET (O) word NOUN (O) ‘ PUNCT (O) ARREST NOUN ’ PUNCT (O) is AUX (O) ‘ PUNCT (O) ER PROPN (O) EH PROPN (O) S NOUN (O) T NOUN ’ PUNCT (O) ; the DET (O) input NOUN (O) and CCONJ (O) output NOUN (O) [vectors NOUN (B)] of ADP (O) the DET (O) [grapheme NOUN (B) and CCONJ (I) phoneme PROPN (I) sequences NOUN (I)] are AUX (O) as SCONJ (O) below ADP (O) : 


 In ADP (O) the DET (O) case NOUN (O) of ADP (O) [LSTMs NOUN (B)] , we PRON (O) applied VERB (O) the DET (O) [Adam PROPN (B) optimization NOUN (I) algorithm PROPN (I)] with ADP (O) a NOUN (O) starting VERB (O) learning NOUN (O) rate NOUN (O) of ADP (O) 0.001 NUM , (O) and CCONJ (O) with ADP (O) baseline NOUN (O) values NOUN (O) of ADP (O) β1 PROPN , (O) β2 PROPN (O) and CCONJ (O) ε NOUN (O) (0.9 NUM , (O) 0.999 NUM (O) and CCONJ (O) 1 NUM (O) × PROPN (O) 10−8 NUM , (O) respectively ADV) . (O) 
For ADP (O) [batch NOUN (B) size NOUN (I)] , 128 NUM (O) was AUX (O) chosen VERB . (O) 
Weights PROPN (O) were AUX (O) saved VERB (O) when ADV (O) the DET (O) PER PROPN (O) on ADP (O) the DET (O) [validation NOUN (B) dataset NOUN (I)] achieved VERB (O) a NOUN (O) lower ADJ (O) value NOUN (O) than SCONJ (O) before ADV . (O) 
When ADV (O) the DET (O) PER PROPN (O) did AUX (O) not PART (O) decrease NOUN (O) further NOUN (O) for ADP (O) 100 NUM (O) [epochs NOUN (B)] , the DET (O) best ADJ (O) model NOUN (O) was AUX (O) chosen VERB , (O) and CCONJ (O) it PRON (O) was AUX (O) trained VERB (O) with ADP (O) stochastic NOUN (O) gradient NOUN (O) descent NOUN (O) (SGD PROPN) (O) further ADV . (O) 
In ADP (O) the DET (O) case NOUN (O) of ADP (O) the DET (O) first ADV , (O) second NOUN (O) and CCONJ (O) third NOUN (O) models NOUN (O) for ADP (O) SGD PROPN , (O) we PRON (O) used VERB (O) 0.005 NUM (O) as SCONJ (O) the DET (O) learning NOUN (O) rate NOUN , (O) and CCONJ (O) 0.8 NUM (O) for ADP (O) momentum NOUN . (O) 
For ADP (O) the DET (O) fourth ADJ (O) (convolutional ADJ (O) with ADP (O) [residual ADJ (B) connections NOUN (I)]) model NOUN , (O) 0.05 NUM (O) ([learning NOUN (B) rate NOUN (I)]) and CCONJ (O) 0.8 NUM (O) (momentum NOUN) (O) were AUX (O) applied VERB , (O) and CCONJ (O) it PRON (O) was AUX (O) trained VERB (O) for ADP (O) 142 NUM (O) when ADV (O) early ADV (O) stopping VERB (O) was AUX (O) called VERB . (O) 
In ADP (O) the DET (O) fifth NOUN (O) model NOUN , (O) 0.5 NUM (O) ([learning NOUN (B) rate NOUN (I)]) of ADP (O) SGD PROPN (O) and CCONJ (O) 0.8 NUM (O) (momentum NOUN) (O) was AUX (O) set VERB , (O) and CCONJ (O) when ADV (O) PER PROPN (O) stopped VERB (O) improving VERB (O) for ADP (O) about ADP (O) 50 NUM (O) [epochs NOUN (B)] , the DET (O) learning NOUN (O) rate NOUN (O) was AUX (O) multiplied ADJ (O) by ADP (O) 4/5 NUM . (O) 
The DET (O) numbers NOUN (O) of ADP (O) [epochs NOUN (B)] for ADP (O) this DET (O) model NOUN (O) reached VERB (O) 147 NUM (O) and CCONJ (O) 135 NUM (O) for ADP (O) [CMUDict NOUN (B)] and CCONJ (O) [NetTalk PROPN (B)] , respectively ADV . (O) 
In ADP (O) all DET (O) proposed VERB (O) models NOUN , (O) the DET (O) patience NOUN (O) of ADP (O) early ADV (O) stopping VERB (O) was AUX (O) set NOUN (O) to ADP (O) 50 NUM (O) in ADP (O) the DET (O) [Adam PROPN (B) optimizer NOUN (I)] and CCONJ (O) 30 NUM (O) in ADP (O) the DET (O) SGD PROPN (O) optimizer PROPN . (O) 
For ADP (O) [NetTalk PROPN (B) experiments NOUN (I)] , the DET (O) sizes VERB (O) of ADP (O) the DET (O) input NOUN (O) and CCONJ (O) [output NOUN (B) layers NOUN (I)] are AUX (O) as SCONJ (O) follows VERB (O) : input—{(length NOUN (O) of ADP (O) the DET (O) longest ADJ (O) input NOUN (O) (19 NUM) (O) × PROPN (O) number NOUN (O) of ADP (O) [graphemes NOUN (B)] (26 NUM) (O) }) (O) ; and CCONJ (O) output—{length PROPN (O) of ADP (O) the DET (O) longest ADJ (O) output NOUN (O) (19 NUM) (O) × PROPN (O) number NOUN (O) of ADP (O) [phonemes NOUN (B)] (52 NUM) (O) } . (O) 
We PRON (O) converted VERB (O) inputs VERB (O) to ADP (O) 26-dimensional NUM (O) and CCONJ (O) outputs NOUN (O) to ADP (O) 52-dimensional NUM (O) [one NUM - hot ADJ (B) vector NOUN (I) representations NOUN (I)] as SCONJ (O) in ADP (O) case NOUN (O) of ADP (O) [CMUDict NOUN (B)] . 
The DET (O) same ADJ (O) model NOUN (O) structure NOUN (O) was AUX (O) used VERB (O) as SCONJ (O) with ADP (O) the DET (O) [CMUDict NOUN (B) experiments NOUN (I)] . 
Moreover ADV , (O) the DET (O) implementation NOUN (O) of ADP (O) a NOUN (O) single ADJ (O) [convolutional NOUN (B) layer NOUN (I)] on ADP (O) [input NOUN (B) data NOUN (I)] is AUX (O) presented VERB (O) in ADP (O) Figure NOUN . (O) 
The DET (O) input NOUN (O) is AUX (O) a NOUN (O) [one NUM - hot ADJ (B) vector NOUN (I)] of ADP (O) ‘ PUNCT (O) ARREST NOUN ’ PUNCT (O) ; 64 NUM (O) filters NOUN (O) of ADP (O) (input NOUN (O) length NOUN) (O) ×3 PUNCT (O) are AUX (O) applied VERB (O) to ADP (O) the DET (O) input NOUN . (O) 
In ADP (O) other ADJ (O) words NOUN , (O) the DET (O) input NOUN (O) is AUX (O) convolved VERB (O) with ADP (O) 64 NUM (O) [feature NOUN (B) maps NOUN (I)] , which DET (O) produce NOUN (O) the DET (O) output NOUN (O) of ADP (O) the DET (O) [convolutional NOUN (B) layer NOUN (I)] . 
Zero PROPN (O) padding NOUN (O) was AUX (O) used VERB (O) to ADP (O) ensure VERB (O) that DET (O) the DET (O) output NOUN (O) of ADP (O) the DET (O) convolution NOUN (O) layer NOUN (O) has AUX (O) the DET (O) same ADJ (O) dimension NOUN (O) as SCONJ (O) the DET (O) input NOUN . (O) 
During ADP (O) training NOUN , (O) the DET (O) filter NOUN (O) weights NOUN (O) are AUX (O) optimized VERB (O) to ADP (O) produce NOUN (O) lower ADJ (O) loss NOUN (O) values NOUN . (O) 

 Figure NOUN . (O) Implementation NOUN (O) of ADP (O) a NOUN (O) single ADJ (O) [convolutional NOUN (B) layer NOUN (I)] with ADP (O) 64 NUM (O) filters NOUN (O) of ADP (O) size NOUN (O) (input NOUN (O) length NOUN) (O) ×3 PUNCT (O) to ADP (O) the DET (O) [input NOUN (B) data NOUN (I)] . 

 During ADP (O) inference NOUN , (O) prediction NOUN (O) of ADP (O) the DET (O) graphemes VERB (O) sequence NOUN (O) is AUX (O) decoded VERB (O) until ADP (O) < /EP NUM (O) > , (O) and CCONJ (O) the DET (O) length NOUN (O) of ADP (O) input NOUN (O) and CCONJ (O) output NOUN (O) are AUX (O) not PART (O) considered VERB . (O) 

 Evaluation NOUN (O) and CCONJ (O) Results NOUN (O) 
  
 NVidia PROPN (O) Titan PROPN (O) Xp NOUN (O) (12 NUM (O) GB PROPN) (O) and CCONJ (O) NVidia PROPN (O) Titan PROPN (O) X NOUN (O) (12 NUM (O) GB PROPN) (O) [GPU PROPN (B)] cards NOUN (O) hosted VERB (O) in ADP (O) two NUM (O) i7 PROPN (O) workstations NOUN (O) with ADP (O) 32 NUM (O) GB PROPN (O) RAM PROPN (O) served VERB (O) for ADP (O) training NOUN (O) and CCONJ (O) inference NOUN . (O) 
Ubuntu PROPN (O) 14.04 NUM (O) with ADP (O) [Cuda PROPN (B)] 8.0 NUM (O) and CCONJ (O) cuDNN NOUN (O) 5.0 NUM (O) was AUX (O) used VERB (O) as SCONJ (O) a NOUN (O) general NOUN (O) software NOUN (O) architecture NOUN . (O) 
For ADP (O) training NOUN (O) and CCONJ (O) evaluation NOUN , (O) the DET (O) Keras PROPN (O) deep ADJ (O) learning NOUN (O) framework NOUN (O) with ADP (O) Theano PROPN (O) backend NOUN (O) was AUX (O) our DET (O) environment NOUN . (O) 
For ADP (O) evaluation NOUN , (O) the DET (O) standard NOUN (O) and CCONJ (O) commonly ADV (O) used VERB (O) measurements NOUN (O) of ADP (O) [phoneme NOUN (B) error NOUN (I) rate NOUN (I)] (PER PROPN) (O) and CCONJ (O) [word NOUN (B) error NOUN (I) rate NOUN (I)] ([WER NOUN (B)]) were AUX (O) calculated VERB . (O) 
PER PROPN (O) was AUX (O) used VERB (O) to ADP (O) measure NOUN (O) the DET (O) distance NOUN (O) between ADP (O) the DET (O) predicted VERB (O) [phoneme NOUN (B) sequence NOUN (I)] and CCONJ (O) reference NOUN (O) pronunciation NOUN (O) divided VERB (O) by ADP (O) the DET (O) number NOUN (O) of ADP (O) [phonemes NOUN (B)] in ADP (O) the DET (O) reference NOUN (O) pronunciation NOUN . (O) 
Edit PROPN (O) distance NOUN (O) (also ADV (O) known VERB (O) as SCONJ (O) Levenshtein PROPN (O) distance NOUN) (O) is AUX (O) the DET (O) minimum NOUN (O) number NOUN (O) of ADP (O) insertions NOUN (O) (I) , (O) deletions NOUN (O) (D NOUN) (O) and CCONJ (O) substitutions NOUN (O) (S PROPN) , (O) that SCONJ (O) are AUX (O) required VERB (O) to ADP (O) transform NOUN (O) one NUM (O) sequence NOUN (O) into ADP (O) the DET (O) other ADJ . (O) 
If SCONJ (O) there PRON (O) are AUX (O) multiple NOUN (O) pronunciation NOUN (O) variants NOUN (O) for ADP (O) a NOUN (O) word NOUN (O) in ADP (O) the DET (O) [reference NOUN (B) data NOUN (I)] , the DET (O) variant NOUN (O) that SCONJ (O) has AUX (O) the DET (O) smallest ADJ (O) Levenshtein PROPN (O) distance NOUN (O) to ADP (O) the DET (O) candidate NOUN (O) is AUX (O) used VERB . (O) 
Levenshtein PROPN (O) distance NOUN (O) can VERB (O) be AUX (O) calculated VERB (O) by ADP (O) dynamic PROPN (O) programming NOUN (O) method NOUN . (O) 
For ADP (O) [WER NOUN (B)] computation NOUN , (O) which DET (O) is AUX (O) only ADV (O) counted VERB (O) if SCONJ (O) the DET (O) predicted VERB (O) pronunciation NOUN (O) does AUX (O) not PART (O) match NOUN (O) any DET (O) reference NOUN (O) pronunciation NOUN , (O) the DET (O) number NOUN (O) of ADP (O) word NOUN (O) errors NOUN (O) is AUX (O) divided VERB (O) by ADP (O) the DET (O) total NOUN (O) number NOUN (O) of ADP (O) unique ADJ (O) words NOUN (O) in ADP (O) the DET (O) reference NOUN . (O) 
After ADP (O) training NOUN (O) the DET (O) model NOUN , (O) predictions NOUN (O) were AUX (O) run NOUN (O) on ADP (O) the DET (O) [test NOUN (B) dataset NOUN (I)] . 
The DET (O) results VERB (O) of ADP (O) evaluation NOUN (O) on ADP (O) the DET (O) [CMUDict NOUN (B) dataset NOUN (I)] are AUX (O) shown VERB (O) in ADP (O) Table NOUN . (O) 
The DET (O) first ADJ (O) and CCONJ (O) second NOUN (O) columns NOUN (O) show NOUN (O) the DET (O) model NOUN (O) number NOUN (O) and CCONJ (O) the DET (O) applied VERB (O) architecture NOUN , (O) respectively ADV . (O) 
The DET (O) third NOUN (O) and CCONJ (O) fourth ADJ (O) columns NOUN (O) show NOUN (O) the DET (O) PER PROPN (O) and CCONJ (O) [WER NOUN (B) values NOUN (I)] . 
The DET (O) fifth NOUN (O) column NOUN (O) of ADP (O) Table NOUN (O) contains VERB (O) the DET (O) average NOUN (O) sum NOUN (O) of ADP (O) training NOUN (O) and CCONJ (O) validation NOUN (O) time NOUN (O) of ADP (O) one NUM (O) epoch PROPN . (O) 
The DET (O) last ADJ (O) two NUM (O) columns NOUN (O) present NOUN (O) information NOUN (O) about ADP (O) the DET (O) size NOUN (O) of ADP (O) models NOUN , (O) which DET (O) shows VERB (O) the DET (O) number NOUN (O) of ADP (O) parameters NOUN (O) (weights NOUN) (O) and CCONJ (O) the DET (O) number NOUN (O) of ADP (O) [epochs NOUN (B)] to PART (O) reach VERB (O) minimum NOUN (O) validation NOUN (O) loss NOUN . (O) 
According VERB (O) to ADP (O) the DET (O) results VERB , (O) the DET (O) [encoder NOUN - decoder NOUN (B) Bi PROPN - LSTM PROPN (I) architecture NOUN (I)] (Model NOUN (O) 2 X) (O) outperforms NOUN (O) the DET (O) first ADJ (O) model NOUN , (O) as SCONJ (O) expected VERB . (O) 
However ADV , (O) [attention NOUN - based VERB (B) Model PROPN (I)] 1 NUM (O) (called VERB (O) Model NOUN (O) 1A PROPN (O) in ADP (O) Table NOUN) (O) outperforms NOUN (O) Model NOUN (O) 2 NUM (O) in ADP (O) terms NOUN (O) of ADP (O) PER PROPN . (O) 
The DET (O) best ADJ (O) [WER NOUN (B)] and CCONJ (O) PER PROPN (O) values NOUN (O) are AUX (O) achieved VERB (O) by ADP (O) the DET (O) fifth NOUN (O) model NOUN (O) : PER PROPN (O) is AUX (O) 4.81 NUM (O) % , (O) and CCONJ (O) [WER NOUN (B)] is AUX (O) 25.13 NUM (O) % . (O) 
[Attention NOUN - based VERB (B) Model PROPN (I)] 2 NUM (O) (called VERB (O) Model NOUN (O) 2A PROPN (O) in ADP (O) Table NOUN) (O) approaches VERB (O) the DET (O) best ADJ (O) results NOUN (O) in ADP (O) terms NOUN (O) of ADP (O) both DET (O) PER PROPN (O) and CCONJ (O) [WER NOUN (B)] . 
However ADV , (O) the DET (O) number NOUN (O) of ADP (O) parameters NOUN (O) of ADP (O) Model NOUN (O) 2A PROPN (O) is AUX (O) twice ADV (O) as SCONJ (O) high ADJ (O) as SCONJ (O) for ADP (O) Model NOUN (O) 5 NUM . (O) 
Although SCONJ (O) the DET (O) fourth ADJ (O) model NOUN (O) was AUX (O) faster ADV (O) than SCONJ (O) all DET (O) of ADP (O) the DET (O) other ADJ (O) models NOUN , (O) both DET (O) the DET (O) PER PROPN (O) and CCONJ (O) [WER NOUN (B)] of ADP (O) this DET (O) model NOUN (O) were AUX (O) the DET (O) highest ADJ (O) ; however ADV , (O) they PRON (O) are AUX (O) still ADV (O) competitive ADJ . (O) 
Moreover ADV , (O) this DET (O) model NOUN (O) also ADV (O) has AUX (O) the DET (O) fewest NOUN (O) parameters NOUN . (O) 

 Table NOUN . (O) Results VERB (O) on ADP (O) the DET (O) [CMUDict NOUN (B) dataset NOUN (I)] . 



 We PRON (O) compared VERB (O) the DET (O) performance NOUN (O) of ADP (O) the DET (O) fifth NOUN (O) model NOUN (O) on ADP (O) both DET (O) [CMUDict NOUN (B)] and CCONJ (O) [NetTalk PROPN (B)] with ADP (O) previously ADV (O) achieved VERB (O) state NOUN - of ADP - the DET - art NOUN (O) results VERB . (O) 
These DET (O) comparisons NOUN (O) are AUX (O) presented VERB (O) in ADP (O) Table NOUN . (O) 
The DET (O) first ADJ (O) column NOUN (O) shows VERB (O) the DET (O) dataset NOUN , (O) the DET (O) second NOUN (O) column NOUN (O) presents VERB (O) the DET (O) method NOUN (O) used VERB (O) in ADP (O) previous ADJ (O) solutions NOUN (O) with ADP (O) references NOUN , (O) PER PROPN (O) and CCONJ (O) [WER NOUN (B)] columns NOUN (O) tell VERB (O) the DET (O) results VERB (O) of ADP (O) the DET (O) referred VERB (O) models NOUN . (O) 
Table NOUN (O) clearly ADV (O) shows VERB (O) that SCONJ (O) our DET (O) fifth NOUN (O) model NOUN (O) outperforms NOUN (O) the DET (O) previous ADJ (O) solutions NOUN (O) by ADP (O) PER PROPN (O) on ADP (O) each DET (O) dataset NOUN , (O) except SCONJ (O) for ADP . (O) 
For ADP (O) [NetTalk PROPN (B)] , we PRON (O) were AUX (O) able ADJ (O) to ADP (O) significantly ADV (O) surpass NOUN (O) the DET (O) previous ADJ (O) state NOUN - of ADP - the DET - art NOUN , (O) but CCONJ (O) a NOUN (O) better ADJ (O) [WER NOUN (B)] was AUX (O) obtained VERB (O) by ADP (O) with ADP (O) an DET (O) [encoder NOUN - decoder NOUN (B) network NOUN (I)] based VERB (O) on ADP (O) an DET (O) [attention NOUN (B) mechanism NOUN (I)] . 
We PRON (O) should VERB (O) point NOUN (O) out NOUN (O) that SCONJ (O) the DET (O) results VERB (O) of ADP (O) the DET (O) fifth NOUN (O) model NOUN (O) are AUX (O) very ADV (O) close NOUN (O) to ADP (O) those DET (O) obtained VERB (O) by ADP . (O) 

 Table NOUN . (O) Comparison PROPN (O) of ADP (O) best ADJ (O) previous ADJ (O) results VERB (O) of ADP (O) [G2P NOUN (B) models NOUN (I)] with ADP (O) our DET (O) fifth NOUN (O) model NOUN (O) ([encoder NOUN (B)] is AUX (O) a NOUN (O) [CNN PROPN (B)] with ADP (O) [residual ADJ (B) connections NOUN (I)] , [Bi PROPN - LSTM PROPN (B) decoder NOUN (I)]) on ADP (O) [CMUDict NOUN (B)] and CCONJ (O) [NetTalk PROPN (B)] . 


 The DET (O) proposed VERB (O) best ADJ (O) model NOUN (O) in ADP (O) consists VERB (O) of ADP (O) the DET (O) combination NOUN (O) of ADP (O) the DET (O) sequitur NOUN (O) [G2P NOUN (B)] (model NOUN (O) order NOUN (O) 8) NUM (O) and CCONJ (O) [seq2seq NOUN - attention NOUN (B)] ([Bi PROPN - LSTM PROPN (B)] 512 NUM (O) × PROPN (O) 3 X) (O) and CCONJ (O) multitask NOUN (O) learning NOUN (O) (ARPAbet NOUN (O) / [IPA PROPN (B)]) , and CCONJ (O) although SCONJ (O) the DET (O) [WER NOUN (B)] in ADP (O) their DET (O) case NOUN (O) is AUX (O) better ADJ , (O) Model NOUN (O) 5 NUM (O) has AUX (O) a NOUN (O) smaller ADJ (O) PER PROPN . (O) 
Although SCONJ (O) the DET (O) [encoder NOUN - decoder NOUN (B) LSTM PROPN (I)] by ADP (O) is AUX (O) similar ADJ (O) to ADP (O) our DET (O) first ADJ (O) model NOUN , (O) the DET (O) PER PROPN (O) is AUX (O) better ADJ (O) in ADP (O) our DET (O) case NOUN (O) ; the DET (O) [WER NOUN (B)] of ADP (O) both DET (O) models NOUN (O) is AUX (O) almost ADV (O) the DET (O) same ADJ . (O) 
Our DET (O) second NOUN (O) model NOUN (O) is AUX (O) comparable ADJ (O) with ADP , (O) in ADP (O) which DET (O) the DET (O) [Bi PROPN - LSTM PROPN (B) method NOUN (I)] was AUX (O) implemented VERB , (O) alignment NOUN (O) was AUX (O) also ADV (O) applied VERB . (O) 

 Discussion NOUN (O) 

 In ADP (O) this DET (O) section NOUN , (O) we PRON (O) discuss NOUN (O) the DET (O) results VERB (O) of ADP (O) the DET (O) previous ADJ (O) section NOUN (O) and CCONJ (O) analyze VERB (O) the DET (O) connection NOUN (O) between ADP (O) PER PROPN (O) values NOUN (O) and CCONJ (O) word NOUN (O) length NOUN , (O) furthermore ADV (O) the DET (O) position NOUN (O) of ADP (O) the DET (O) error NOUN (O) within ADP (O) the DET (O) word NOUN . (O) 

 We PRON (O) categorize NOUN (O) the DET (O) word NOUN (O) length NOUN (O) into ADP (O) 3 NUM (O) classes NOUN (O) : short ADJ (O) (shorter ADJ (O) than SCONJ (O) 6 NUM (O) characters NOUN) , (O) medium NOUN (O) (between ADP (O) 6 NUM (O) and CCONJ (O) 10 NUM (O) characters NOUN) , (O) long ADJ (O) (more ADJ (O) than SCONJ (O) 10 NUM (O) characters NOUN) . (O) 
According VERB (O) to ADP (O) this DET (O) categorization NOUN , (O) there PRON (O) were AUX (O) 4306 NUM (O) short ADJ , (O) 5993 NUM (O) medium NOUN (O) and CCONJ (O) 1028 NUM (O) long ADJ (O) words NOUN (O) in ADP (O) the DET (O) [CMUDict NOUN (B) dataset NOUN (I)] . 
In ADP (O) this DET (O) analysis NOUN , (O) we PRON (O) ignored VERB (O) approximately ADV (O) 600 NUM (O) words NOUN (O) that SCONJ (O) have AUX (O) multiple NOUN (O) pronunciation NOUN (O) variants NOUN (O) in ADP (O) the DET (O) [reference NOUN (B) data NOUN (I)] . 
The DET (O) results VERB (O) of ADP (O) this DET (O) comparison NOUN (O) are AUX (O) presented VERB (O) in ADP (O) Figurea PROPN . (O) 
For ADP (O) short ADJ (O) words NOUN , (O) all DET (O) models NOUN (O) show NOUN (O) similar ADJ (O) PERs PROPN (O) ; for ADP (O) medium NOUN (O) length NOUN (O) words NOUN , (O) except SCONJ (O) the DET (O) [end NOUN - to ADP - end NOUN (B) CNN PROPN (I) model NOUN (I)] (fourth ADJ (O) model NOUN) , (O) the DET (O) other ADJ (O) models NOUN (O) resulted VERB (O) in ADP (O) similar ADJ (O) error NOUN (O) ; for ADP (O) long ADJ (O) words NOUN , (O) [encoder NOUN (B) CNN PROPN (I)] with ADP (O) [residual ADJ (B) connection NOUN (I)] , [decoder NOUN (B) Bi PROPN - LSTM PROPN (I)] (fourth ADJ (O) model NOUN) (O) and CCONJ (O) [encoder NOUN (B) CNN PROPN (I)] , [decoder NOUN (B) Bi PROPN - LSTM PROPN (I)] (third ADJ (O) model NOUN) (O) got VERB (O) similar ADJ (O) minimum NOUN (O) errors NOUN . (O) 
The DET (O) fourth ADJ (O) model NOUN (O) showed VERB (O) the DET (O) highest ADJ (O) error NOUN (O) in ADP (O) both DET (O) medium NOUN (O) and CCONJ (O) long ADJ (O) length NOUN (O) words NOUN . (O) 
According VERB (O) to ADP (O) Figurea PROPN , (O) the DET (O) advantage NOUN (O) of ADP (O) [Bi PROPN - LSTM PROPN - based VERB (B) models NOUN (I)] is AUX (O) clearly ADV (O) shown VERB (O) for ADP (O) learning NOUN (O) long ADJ (O) sequences NOUN . (O) 
Moreover ADV , (O) errors NOUN (O) occurring NOUN (O) in ADP (O) the DET (O) first ADJ (O) half NOUN (O) of ADP (O) the DET (O) pronunciation NOUN (O) (in ADP (O) the DET (O) reference NOUN) (O) increases VERB (O) the DET (O) probability NOUN (O) of ADP (O) predicting VERB (O) incorrect NOUN (O) [phonemes NOUN (B)] in ADP (O) the DET (O) second NOUN (O) half NOUN . (O) 
Still ADV , (O) a NOUN (O) correctly ADV (O) predicted VERB (O) first ADJ (O) half NOUN (O) can VERB (O) not PART (O) guarantee NOUN (O) a NOUN (O) correctly ADV (O) predicted VERB (O) second NOUN (O) half NOUN . (O) 
In ADP (O) our DET (O) experiments NOUN , (O) convolutional ADJ (O) architectures VERB (O) also ADV (O) performed VERB (O) well INTJ (O) on ADP (O) short ADJ (O) and CCONJ (O) on ADP (O) long ADJ - range NOUN (O) dependencies NOUN . (O) 
Our DET (O) intuition NOUN (O) is AUX (O) that SCONJ (O) the DET (O) [residual ADJ (B) connections NOUN (I)] enable VERB (O) the DET (O) network NOUN (O) to ADP (O) consider VERB (O) features VERB (O) learned VERB (O) by ADP (O) lower ADJ (O) and CCONJ (O) higher ADJ (O) layers NOUN (O) — PUNCT (O) which DET (O) represents VERB (O) shorter ADJ (O) and CCONJ (O) longer ADJ (O) dependencies NOUN . (O) 
We PRON (O) also ADV (O) analyzed VERB (O) the DET (O) position NOUN (O) of ADP (O) the DET (O) errors NOUN (O) in ADP (O) the DET (O) reference NOUN (O) pronunciation NOUN (O) : we PRON (O) investigated VERB (O) whether SCONJ (O) the DET (O) error NOUN (O) occurred VERB (O) in ADP (O) the DET (O) first ADJ (O) or CCONJ (O) in ADP (O) the DET (O) second NOUN (O) half NOUN (O) of ADP (O) the DET (O) word NOUN . (O) 
The DET (O) type NOUN (O) of ADP (O) error NOUN (O) can VERB (O) be AUX (O) insertion NOUN (O) (I) , (O) deletion NOUN (O) (D NOUN) (O) and CCONJ (O) substitution NOUN (O) (S PROPN) . (O) 
By ADP (O) using VERB (O) this DET (O) position NOUN (O) information NOUN , (O) we PRON (O) can VERB (O) analyze VERB (O) the DET (O) distribution NOUN (O) of ADP (O) these DET (O) errors NOUN (O) across ADP (O) the DET (O) first ADJ (O) or CCONJ (O) second NOUN (O) half NOUN (O) of ADP (O) the DET (O) word NOUN . (O) 
The DET (O) position NOUN (O) of ADP (O) error NOUN (O) was AUX (O) calculated VERB (O) by ADP (O) enumerating VERB (O) [graphemes NOUN (B)] in ADP (O) the DET (O) reference NOUN . (O) 
For ADP (O) insertion NOUN (O) error NOUN (O) (I) , (O) the DET (O) position NOUN (O) of ADP (O) the DET (O) previous ADJ (O) [grapheme NOUN (B)] was AUX (O) taken VERB (O) into ADP (O) account NOUN . (O) 
The DET (O) example NOUN (O) below ADP (O) describes VERB (O) the DET (O) process NOUN (O) details NOUN (O) : 
Word NOUN (O) : Acknowledgement NOUN (O) 
Enumeration NOUN (O) : 0 PUNCT (O) 1 NUM (O) 2 NUM (O) 3 NUM (O) 4 NUM (O) 5 NUM (O) 6 NUM (O) 7 NUM (O) 8 NUM (O) 9 NUM (O) 10 NUM (O) 11 NUM (O) 12 NUM (O) 
Reference NOUN (O) : (EP PROPN (O) AE NOUN (O) K NOUN (O) N NOUN (O) AA PROPN (O) L NOUN (O) IH PROPN (O) JH PROPN (O) M NOUN (O) AH PROPN (O) N NOUN (O) T NOUN (O) /EP NUM) (O) 
Prediction NOUN (O) : (EP PROPN (O) IH PROPN (O) K PROPN (O) N NOUN (O) AA PROPN (O) L NOUN (O) IH PROPN (O) JH PROPN (O) IH PROPN (O) JH PROPN (O) AH PROPN (O) N NOUN (O) T NOUN (O) /EP NUM) (O) 
Types PROPN (O) of ADP (O) errors NOUN (O) : S NOUN (O) S NOUN (O) S NOUN (O) I (O) S NOUN (O) I (O) 
Position NOUN (O) : (1,8,8 X) (O) 
As SCONJ (O) example NOUN (O) the DET (O) example NOUN (O) shows VERB , (O) two NUM (O) substitutions NOUN (O) substitutions NOUN (O) (S PROPN) (O) and CCONJ (O) (I) (O) occurred VERB (O) in ADP (O) our DET (O) fifth NOUN (O) model NOUN (O) output NOUN . (O) 

 Figure NOUN (O) shows VERB (O) position NOUN (O) errors NOUN (O) calculated VERB (O) the DET (O) models NOUN (O) models NOUN (O) on ADP (O) on ADP (O) the DET (O) the DET (O) reference NOUN (O) [reference NOUN (B) dataset NOUN (I)] . 

 Furthermore ADV , (O) in ADP (O) all DET (O) models NOUN (O) presented VERB (O) here ADV , (O) PER PROPN (O) is AUX (O) better ADJ (O) than SCONJ (O) the DET (O) previous ADJ (O) results VERB (O) on ADP (O) [CMUDict NOUN (B)] except SCONJ (O) the DET (O) first ADJ (O) four NUM (O) models NOUN (O) in ADP , (O) while SCONJ (O) [WER NOUN (B)] is AUX (O) still ADV (O) reasonable ADJ . (O) 
This DET (O) means VERB (O) that SCONJ (O) even ADV (O) most ADJ (O) of ADP (O) the DET (O) incorrect ADJ (O) predictions NOUN (O) are AUX (O) very ADV (O) close NOUN (O) to ADP (O) the DET (O) reference NOUN (O) ; therefore ADV , (O) they PRON (O) have AUX (O) small ADJ (O) PER PROPN . (O) 
Accordingly ADV , (O) we PRON (O) need NOUN (O) to ADP (O) analyze VERB (O) the DET (O) incorrect ADJ (O) predictions NOUN (O) (outputs NOUN) (O) for ADP (O) each DET (O) model NOUN (O) to ADP (O) see VERB (O) how ADV (O) many ADJ (O) [phonemes NOUN (B)] are AUX (O) correct ADJ (O) in ADP (O) the DET (O) reference NOUN . (O) 
In ADP (O) the DET (O) fifth NOUN (O) model NOUN , (O) 25.3 NUM (O) % of ADP (O) the DET (O) [test NOUN (B) data NOUN (I)] are AUX (O) not PART (O) correct ADJ (O) (about ADP (O) 3000 NUM (O) test NOUN (O) samples NOUN) . (O) 
After ADP (O) the DET (O) analysis NOUN (O) of ADP (O) these DET (O) predictions NOUN , (O) more ADJ (O) than SCONJ (O) half NOUN (O) of ADP (O) them PRON (O) have AUX (O) 1 NUM (O) incorrect NOUN (O) [phoneme NOUN (B)] . 
In ADP (O) particular ADJ , (O) the DET (O) PER PROPN (O) for ADP (O) 59 NUM (O) test NOUN (O) samples NOUN (O) is AUX (O) higher ADJ (O) than SCONJ (O) 50 NUM (O) % (11 NUM (O) test NOUN (O) samples NOUN (O) are AUX (O) greater ADJ (O) than SCONJ (O) 60 NUM (O) % , (O) and CCONJ (O) only ADV (O) 1 NUM (O) test NOUN (O) sample NOUN (O) is AUX (O) more ADJ (O) than SCONJ (O) 70 NUM (O) %) . (O) 
These DET (O) percentages NOUN (O) in ADP (O) the DET (O) other ADJ (O) presented VERB (O) models NOUN (O) are AUX (O) more ADJ (O) or CCONJ (O) less ADJ (O) the DET (O) same ADJ . (O) 
Generally ADV , (O) the DET (O) same ADJ (O) 1000 NUM (O) words NOUN (O) are AUX (O) incorrectly ADV (O) predicted VERB (O) by ADP (O) all DET (O) presented VERB (O) models NOUN . (O) 
We PRON (O) can VERB (O) see VERB (O) different ADJ (O) types NOUN (O) of ADP (O) error NOUN (O) when ADV (O) generating NOUN (O) [phoneme NOUN (B) sequences NOUN (I)] . 
One NUM (O) of ADP (O) these DET (O) errors NOUN (O) is AUX (O) that SCONJ (O) some DET (O) [phonemes NOUN (B)] are AUX (O) unnecessarily NOUN (O) generated VERB (O) multiple NOUN (O) times NOUN . (O) 
For ADP (O) example NOUN , (O) for ADP (O) the DET (O) word NOUN (O) YELLOWKNIFE PROPN , (O) reference NOUN (O) is AUX (O) (Y NOUN (O) EH PROPN (O) L NOUN (O) OWN PROPN (O) AY NOUN (O) F PROPN) , (O) the DET (O) prediction NOUN (O) of ADP (O) Model NOUN (O) 5 NUM (O) for ADP (O) this DET (O) word NOUN (O) is AUX (O) (Y NOUN (O) EH PROPN (O) L NOUN (O) OW PROPN (O) K NOUN (O) N NOUN (O) N NOUN (O) F PROPN) , (O) where ADV (O) the DET (O) character NOUN (O) N NOUN (O) was AUX (O) generated VERB (O) twice ADV . (O) 
Another DET (O) error NOUN (O) type NOUN (O) regards VERB (O) [sequences NOUN (B) of ADP (I) graphemes VERB (I)] that DET (O) are AUX (O) rarely ADV (O) represented VERB (O) in ADP (O) the DET (O) training NOUN (O) process NOUN . (O) 
For ADP (O) example NOUN , (O) for ADP (O) the DET (O) word NOUN (O) ZANGHI PROPN (O) Model NOUN (O) 5 NUM (O) output NOUN (O) is AUX (O) (Z NOUN (O) AE NOUN (O) N NOUN (O) G NOUN) , (O) while SCONJ (O) the DET (O) reference NOUN (O) is AUX (O) (Z NOUN (O) AA PROPN (O) N NOUN (O) G NOUN (O) IY PROPN) . (O) 
The DET (O) [graphemes NOUN (B)] ‘ PUNCT (O) NGHI PROPN ’ PUNCT (O) appeared VERB (O) only ADV (O) 7 NUM (O) times NOUN (O) in ADP (O) the DET (O) [training NOUN (B) data NOUN (I)] . 
Furthermore ADV , (O) many ADJ (O) words NOUN (O) are AUX (O) of ADP (O) foreign ADJ (O) origin NOUN , (O) for ADP (O) example NOUN , (O) GDANSK PROPN (O) is AUX (O) Polish PROPN (O) a NOUN (O) city NOUN , (O) SCICCHITANO PROPN (O) is AUX (O) an DET (O) Italian PROPN (O) name NOUN , (O) KOVACIK PROPN (O) is AUX (O) a NOUN (O) Turkish ADJ (O) surname NOUN . (O) 
Generating NOUN (O) [phoneme NOUN (B) sequences NOUN (I)] of ADP (O) abbreviations NOUN (O) is AUX (O) one NUM (O) of ADP (O) the DET (O) hard ADJ (O) challenges NOUN . (O) 
For ADP (O) example NOUN , (O) LPN PROPN , (O) INES PROPN (O) are AUX (O) shown VERB (O) with ADP (O) their DET (O) references NOUN (O) and CCONJ (O) the DET (O) prediction NOUN (O) form NOUN (O) of ADP (O) Model NOUN (O) 5 NUM (O) in ADP (O) Table NOUN (O) :     

 Table NOUN . (O) Examples PROPN (O) of ADP (O) errors NOUN (O) predicted VERB (O) by ADP (O) Model NOUN (O) 5 NUM . (O) 

 In ADP (O) the DET (O) proposed VERB (O) models NOUN , (O) we PRON (O) were AUX (O) able ADJ (O) to ADP (O) achieve VERB (O) smaller ADJ (O) PERs PROPN (O) with ADP (O) different ADJ (O) [hyperparameter NOUN (B) settings NOUN (I)] , but CCONJ (O) [WERs NOUN (B)] showed VERB (O) different ADJ (O) behavior NOUN , (O) in ADP (O) contrast NOUN (O) with ADP (O) what PRON (O) we PRON (O) expected VERB . (O) 
To NOUN (O) calculate NOUN (O) [WER NOUN (B)] , the DET (O) number NOUN (O) of ADP (O) word NOUN (O) errors NOUN (O) is AUX (O) divided VERB (O) by ADP (O) the DET (O) total NOUN (O) number NOUN (O) of ADP (O) unique ADJ (O) words NOUN (O) in ADP (O) the DET (O) reference NOUN . (O) 
These DET (O) word NOUN (O) errors NOUN (O) are AUX (O) counted VERB (O) only ADV (O) if SCONJ (O) the DET (O) predicted VERB (O) pronunciation NOUN (O) does AUX (O) not PART (O) match NOUN (O) any DET (O) reference NOUN (O) pronunciation NOUN . (O) 
Therefore ADV , (O) in ADP (O) the DET (O) generated VERB (O) [phoneme NOUN (B) sequences NOUN (I)] of ADP (O) words NOUN (O) that SCONJ (O) contained VERB (O) errors NOUN , (O) there PRON (O) is AUX (O) at ADP (O) least ADJ (O) one NUM (O) [phoneme NOUN (B) error NOUN (I)] . 
For ADP (O) that SCONJ (O) reason NOUN , (O) we PRON (O) calculated VERB (O) the DET (O) number NOUN (O) of ADP (O) word NOUN (O) errors NOUN (O) depending VERB (O) on ADP (O) the DET (O) number NOUN (O) of ADP (O) [phoneme NOUN (B) errors NOUN (I)] for ADP (O) all DET (O) proposed VERB (O) models NOUN (O) on ADP (O) [CMUDict NOUN (B)] , as SCONJ (O) presented VERB (O) in ADP (O) Figure NOUN . (O) 

 Figure NOUN . (O) Number NOUN (O) of ADP (O) word NOUN (O) errors NOUN (O) depending VERB (O) on ADP (O) the DET (O) number NOUN (O) of ADP (O) [phoneme NOUN (B) errors NOUN (I)] for ADP (O) all DET (O) models NOUN . (O) 

 In ADP (O) the DET (O) case NOUN (O) of ADP (O) each DET (O) model NOUN , (O) there PRON (O) are AUX (O) twice ADV (O) as SCONJ (O) many ADJ (O) words NOUN (O) with ADP (O) only ADV (O) one NUM (O) [phoneme NOUN (B) error NOUN (I)] than SCONJ (O) words NOUN (O) with ADP (O) two NUM (O) [phoneme NOUN (B) errors NOUN (I)] . 
Words NOUN (O) with ADP (O) one NUM (O) [phoneme NOUN (B) error NOUN (I)] significantly ADV (O) effect NOUN (O) the DET (O) [WER NOUN (B)] . 
The DET (O) number NOUN (O) of ADP (O) words NOUN (O) with ADP (O) two NUM (O) [phoneme NOUN (B) errors NOUN (I)] was AUX (O) the DET (O) greatest ADJ (O) in ADP (O) Model NOUN (O) 4 NUM (O) (908 NUM) , (O) and CCONJ (O) the DET (O) lowest NOUN (O) in ADP (O) Model NOUN (O) 5 NUM (O) (739 NUM) . (O) 
The DET (O) number NOUN (O) of ADP (O) words NOUN (O) with ADP (O) three NUM (O) [phoneme NOUN (B) errors NOUN (I)] was AUX (O) the DET (O) lowest NOUN (O) (230 NUM) (O) in ADP (O) Model NOUN (O) 5 NUM . (O) 
There PRON (O) was AUX (O) approximately ADV (O) the DET (O) same ADJ (O) number NOUN (O) of ADP (O) words NOUN (O) with ADP (O) four NUM (O) [phoneme NOUN (B) errors NOUN (I)] in ADP (O) Model NOUN (O) 2 NUM (O) and CCONJ (O) Model NOUN (O) 5 NUM (O) (84 NUM (O) in ADP (O) Model NOUN (O) 2 NUM (O) and CCONJ (O) 86 NUM (O) in ADP (O) Model NOUN (O) 5 NUM) . (O) 
There PRON (O) were AUX (O) very ADV (O) few ADJ (O) words NOUN (O) with ADP (O) five NUM (O) or CCONJ (O) more ADJ (O) [phoneme NOUN (B) errors NOUN (I)] in ADP (O) any DET (O) of ADP (O) the DET (O) models NOUN . (O) 
Model PROPN (O) 1 NUM (O) and CCONJ (O) Model NOUN (O) 3 NUM (O) have AUX (O) only ADV (O) 1 NUM (O) word NOUN (O) which DET (O) has AUX (O) seven NUM (O) [phoneme NOUN (B) errors NOUN (I)] ; Model NOUN (O) 5 NUM (O) has AUX (O) 2 NUM (O) words NOUN (O) ; Model NOUN (O) 4 NUM (O) has AUX (O) 6 NUM (O) words NOUN . (O) 
The DET (O) number NOUN (O) of ADP (O) words NOUN (O) with ADP (O) eight NUM (O) [phoneme NOUN (B) errors NOUN (I)] was AUX (O) 0 PUNCT (O) in ADP (O) Model NOUN (O) 3 NUM (O) and CCONJ (O) Model NOUN (O) 5 NUM (O) ; 1 NUM (O) in ADP (O) Model NOUN (O) 4 NUM . (O) 
Figure NOUN (O) helps VERB (O) to ADP (O) understand VERB (O) why ADV (O) PER PROPN (O) in ADP (O) our DET (O) models NOUN (O) can VERB (O) be AUX (O) smaller ADJ (O) while SCONJ (O) [WER NOUN (B)] is AUX (O) higher ADJ . (O)    

 Conclusions PROPN (O) 
 
 In ADP (O) this DET (O) paper NOUN , (O) [convolutional NOUN (B) neural NOUN (I) networks NOUN (I)] for ADP (O) [grapheme NOUN - to ADP - phoneme NOUN (B) conversion NOUN (I)] are AUX (O) introduced VERB . (O) 
Five NUM (O) different ADJ (O) models NOUN (O) for ADP (O) the DET (O) [G2P NOUN (B) task NOUN (I)] are AUX (O) described VERB , (O) and CCONJ (O) the DET (O) results VERB (O) are AUX (O) compared VERB (O) to ADP (O) previously ADV (O) reported VERB (O) state NOUN - of ADP - the DET - art NOUN (O) research NOUN . (O) 
Our DET (O) models NOUN (O) are AUX (O) based VERB (O) on ADP (O) the DET (O) [seq2seq NOUN (B) architecture NOUN (I)] , and CCONJ (O) in ADP (O) the DET (O) fourth ADJ (O) and CCONJ (O) fifth NOUN (O) models NOUN , (O) we PRON (O) applied VERB (O) [CNNs PROPN (B)] with ADP (O) [residual ADJ (B) connections NOUN (I)] . 
The DET (O) fifth NOUN (O) model NOUN , (O) which DET (O) uses VERB (O) [convolutional NOUN (B) layers NOUN (I)] with ADP (O) [residual ADJ (B) connections NOUN (I)] as SCONJ (O) [encoder NOUN (B)] and CCONJ (O) [Bi PROPN - LSTM PROPN (B)] as SCONJ (O) [decoder NOUN (B)] outperformed VERB (O) most ADJ (O) the DET (O) previous ADJ (O) solutions NOUN (O) on ADP (O) the DET (O) [CMUDict NOUN (B)] and CCONJ (O) [NetTalk PROPN (B) datasets VERB (I)] in ADP (O) terms NOUN (O) of ADP (O) PER PROPN . (O) 
Furthermore ADV , (O) the DET (O) fourth ADJ (O) model NOUN , (O) which DET (O) contains VERB (O) [convolutional NOUN (B) layers NOUN (I)] only ADV , (O) is AUX (O) significantly ADV (O) faster ADV (O) than SCONJ (O) other ADJ (O) models NOUN (O) and CCONJ (O) still ADV (O) has AUX (O) competitive ADJ (O) accuracy NOUN . (O) 
Our DET (O) solution NOUN (O) achieved VERB (O) these DET (O) results VERB (O) without ADP (O) explicit NOUN (O) alignments NOUN . (O) 
The DET (O) experiments VERB (O) were AUX (O) conducted VERB (O) on ADP (O) a NOUN (O) test NOUN (O) set NOUN (O) corresponding VERB (O) to ADP (O) 9.8 NUM (O) % and CCONJ (O) 24.9 NUM (O) % of ADP (O) the DET (O) whole NOUN (O) [CMUDict NOUN (B)] and CCONJ (O) [NetTalk PROPN (B) databases NOUN (I)] , respectively ADV . (O) 
The DET (O) same ADJ (O) test NOUN (O) set NOUN (O) was AUX (O) used VERB (O) in ADP (O) all DET (O) cases NOUN , (O) so CCONJ (O) we PRON (O) consider VERB (O) the DET (O) results VERB (O) to ADP (O) be AUX (O) comparable ADJ . (O) 
To NOUN (O) draw NOUN (O) conclusions NOUN (O) on ADP (O) whether SCONJ (O) one NUM (O) model NOUN (O) is AUX (O) better ADJ (O) than SCONJ (O) another DET , (O) the DET (O) goal NOUN (O) must VERB (O) be AUX (O) defined VERB . (O) 
If SCONJ (O) inference NOUN (O) time NOUN (O) is AUX (O) crucial ADJ , (O) then ADV (O) smaller ADJ (O) model NOUN (O) sizes VERB (O) are AUX (O) favorable ADJ (O) (e.g. ADV , (O) Model NOUN (O) 4 NUM) , (O) but CCONJ (O) if SCONJ (O) lower ADJ (O) [WER NOUN (B)] and CCONJ (O) PER PROPN (O) are AUX (O) the DET (O) main ADJ (O) factors NOUN , (O) then ADV (O) Model NOUN (O) 5 NUM (O) outperforms NOUN (O) the DET (O) others NOUN . (O) 
The DET (O) results VERB (O) presented VERB (O) in ADP (O) this DET (O) paper NOUN (O) can VERB (O) be AUX (O) applied VERB (O) in ADP (O) [TTS PROPN (B) systems NOUN (I)] ; however ADV , (O) because SCONJ (O) of ADP (O) the DET (O) rapid ADJ (O) development NOUN (O) of ADP (O) deep ADJ (O) learning NOUN (O) further NOUN (O) aspects NOUN (O) will VERB (O) be AUX (O) investigated VERB , (O) like INTJ (O) dilated VERB (O) [convolutional NOUN (B) networks NOUN (I)] and CCONJ (O) neural NOUN (O) architecture NOUN (O) search VERB . (O) 
These DET (O) are AUX (O) possible ADJ (O) further NOUN (O) extensions NOUN (O) of ADP (O) the DET (O) current ADJ (O) research NOUN . (O) 
Author NOUN (O) Contributions PROPN (O) : All DET (O) authors NOUN (O) have AUX (O) read VERB (O) and CCONJ (O) approved VERB (O) the DET (O) final ADJ (O) manuscript NOUN . (O) 

 Acknowledgments NOUN (O) : The DET (O) research NOUN (O) presented VERB (O) in ADP (O) this DET (O) paper NOUN (O) has AUX (O) been AUX (O) supported VERB (O) by ADP (O) the DET (O) European PROPN (O) Union PROPN , (O) co VERB - financed ADJ (O) by ADP (O) the DET (O) European PROPN (O) Social PROPN (O) Fund PROPN (O) (EFOP-3.6.2 PROPN - 16 NUM - 2017 NUM - 00013 NUM) , (O) by ADP (O) the DET (O) BME PROPN - Artificial PROPN (O) Intelligence NOUN (O) FIKP PROPN (O) grant NOUN (O) of ADP (O) Ministry PROPN (O) of ADP (O) Human NOUN (O) Resources NOUN (O) (BME PROPN (O) FIKP PROPN - MI PROPN (O) / SC PROPN) , (O) by ADP (O) Doctoral PROPN (O) Research NOUN (O) Scholarship NOUN (O) of ADP (O) Ministry PROPN (O) of ADP (O) Human NOUN (O) Resources NOUN (O) (ÚNKP-18 PROPN - 4-BME-394 X) (O) in ADP (O) the DET (O) scope NOUN (O) of ADP (O) New PROPN (O) National PROPN (O) Excellence PROPN (O) Program PROPN , (O) by ADP (O) János PROPN (O) Bolyai PROPN (O) Research NOUN (O) Scholarship NOUN (O) of ADP (O) the DET (O) Hungarian PROPN (O) Academy PROPN (O) of ADP (O) Sciences PROPN , (O) by ADP (O) the DET (O) VUK PROPN (O) project NOUN (O) (AAL NOUN (O) 2014 NUM - 183 NUM) , (O) and CCONJ (O) the DET (O) DANSPLAT PROPN (O) project NOUN (O) (Eureka NOUN (O) 9944 NUM) . (O) 
We PRON (O) gratefully ADV (O) acknowledge NOUN (O) the DET (O) support NOUN (O) of ADP (O) NVIDIA PROPN (O) Corporation PROPN (O) with ADP (O) the DET (O) donation NOUN (O) of ADP (O) the DET (O) Titan PROPN (O) Xp NOUN (O) [GPU PROPN (B)] used VERB (O) for ADP (O) this DET (O) research NOUN . (O) 

 Conflicts NOUN (O) of ADP (O) Interest NOUN (O) : The DET (O) authors NOUN (O) declare NOUN (O) that SCONJ (O) they PRON (O) have AUX (O) no DET (O) competing VERB (O) interests NOUN . (O) 

 Abbreviations NOUN (O) 
[G2P NOUN (B)]               [Grapheme PROPN - to ADP - phoneme NOUN (B)] 
[ASR PROPN (B)]               [Automatic PROPN (B) Speech NOUN (I) Recognition PROPN (I)] 
[CNN PROPN (B)]               [Convolutional PROPN (B) neural NOUN (I) network NOUN (I)] 
PER PROPN (O)               [Phoneme PROPN (B) error NOUN (I) rate NOUN (I)] 
[WER NOUN (B)]               [Word NOUN (B) error NOUN (I) rate NOUN (I)] 
[Bi PROPN - LSTM PROPN (B)]           [bi ADJ - directional ADJ (B)] Long ADV (O) Short ADJ (O) Term NOUN (O) Memory NOUN (O) 
