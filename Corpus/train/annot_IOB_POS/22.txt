[TACOTRON PROPN (B)] : TOWARDS PROPN (O) [END NOUN - TO ADP - END NOUN (B) SPEECH PROPN (I) SYNTHESIS PROPN (I)] 

 ABSTRACT PROPN (O) 

 A NOUN (O) [text NOUN - to ADP - speech NOUN (B) synthesis NOUN (I) system NOUN (I)] typically ADV (O) consists VERB (O) of ADP (O) multiple NOUN (O) stages NOUN , (O) such ADJ (O) as SCONJ (O) a NOUN (O) text NOUN (O) analysis NOUN (O) frontend NOUN , (O) an DET (O) [acoustic ADJ (B) model NOUN (I)] and CCONJ (O) an DET (O) [audio NOUN (B) synthesis NOUN (I) module PROPN (I)] . 
Building NOUN (O) these DET (O) components NOUN (O) often ADV (O) requires VERB (O) extensive ADJ (O) domain NOUN (O) expertise NOUN (O) and CCONJ (O) may VERB (O) contain NOUN (O) brittle NOUN (O) design NOUN (O) choices NOUN . (O) 
In ADP (O) this DET (O) paper NOUN , (O) we PRON (O) present NOUN (O) [Tacotron PROPN (B)] , an DET (O) [end NOUN - to ADP - end NOUN (B) generative PROPN (I) text NOUN - to ADP - speech NOUN (I) model NOUN (I)] that DET (O) [synthesizes NOUN (B) speech NOUN (I)] directly ADV (O) from ADP (O) characters NOUN . (O) 
Given VERB (O) < text NOUN , (O) [audio NOUN (B)] > pairs NOUN , (O) the DET (O) model NOUN (O) can VERB (O) be AUX (O) trained VERB (O) completely ADV (O) from ADP (O) scratch NOUN (O) with ADP (O) random ADJ (O) initialization NOUN . (O) 
We PRON (O) present NOUN (O) several ADJ (O) key NOUN (O) techniques NOUN (O) to ADP (O) make VERB (O) the DET (O) sequence NOUN - tosequence NOUN (O) framework NOUN (O) perform VERB (O) well INTJ (O) for ADP (O) this DET (O) challenging NOUN (O) task NOUN . (O) 
[Tacotron PROPN (B)] achieves VERB (O) a NOUN (O) 3.82 NUM (O) subjective ADJ (O) 5-scale NUM (O) [mean NOUN (B) opinion NOUN (I) score NOUN (I)] on ADP (O) US PROPN (O) English PROPN , (O) outperforming VERB (O) a NOUN (O) production NOUN (O) [parametric NOUN (B) system NOUN (I)] in ADP (O) terms NOUN (O) of ADP (O) naturalness NOUN . (O) 
In ADP (O) addition NOUN , (O) since SCONJ (O) [Tacotron PROPN (B)] generates VERB (O) [speech NOUN (B)] at ADP (O) the DET (O) frame NOUN (O) level NOUN , (O) it PRON ’s VERB (O) substantially ADV (O) faster ADV (O) than SCONJ (O) sample NOUN - level NOUN (O) autoregressive ADJ (O) methods NOUN . (O)                                                     


 INTRODUCTION PROPN (O) 

 [Modern ADJ (B) text NOUN - to ADP - speech NOUN (I) (TTS PROPN) (I) pipelines NOUN (I)] are AUX (O) complex NOUN . (O) 
For ADP (O) example NOUN , (O) it PRON (O) is AUX (O) common ADJ (O) for ADP (O) [statistical ADJ (B) parametric NOUN (I) TTS PROPN (I)] to PART (O) have AUX (O) a NOUN (O) text NOUN (O) frontend NOUN (O) extracting NOUN (O) various ADJ (O) [linguistic NOUN (B) features VERB (I)] , a DET (O) duration NOUN (O) model NOUN , (O) an DET (O) [acoustic ADJ (B) feature NOUN (I) prediction NOUN (I) model NOUN (I)] and CCONJ (O) a NOUN (O) complex NOUN (O) signal NOUN - processing NOUN - based VERB (O) [vocoder NOUN (B)] . 
These DET (O) components NOUN (O) are AUX (O) based VERB (O) on ADP (O) extensive ADJ (O) domain NOUN (O) expertise NOUN (O) and CCONJ (O) are AUX (O) laborious ADJ (O) to ADP (O) design NOUN . (O) 
They PRON (O) are AUX (O) also ADV (O) trained VERB (O) independently ADV , (O) so CCONJ (O) errors NOUN (O) from ADP (O) each DET (O) component NOUN (O) may VERB (O) compound NOUN . (O) 
The DET (O) complexity NOUN (O) of ADP (O) [modern NOUN (B) TTS PROPN (I)] designs NOUN (O) thus ADV (O) leads VERB (O) to ADP (O) substantial NOUN (O) engineering NOUN (O) efforts NOUN (O) when ADV (O) building NOUN (O) a NOUN (O) new ADJ (O) system NOUN . (O) 
There PRON (O) are AUX (O) thus ADV (O) many ADJ (O) advantages NOUN (O) of ADP (O) an DET (O) integrated VERB (O) [end NOUN - to ADP - end NOUN (B) TTS PROPN (I) system NOUN (I)] that DET (O) can VERB (O) be AUX (O) trained VERB (O) on ADP (O) < text NOUN , (O) [audio NOUN (B)] > pairs NOUN (O) with ADP (O) minimal ADJ (O) human NOUN (O) annotation NOUN . (O) 
First ADV , (O) such ADJ (O) a NOUN (O) system NOUN (O) alleviates VERB (O) the DET (O) need NOUN (O) for ADP (O) [laborious ADJ (B) feature NOUN (I) engineering NOUN (I)] , which DET (O) may VERB (O) involve VERB (O) heuristics NOUN (O) and CCONJ (O) brittle PROPN (O) design NOUN (O) choices NOUN . (O) 
Second PROPN , (O) it PRON (O) more ADJ (O) easily ADV (O) allows VERB (O) for ADP (O) rich ADJ (O) conditioning NOUN (O) on ADP (O) various ADJ (O) attributes VERB , (O) such ADJ (O) as SCONJ (O) speaker NOUN (O) or CCONJ (O) language NOUN , (O) or CCONJ (O) high-[level NOUN (B) features VERB (I)] like SCONJ (O) sentiment NOUN . (O) 
This DET (O) is AUX (O) because SCONJ (O) conditioning NOUN (O) can VERB (O) occur VERB (O) at ADP (O) the DET (O) very ADV (O) beginning NOUN (O) of ADP (O) the DET (O) model NOUN (O) rather ADV (O) than SCONJ (O) only ADV (O) on ADP (O) certain ADJ (O) components NOUN . (O) 
Similarly ADV , (O) adaptation NOUN (O) to ADP (O) [new PROPN (B) data NOUN (I)] might VERB (O) also ADV (O) be AUX (O) easier ADJ . (O) 
Finally ADV , (O) a NOUN (O) single ADJ (O) model NOUN (O) is AUX (O) likely ADV (O) to ADP (O) be AUX (O) more ADJ (O) robust ADJ (O) than SCONJ (O) a NOUN (O) multi ADJ - stage NOUN (O) model NOUN (O) where ADV (O) each DET (O) component NOUN ’s PUNCT (O) errors NOUN (O) can VERB (O) compound NOUN . (O) 
These DET (O) advantages NOUN (O) imply VERB (O) that SCONJ (O) an DET (O) [end NOUN - to ADP - end NOUN (B) model NOUN (I)] could VERB (O) allow VERB (O) us PROPN (O) to ADP (O) train NOUN (O) on ADP (O) huge ADJ (O) amounts VERB (O) of ADP (O) rich ADJ , (O) expressive NOUN (O) yet CCONJ (O) often ADV (O) [noisy NOUN (B) data NOUN (I)] found VERB (O) in ADP (O) the DET (O) real NOUN (O) world NOUN . (O) 
[TTS PROPN (B)] is AUX (O) a NOUN (O) large ADJ - scale NOUN (O) inverse NOUN (O) problem NOUN (O) : a DET (O) highly ADV (O) compressed VERB (O) source NOUN (O) (text NOUN) (O) is AUX (O) “ PUNCT (O) decompressed VERB (O) ” PUNCT (O) into ADP (O) [audio NOUN (B)] . 
Since SCONJ (O) the DET (O) same ADJ (O) text NOUN (O) can VERB (O) correspond NOUN (O) to ADP (O) different ADJ (O) pronunciations NOUN (O) or CCONJ (O) speaking VERB (O) styles NOUN , (O) this DET (O) is AUX (O) a NOUN (O) particularly ADV (O) difficult ADJ (O) learning NOUN (O) task NOUN (O) for ADP (O) an DET (O) [end NOUN - to ADP - end NOUN (B) model NOUN (I)] : it PRON (O) must VERB (O) cope VERB (O) with ADP (O) large ADJ (O) variations NOUN (O) at ADP (O) the DET (O) signal NOUN (O) level NOUN (O) for ADP (O) a NOUN (O) given VERB (O) input NOUN . (O) 
Moreover ADV , (O) unlike ADP (O) [end NOUN - to ADP - end NOUN (B) speech NOUN (I) recognition NOUN (I)] or CCONJ (O) [machine NOUN (B) translation NOUN (I)] , [TTS PROPN (B) outputs NOUN (I)] are AUX (O) continuous ADJ , (O) and CCONJ (O) output NOUN (O) sequences NOUN (O) are AUX (O) usually ADV (O) much ADJ (O) longer ADJ (O) than SCONJ (O) those DET (O) of ADP (O) the DET (O) input NOUN . (O) 
These DET (O) attributes VERB (O) cause NOUN (O) prediction NOUN (O) errors NOUN (O) to ADP (O) accumulate NOUN (O) quickly ADV . (O) 
In ADP (O) this DET (O) paper NOUN , (O) we PRON (O) propose NOUN (O) [Tacotron PROPN (B)] , an DET (O) [end NOUN - to ADP - end NOUN (B) generative PROPN (I) TTS PROPN (I) model NOUN (I)] based VERB (O) on ADP (O) the DET (O) [sequence NOUN - to ADP - sequence NOUN (B)] ([seq2seq NOUN (B)]) with ADP (O) [attention NOUN (B) paradigm NOUN (I)] . 
Our DET (O) model NOUN (O) takes VERB (O) characters NOUN (O) as SCONJ (O) input NOUN (O) and CCONJ (O) outputs NOUN (O) [raw ADJ (B) spectrogram NOUN (I)] , using VERB (O) several ADJ (O) techniques NOUN (O) to ADP (O) improve VERB (O) the DET (O) capability NOUN (O) of ADP (O) a NOUN (O) [vanilla NOUN (B) seq2seq NOUN (I) model NOUN (I)] . 
Given VERB (O) < text NOUN , (O) [audio NOUN (B)] > pairs NOUN , (O) [Tacotron PROPN (B)] can VERB (O) be AUX (O) trained VERB (O) completely ADV (O) from ADP (O) scratch NOUN (O) with ADP (O) random ADJ (O) initialization NOUN . (O) 
It PRON (O) does AUX (O) not PART (O) require VERB (O) [phoneme NOUN - level NOUN (B) alignment NOUN (I)] , so CCONJ (O) it PRON (O) can VERB (O) easily ADV (O) scale NOUN (O) to ADP (O) using VERB (O) large ADJ (O) amounts VERB (O) of ADP (O) [acoustic ADJ (B) data NOUN (I)] with ADP (O) transcripts NOUN . (O) 
With ADP (O) a NOUN (O) simple ADJ (O) [waveform PROPN (B) synthesis NOUN (I)] technique NOUN , (O) [Tacotron PROPN (B)] produces VERB (O) a NOUN (O) 3.82 NUM (O) [mean NOUN (B) opinion NOUN (I) score NOUN (I)] ([MOS PROPN (B)]) on ADP (O) an DET (O) US PROPN (O) English PROPN (O) eval NOUN (O) set VERB , (O) outperforming VERB (O) a NOUN (O) production NOUN (O) [parametric NOUN (B) system NOUN (I)] in ADP (O) terms NOUN (O) of ADP (O) naturalness NOUN (O) 1 NUM . (O)                                        
                                        
                                                                                         
 Figure PROPN (O) : [Model NOUN (B) architecture NOUN (I)] . 
The DET (O) model NOUN (O) takes VERB (O) characters NOUN (O) as SCONJ (O) input NOUN (O) and CCONJ (O) outputs NOUN (O) the DET (O) corresponding VERB (O) [raw ADJ (B) spectrogram NOUN (I)] , which DET (O) is AUX (O) then ADV (O) fed PROPN (O) to ADP (O) the DET (O) [Griffin PROPN - Lim PROPN (B) reconstruction NOUN (I) algorithm PROPN (I)] to PART (O) [synthesize VERB (B) speech NOUN (I)] . 


 RELATED PROPN (O) WORK PROPN (O) 

 [WaveNet PROPN (B)] is AUX (O) a NOUN (O) powerful ADJ (O) generative ADJ (O) model NOUN (O) of ADP (O) [audio NOUN (B)] . 
It PRON (O) works VERB (O) well INTJ (O) for ADP (O) [TTS PROPN (B)] , but CCONJ (O) is AUX (O) slow ADJ (O) due ADJ (O) to ADP (O) its DET (O) sample NOUN - level NOUN (O) autoregressive ADJ (O) nature NOUN . (O) 
It PRON (O) also ADV (O) requires VERB (O) conditioning NOUN (O) on ADP (O) [linguistic NOUN (B) features VERB (I)] from ADP (O) an DET (O) existing VERB (O) [TTS PROPN (B) frontend NOUN (I)] , and CCONJ (O) thus ADV (O) is AUX (O) not PART (O) [end NOUN - to ADP - end NOUN (B)] : it PRON (O) only ADV (O) replaces VERB (O) the DET (O) [vocoder NOUN (B)] and CCONJ (O) [acoustic ADJ (B) model NOUN (I)] . 
Another DET (O) recently ADV - developed VERB (O) [neural NOUN (B) model NOUN (I)] is AUX (O) [DeepVoice PROPN (B)] , which DET (O) replaces VERB (O) every DET (O) component NOUN (O) in ADP (O) a NOUN (O) typical ADJ (O) [TTS PROPN (B) pipeline NOUN (I)] by ADP (O) a NOUN (O) corresponding VERB (O) [neural NOUN (B) network NOUN (I)] . 
However ADV , (O) each DET (O) component NOUN (O) is AUX (O) independently ADV (O) trained VERB , (O) and CCONJ (O) it PRON ’s VERB (O) nontrivial NOUN (O) to ADP (O) change NOUN (O) the DET (O) system NOUN (O) to ADP (O) train NOUN (O) in ADP (O) an DET (O) [end NOUN - to ADP - end NOUN (B) fashion NOUN (I)] . 
To PART (O) our DET (O) knowledge NOUN (O) is AUX (O) the DET (O) earliest ADJ (O) work NOUN (O) touching NOUN (O) [end NOUN - to ADP - end NOUN (B) TTS PROPN (I)] using VERB (O) [seq2seq NOUN (B)] with ADP (O) attention NOUN . (O) 
However ADV , (O) it PRON (O) requires VERB (O) a NOUN (O) pre VERB - trained ADJ (O) [hidden VERB (B) Markov PROPN (I) model NOUN (I) (HMM PROPN) (I) aligner PROPN (I)] to PART (O) help NOUN (O) the DET (O) [seq2seq NOUN (B) model NOUN (I)] learn VERB (O) the DET (O) alignment NOUN . (O) 
It PRON ’s PUNCT (O) hard ADJ (O) to ADP (O) tell VERB (O) how ADV (O) much ADJ (O) alignment NOUN (O) is AUX (O) learned VERB (O) by ADP (O) the DET (O) [seq2seq NOUN (B)] per ADP (O) se X . (O) 
Second PROPN , (O) a NOUN (O) few ADJ (O) tricks NOUN (O) are AUX (O) used VERB (O) to ADP (O) get AUX (O) the DET (O) model NOUN (O) trained VERB , (O) which DET (O) the DET (O) authors NOUN (O) note NOUN (O) hurts VERB (O) [prosody NOUN (B)] . 
Third ADJ , (O) it PRON (O) predicts VERB (O) [vocoder NOUN (B) parameters NOUN (I)] hence ADV (O) needs VERB (O) a NOUN (O) [vocoder NOUN (B)] . 
Furthermore ADV , (O) the DET (O) model NOUN (O) is AUX (O) trained VERB (O) on ADP (O) [phoneme NOUN (B)] inputs VERB (O) and CCONJ (O) the DET (O) experimental NOUN (O) results VERB (O) seem VERB (O) to ADP (O) be AUX (O) somewhat ADV (O) limited ADJ . (O) 
Char2Wav PROPN (O) is AUX (O) an DET (O) independently ADV - developed VERB (O) [end NOUN - to ADP - end NOUN (B) model NOUN (I)] that DET (O) can VERB (O) be AUX (O) trained VERB (O) on ADP (O) characters NOUN . (O) 
However ADV , (O) Char2Wav PROPN (O) still ADV (O) predicts VERB (O) [vocoder NOUN (B) parameters NOUN (I)] before ADP (O) using VERB (O) a NOUN (O) [SampleRNN PROPN (B) neural NOUN (I) vocoder NOUN (I)] , whereas SCONJ (O) [Tacotron PROPN (B)] directly ADV (O) predicts VERB (O) [raw ADJ (B) spectrogram NOUN (I)] . 
Also ADV , (O) their DET (O) [seq2seq NOUN (B)] and CCONJ (O) [SampleRNN PROPN (B) models NOUN (I)] need VERB (O) to ADP (O) be AUX (O) separately ADV (O) pre VERB - trained VERB , (O) but CCONJ (O) our DET (O) model NOUN (O) can VERB (O) be AUX (O) trained VERB (O) from ADP (O) scratch NOUN . (O) 
Finally ADV , (O) we PRON (O) made VERB (O) several ADJ (O) key NOUN (O) modifications NOUN (O) to ADP (O) the DET (O) [vanilla NOUN (B) seq2seq NOUN (I)] paradigm PROPN . (O) 
As SCONJ (O) shown VERB (O) later ADV , (O) a NOUN (O) [vanilla NOUN (B) seq2seq NOUN (I) model NOUN (I)] does AUX (O) not PART (O) work NOUN (O) well INTJ (O) for ADP (O) [character NOUN - level NOUN (B) inputs NOUN (I)] . 

 [MODEL NOUN (B) ARCHITECTURE PROPN (I)] 

 The DET (O) backbone NOUN (O) of ADP (O) [Tacotron PROPN (B)] is AUX (O) a NOUN (O) [seq2seq NOUN (B) model NOUN (I)] with ADP (O) attention NOUN . (O) 
Figure NOUN (O) 1 NUM (O) depicts VERB (O) the DET (O) model NOUN , (O) which DET (O) includes VERB (O) an DET (O) [encoder NOUN (B)] , an DET (O) [attention NOUN - based VERB (B) decoder NOUN (I)] , and CCONJ (O) a NOUN (O) [post NOUN - processing ADJ (B) net NOUN (I)] . 
At ADP (O) a NOUN (O) high ADJ - level NOUN , (O) our DET (O) model NOUN (O) takes VERB (O) characters NOUN (O) as SCONJ (O) input NOUN (O) and CCONJ (O) produces VERB (O) [spectrogram NOUN (B) frames NOUN (I)] , which DET (O) are AUX (O) then ADV (O) converted VERB (O) to ADP (O) [waveforms NOUN (B)] . 
We PRON (O) describe NOUN (O) these DET (O) components NOUN (O) below ADV . (O) 

 Figure NOUN (O) : The DET (O) [CBHG PROPN (B) module NOUN (I)] ([1D PROPN (B) convolution NOUN (I) bank NOUN (I)] + highway NOUN (O) network NOUN (O) + [bidirectional ADJ (B) GRU PROPN (I)]) adapted VERB . (O) 

 [CBHG PROPN (B) MODULE PROPN (I)] 

 We PRON (O) first ADJ (O) describe VERB (O) a NOUN (O) building NOUN (O) block NOUN (O) dubbed VERB (O) [CBHG PROPN (B)] , illustrated VERB (O) in ADP (O) Figure NOUN (O) 2 NUM . (O) 
[CBHG PROPN (B)] consists VERB (O) of ADP (O) a NOUN (O) bank NOUN (O) of ADP (O) [1D PROPN (B) convolutional ADJ (I) filters NOUN (I)] , followed VERB (O) by ADP (O) highway NOUN (O) networks NOUN (O) and CCONJ (O) a NOUN (O) [bidirectional NOUN (B) gated VERB (I) recurrent ADJ (I) unit NOUN (I)] ([GRU PROPN (B)]) [recurrent NOUN (B) neural NOUN (I) network NOUN (I)] ([RNN PROPN (B)]) . 
[CBHG PROPN (B)] is AUX (O) a NOUN (O) powerful ADJ (O) module NOUN (O) for ADP (O) extracting NOUN (O) representations NOUN (O) from ADP (O) sequences NOUN . (O) 
The DET (O) input NOUN (O) sequence NOUN (O) is AUX (O) first ADJ (O) convolved VERB (O) with ADP (O) K NOUN (O) sets VERB (O) of ADP (O) [1D PROPN (B) convolutional ADJ (I) filters NOUN (I)] , where ADV (O) the DET (O) k PROPN - th NOUN (O) set NOUN (O) contains VERB (O) C NOUN (O) k NOUN (O) filters NOUN (O) of ADP (O) width NOUN (O) k. PROPN (O) 
These DET (O) filters NOUN (O) explicitly ADV (O) model NOUN (O) local ADJ (O) and CCONJ (O) contextual NOUN (O) information NOUN (O) (akin PROPN (O) to ADP (O) modeling NOUN (O) unigrams PROPN , (O) bigrams NOUN , (O) up NOUN (O) to ADP (O) [K PROPN - grams NOUN (B)]) . 
The DET (O) convolution NOUN (O) outputs NOUN (O) are AUX (O) stacked VERB (O) together ADV (O) and CCONJ (O) further NOUN (O) max NOUN (O) pooled VERB (O) along ADP (O) time NOUN (O) to ADP (O) increase NOUN (O) local ADJ (O) invariances NOUN . (O) 
Note NOUN (O) that SCONJ (O) we PRON (O) use NOUN (O) a NOUN (O) stride NOUN (O) of ADP (O) 1 NUM (O) to ADP (O) preserve NOUN (O) the DET (O) original ADJ (O) time NOUN (O) resolution NOUN . (O) 
We PRON (O) further NOUN (O) pass VERB (O) the DET (O) processed VERB (O) sequence NOUN (O) to ADP (O) a NOUN (O) few ADJ (O) fixed VERB - width NOUN (O) [1D PROPN (B) convolutions NOUN (I)] , whose DET (O) outputs VERB (O) are AUX (O) added VERB (O) with ADP (O) the DET (O) original ADJ (O) input NOUN (O) sequence NOUN (O) via ADP (O) [residual ADJ (B) connections NOUN (I)] . 
[Batch PROPN (B) normalization NOUN (I)] is AUX (O) used VERB (O) for ADP (O) all DET (O) [convolutional NOUN (B) layers NOUN (I)] . 
The DET (O) convolution NOUN (O) outputs NOUN (O) are AUX (O) fed PROPN (O) into ADP (O) a NOUN (O) [multi ADJ - layer ADJ (B) highway NOUN (I) network NOUN (I)] to PART (O) extract NOUN (O) high-[level NOUN (B) features VERB (I)] . 
Finally ADV , (O) we PRON (O) stack NOUN (O) a NOUN (O) [bidirectional ADJ (B) GRU PROPN (I) RNN PROPN (I)] on ADP (O) top NOUN (O) to ADP (O) extract NOUN (O) [sequential NOUN (B) features VERB (I)] from ADP (O) both DET (O) forward NOUN (O) and CCONJ (O) backward NOUN (O) context NOUN . (O) 
[CBHG PROPN (B)] is AUX (O) inspired VERB (O) from ADP (O) work NOUN (O) in ADP (O) [machine NOUN (B) translation NOUN (I)] , where ADV (O) the DET (O) main ADJ (O) differences NOUN (O) include VERB (O) using VERB (O) non ADJ - causal ADJ (O) convolutions NOUN , (O) [batch NOUN (B) normalization NOUN (I)] , [residual ADJ (B) connections NOUN (I)] , and CCONJ (O) stride=1 X (O) max NOUN (O) pooling VERB . (O) 
We PRON (O) found VERB (O) that SCONJ (O) these DET (O) modifications NOUN (O) improved VERB (O) generalization NOUN . (O) 

 [ENCODER PROPN (B)] 

 The DET (O) goal NOUN (O) of ADP (O) the DET (O) [encoder NOUN (B)] is AUX (O) to ADP (O) extract NOUN (O) robust ADJ (O) sequential ADJ (O) representations NOUN (O) of ADP (O) text NOUN . (O) 
The DET (O) input NOUN (O) to ADP (O) the DET (O) [encoder NOUN (B)] is AUX (O) a NOUN (O) character NOUN (O) sequence NOUN , (O) where ADV (O) each DET (O) character NOUN (O) is AUX (O) represented VERB (O) as SCONJ (O) a NOUN (O) [one NUM - hot ADJ (B) vector NOUN (I)] and CCONJ (O) embedded VERB (O) into ADP (O) a NOUN (O) [continuous ADJ (B) vector NOUN (I)] . 
We PRON (O) then ADV (O) apply VERB (O) a NOUN (O) set NOUN (O) of ADP (O) non ADJ - linear ADJ (O) transformations NOUN , (O) collectively ADV (O) called VERB (O) a NOUN (O) “ PUNCT (O) [pre ADJ - net ADJ (B)] ” PUNCT , (O) to ADP (O) each DET (O) embedding NOUN . (O) 
We PRON (O) use NOUN (O) a NOUN (O) bottleneck NOUN (O) layer NOUN (O) with ADP (O) [dropout NOUN (B)] as SCONJ (O) the DET (O) [pre ADJ - net ADJ (B)] in ADP (O) this DET (O) work NOUN , (O) which DET (O) helps VERB (O) convergence NOUN (O) and CCONJ (O) improves VERB (O) generalization NOUN . (O) 
A NOUN (O) [CBHG PROPN (B) module NOUN (I)] transforms VERB (O) the DET (O) [pre ADJ - net ADJ (B) outputs NOUN (I)] into ADP (O) the DET (O) final ADJ (O) [encoder NOUN (B) representation NOUN (I)] used VERB (O) by ADP (O) the DET (O) attention NOUN (O) module PROPN . (O) 
We PRON (O) found VERB (O) that SCONJ (O) this DET (O) [CBHG PROPN - based VERB (B) encoder NOUN (I)] not PART (O) only ADV (O) reduces VERB (O) overfitting NOUN , (O) but CCONJ (O) also ADV (O) makes VERB (O) fewer ADJ (O) mispronunciations NOUN (O) than SCONJ (O) a NOUN (O) standard NOUN (O) [multi ADJ - layer ADJ (B) RNN PROPN (I) encoder NOUN (I)] (see VERB (O) our DET (O) linked VERB (O) page NOUN (O) of ADP (O) [audio NOUN (B) samples NOUN (I)]) . 

 Table NOUN (O) : [Hyper PROPN - parameters NOUN (B)] and CCONJ (O) [network NOUN (B) architectures NOUN (I)] . “ PUNCT (O) conv NOUN - k PROPN - c-[ReLU NOUN (B)] ” PUNCT (O) denotes VERB (O) [1D PROPN (B) convolution NOUN (I)] 
with ADP (O) width NOUN (O) k NOUN (O) and CCONJ (O) c NOUN (O) output NOUN (O) channels NOUN (O) with ADP (O) [ReLU NOUN (B) activation NOUN (I)] . FC PROPN (O) stands VERB (O) for ADP (O) fully ADV - connected VERB . (O) 

 [DECODER PROPN (B)] 

 We PRON (O) use NOUN (O) a NOUN (O) content NOUN - based VERB (O) [tanh NOUN (B) attention NOUN (I) decoder NOUN (I)] , where ADV (O) a NOUN (O) stateful ADJ (O) [recurrent NOUN (B) layer NOUN (I)] produces VERB (O) the DET (O) attention NOUN (O) query NOUN (O) at ADP (O) each DET (O) decoder NOUN (O) time NOUN (O) step NOUN . (O) 
We PRON (O) concatenate NOUN (O) the DET (O) [context NOUN (B) vector NOUN (I)] and CCONJ (O) the DET (O) [attention NOUN (B) RNN PROPN (I) cell NOUN (I) output NOUN (I)] to PART (O) form NOUN (O) the DET (O) input NOUN (O) to ADP (O) the DET (O) [decoder NOUN (B) RNNs PROPN (I)] . 
We PRON (O) use NOUN (O) a NOUN (O) stack NOUN (O) of ADP (O) [GRUs NOUN (B)] with ADP (O) vertical ADJ (O) [residual ADJ (B) connections NOUN (I)] for ADP (O) the DET (O) [decoder NOUN (B)] . 
We PRON (O) found VERB (O) the DET (O) [residual ADJ (B) connections NOUN (I)] speed NOUN (O) up NOUN (O) convergence NOUN . (O) 
The DET (O) [decoder NOUN (B) target NOUN (I)] is AUX (O) an DET (O) important ADJ (O) design NOUN (O) choice NOUN . (O) 
While SCONJ (O) we PRON (O) could VERB (O) directly ADV (O) predict VERB (O) [raw ADJ (B) spectrogram NOUN (I)] , it PRON ’s VERB (O) a NOUN (O) highly ADV (O) redundant ADJ (O) representation NOUN (O) for ADP (O) the DET (O) purpose NOUN (O) of ADP (O) learning NOUN (O) alignment NOUN (O) between ADP (O) [speech NOUN (B) signal NOUN (I)] and CCONJ (O) text NOUN (O) (which DET (O) is AUX (O) really ADV (O) the DET (O) motivation NOUN (O) of ADP (O) using VERB (O) [seq2seq NOUN (B)] for ADP (O) this DET (O) task NOUN) . (O) 
Because SCONJ (O) of ADP (O) this DET (O) redundancy NOUN , (O) we PRON (O) use NOUN (O) a NOUN (O) different ADJ (O) target NOUN (O) for ADP (O) [seq2seq NOUN (B) decoding VERB (I)] and CCONJ (O) [waveform PROPN (B) synthesis NOUN (I)] . 
The DET (O) [seq2seq NOUN (B) target NOUN (I)] can VERB (O) be AUX (O) highly ADV (O) compressed VERB (O) as SCONJ (O) long ADJ (O) as SCONJ (O) it PRON (O) provides VERB (O) sufficient ADJ (O) intelligibility NOUN (O) and CCONJ (O) [prosody NOUN (B) information NOUN (I)] for ADP (O) an DET (O) inversion NOUN (O) process NOUN , (O) which DET (O) could VERB (O) be AUX (O) fixed VERB (O) or CCONJ (O) trained VERB . (O) 
We PRON (O) use NOUN (O) 80-band NUM (O) [mel ADJ - scale NOUN (B) spectrogram NOUN (I)] as SCONJ (O) the DET (O) target VERB , (O) though SCONJ (O) fewer ADJ (O) bands NOUN (O) or CCONJ (O) more ADJ (O) concise NOUN (O) targets VERB (O) such ADJ (O) as SCONJ (O) cepstrum NOUN (O) could VERB (O) be AUX (O) used VERB . (O) 
We PRON (O) use NOUN (O) a NOUN (O) [post NOUN - processing ADJ (B) network NOUN (I)] (discussed VERB (O) below ADV) (O) to ADP (O) convert NOUN (O) from ADP (O) the DET (O) [seq2seq NOUN (B) target NOUN (I)] to PART (O) [waveform PROPN (B)] . 
We PRON (O) use NOUN (O) a NOUN (O) simple ADJ (O) fully ADV - connected VERB (O) [output NOUN (B) layer NOUN (I)] to PART (O) predict VERB (O) the DET (O) [decoder NOUN (B) targets NOUN (I)] . 
An PROPN (O) important ADJ (O) trick NOUN (O) we PRON (O) discovered VERB (O) was AUX (O) predicting VERB (O) multiple NOUN , (O) non ADJ - overlapping ADJ (O) output NOUN (O) frames VERB (O) at ADP (O) each DET (O) [decoder NOUN (B) step NOUN (I)] . 
Predicting PROPN (O) r NOUN (O) frames NOUN (O) at ADP (O) once ADV (O) divides VERB (O) the DET (O) total NOUN (O) number NOUN (O) of ADP (O) [decoder NOUN (B) steps NOUN (I)] by ADP (O) r NOUN , (O) which DET (O) reduces VERB (O) model NOUN (O) size NOUN , (O) training NOUN (O) time NOUN , (O) and CCONJ (O) inference NOUN (O) time NOUN . (O) 
More ADJ (O) importantly ADV , (O) we PRON (O) found VERB (O) this DET (O) trick NOUN (O) to ADP (O) substantially ADV (O) increase NOUN (O) convergence NOUN (O) speed NOUN , (O) as SCONJ (O) measured VERB (O) by ADP (O) a NOUN (O) much ADJ (O) faster ADV (O) (and CCONJ (O) more ADJ (O) stable ADJ) (O) alignment NOUN (O) learned VERB (O) from ADP (O) attention NOUN . (O) 
This DET (O) is AUX (O) likely ADV (O) because SCONJ (O) neighboring NOUN (O) [speech NOUN (B) frames NOUN (I)] are AUX (O) correlated VERB (O) and CCONJ (O) each DET (O) character NOUN (O) usually ADV (O) corresponds VERB (O) to ADP (O) multiple NOUN (O) frames NOUN . (O) 
Emitting PROPN (O) one NUM (O) frame NOUN (O) at ADP (O) a NOUN (O) time NOUN (O) forces NOUN (O) the DET (O) model NOUN (O) to ADP (O) attend VERB (O) to ADP (O) the DET (O) same ADJ (O) input NOUN (O) token NOUN (O) for ADP (O) multiple NOUN (O) timesteps PROPN (O) ; emitting VERB (O) multiple NOUN (O) frames VERB (O) allows VERB (O) the DET (O) attention NOUN (O) to ADP (O) move NOUN (O) forward NOUN (O) early ADV (O) in ADP (O) training NOUN . (O) 
A PROPN (O) similar ADJ (O) trick NOUN (O) is AUX (O) also ADV (O) used VERB (O) but CCONJ (O) mainly ADV (O) to ADP (O) speed NOUN (O) up NOUN (O) inference NOUN . (O) 
The DET (O) first ADJ (O) [decoder NOUN (B) step NOUN (I)] is AUX (O) conditioned VERB (O) on ADP (O) an DET (O) all DET - zero NUM (O) frame NOUN , (O) which DET (O) represents VERB (O) a NOUN (O) < GO PROPN (O) > frame NOUN . (O) 
In ADP (O) inference NOUN , (O) at ADP (O) [decoder NOUN (B) step NOUN (I)] t NOUN , (O) the DET (O) last ADJ (O) frame NOUN (O) of ADP (O) the DET (O) r NOUN (O) predictions NOUN (O) is AUX (O) fed PROPN (O) as SCONJ (O) input NOUN (O) to ADP (O) the DET (O) [decoder NOUN (B)] at ADP (O) step NOUN (O) t NOUN (O) + 1 NUM . (O) 
Note NOUN (O) that SCONJ (O) feeding NOUN (O) the DET (O) last ADJ (O) prediction NOUN (O) is AUX (O) an DET (O) ad X - hoc X (O) choice NOUN (O) here ADV (O) – PUNCT (O) we PRON (O) could VERB (O) use NOUN (O) all DET (O) r NOUN (O) predictions NOUN . (O) 
During ADP (O) training NOUN , (O) we PRON (O) always ADV (O) feed NOUN (O) every DET (O) r NOUN - th NOUN (O) ground NOUN (O) truth NOUN (O) frame NOUN (O) to ADP (O) the DET (O) [decoder NOUN (B)] . 
The DET (O) input NOUN (O) frame NOUN (O) is AUX (O) passed VERB (O) to ADP (O) a NOUN (O) [pre ADJ - net ADJ (B)] as SCONJ (O) is AUX (O) done VERB (O) in ADP (O) the DET (O) [encoder NOUN (B)] . 
Since SCONJ (O) we PRON (O) do AUX (O) not PART (O) use NOUN (O) techniques NOUN (O) such ADJ (O) as SCONJ (O) scheduled VERB (O) sampling NOUN (O) (we PRON (O) found VERB (O) it PRON (O) to ADP (O) hurt VERB (O) [audio NOUN (B) quality NOUN (I)]) , the DET (O) [dropout NOUN (B)] in ADP (O) the DET (O) [pre ADJ - net ADJ (B)] is AUX (O) critical ADJ (O) for ADP (O) the DET (O) model NOUN (O) to ADP (O) generalize VERB , (O) as SCONJ (O) it PRON (O) provides VERB (O) a NOUN (O) noise NOUN (O) source NOUN (O) to ADP (O) resolve NOUN (O) the DET (O) multiple NOUN (O) modalities NOUN (O) in ADP (O) the DET (O) output NOUN (O) distribution NOUN . (O) 

 [POST PROPN - PROCESSING PROPN (B) NET PROPN (I)] AND CCONJ (O) [WAVEFORM PROPN (B) SYNTHESIS PROPN (I)] 

 As SCONJ (O) mentioned VERB (O) above ADV , (O) the DET (O) [post NOUN - processing ADJ (B) net NOUN (I)] ’s PART task NOUN (O) is AUX (O) to ADP (O) convert NOUN (O) the DET (O) [seq2seq NOUN (B) target NOUN (I)] to PART (O) a NOUN (O) target NOUN (O) that SCONJ (O) can VERB (O) be AUX (O) synthesized VERB (O) into ADP (O) [waveforms NOUN (B)] . 
Since SCONJ (O) we PRON (O) use NOUN (O) [Griffin PROPN - Lim PROPN (B)] as SCONJ (O) the DET (O) synthesizer PROPN , (O) the DET (O) [post NOUN - processing ADJ (B) net NOUN (I) learns VERB (I)] to PART (O) predict VERB (O) [spectral NOUN (B)] magnitude NOUN (O) sampled VERB (O) on ADP (O) a NOUN (O) linear NOUN - frequency NOUN (O) scale NOUN . (O) 
Another DET (O) motivation NOUN (O) of ADP (O) the DET (O) [post NOUN - processing ADJ (B) net NOUN (I)] is AUX (O) that SCONJ (O) it PRON (O) can VERB (O) see VERB (O) the DET (O) full ADJ (O) decoded VERB (O) sequence NOUN . (O) 
In ADP (O) contrast NOUN (O) to ADP (O) [seq2seq NOUN (B)] , which DET (O) always ADV (O) runs VERB (O) from ADP (O) left VERB (O) to ADP (O) right INTJ , (O) it PRON (O) has AUX (O) both DET (O) forward NOUN (O) and CCONJ (O) backward NOUN (O) information NOUN (O) to ADP (O) correct ADJ (O) the DET (O) prediction NOUN (O) error NOUN (O) for ADP (O) each DET (O) individual NOUN (O) frame NOUN . (O) 
In ADP (O) this DET (O) work NOUN , (O) we PRON (O) use NOUN (O) a NOUN (O) [CBHG PROPN (B) module NOUN (I)] for ADP (O) the DET (O) [post NOUN - processing ADJ (B) net NOUN (I)] , though SCONJ (O) a NOUN (O) simpler ADJ (O) architecture NOUN (O) likely ADV (O) works VERB (O) as SCONJ (O) well INTJ . (O) 
The DET (O) concept NOUN (O) of ADP (O) a NOUN (O) [post NOUN - processing ADJ (B) network NOUN (I)] is AUX (O) highly ADV (O) general ADJ . (O) 
It PRON (O) could VERB (O) be AUX (O) used VERB (O) to ADP (O) predict VERB (O) alternative NOUN (O) targets VERB (O) such ADJ (O) as SCONJ (O) [vocoder NOUN (B) parameters NOUN (I)] , or CCONJ (O) as SCONJ (O) a NOUN (O) [WaveNet PROPN (B)]-like PROPN [neural NOUN (B) vocoder NOUN (I)] that DET (O) synthesizes NOUN (O) [waveform PROPN (B) samples NOUN (I)] directly ADV . (O) 
We PRON (O) use NOUN (O) the DET (O) [Griffin PROPN - Lim PROPN (B) algorithm NOUN (I)] to PART (O) synthesize VERB (O) [waveform PROPN (B)] from ADP (O) the DET (O) predicted VERB (O) [spectrogram NOUN (B)] . 
We PRON (O) found VERB (O) that SCONJ (O) raising NOUN (O) the DET (O) predicted VERB (O) magnitudes NOUN (O) by ADP (O) a NOUN (O) power NOUN (O) of ADP (O) 1.2 NUM (O) before ADP (O) feeding NOUN (O) to ADP (O) [Griffin PROPN - Lim PROPN (B) reduces VERB (I) artifacts NOUN (I)] , likely ADJ (O) due ADJ (O) to ADP (O) its DET (O) harmonic PROPN (O) enhancement NOUN (O) effect NOUN . (O) 
We PRON (O) observed VERB (O) that SCONJ (O) [Griffin PROPN - Lim PROPN (B) converges NOUN (I)] after ADP (O) 50 NUM (O) iterations NOUN (O) (in ADP (O) fact NOUN , (O) about ADP (O) 30 NUM (O) iterations NOUN (O) seems VERB (O) to ADP (O) be AUX (O) enough ADJ) , (O) which DET (O) is AUX (O) reasonably ADV (O) fast ADV . (O) 
We PRON (O) implemented VERB (O) [Griffin PROPN - Lim PROPN (B)] in ADP (O) TensorFlow PROPN (O) hence ADV (O) it PRON ’s VERB (O) also ADV (O) part NOUN (O) of ADP (O) the DET (O) model NOUN . (O) 
While SCONJ (O) [Griffin PROPN - Lim PROPN (B)] is AUX (O) differentiable ADJ (O) (it PRON (O) does AUX (O) not PART (O) have AUX (O) trainable NOUN (O) weights NOUN) , (O) we PRON (O) do AUX (O) not PART (O) impose VERB (O) any DET (O) loss NOUN (O) on ADP (O) it PRON (O) in ADP (O) this DET (O) work NOUN . (O) 
We PRON (O) emphasize VERB (O) that SCONJ (O) our DET (O) choice NOUN (O) of ADP (O) [Griffin PROPN - Lim PROPN (B)] is AUX (O) for ADP (O) simplicity NOUN (O) ; while SCONJ (O) it PRON (O) already ADV (O) yields NOUN (O) strong ADJ (O) results VERB , (O) developing VERB (O) a NOUN (O) fast VERB (O) and CCONJ (O) [high ADJ - quality NOUN (B) trainable NOUN (I) spectrogram PROPN (I)] to PART (O) [waveform PROPN (B)] inverter NOUN (O) is AUX (O) ongoing ADJ (O) work NOUN . (O) 

 MODEL NOUN (O) DETAILS PROPN (O) 

 Table NOUN (O) 1 NUM (O) lists VERB (O) the DET (O) [hyper NOUN - parameters NOUN (B)] and CCONJ (O) [network NOUN (B) architectures NOUN (I)] . 
We PRON (O) use NOUN (O) [log NOUN (B) magnitude NOUN (I) spectrogram PROPN (I)] with ADP (O) Hann PROPN (O) windowing NOUN , (O) 50 NUM (O) ms NOUN (O) frame NOUN (O) length NOUN , (O) 12.5 NUM (O) ms NOUN (O) frame NOUN (O) shift NOUN , (O) and CCONJ (O) 2048-point NUM (O) [Fourier PROPN (B) transform NOUN (I)] . 
We PRON (O) also ADV (O) found VERB (O) pre ADJ - emphasis ADJ (O) (0.97 NUM) (O) to ADP (O) be AUX (O) helpful ADJ . (O) 
We PRON (O) use NOUN (O) 24 NUM (O) kHz PROPN (O) sampling NOUN (O) rate NOUN (O) for ADP (O) all DET (O) experiments NOUN . (O) 
We PRON (O) use NOUN (O) r NOUN (O) = 2 NUM (O) ([output NOUN (B) layer NOUN (I)] reduction NOUN (O) factor NOUN) (O) for ADP (O) the DET (O) [MOS PROPN (B) results NOUN (I)] in ADP (O) this DET (O) paper NOUN , (O) though SCONJ (O) larger ADJ (O) r NOUN (O) values NOUN (O) (e.g. ADV (O) r NOUN (O) = 5 NUM) (O) also ADV (O) work NOUN (O) well INTJ . (O) 
We PRON (O) use NOUN (O) the DET (O) [Adam PROPN (B) optimizer NOUN (I)] with ADP (O) [learning NOUN (B) rate NOUN (I)] decay NOUN , (O) which DET (O) starts VERB (O) from ADP (O) 0.001 NUM (O) and CCONJ (O) is AUX (O) reduced VERB (O) to ADP (O) 0.0005 NUM , (O) 0.0003 NUM , (O) and CCONJ (O) 0.0001 PUNCT (O) after ADP (O) 500000 NUM , (O) 1 NUM (O) M NOUN (O) and CCONJ (O) 2 NUM (O) M NOUN (O) global ADJ (O) steps NOUN , (O) respectively ADV . (O) 
We PRON (O) use NOUN (O) a NOUN (O) simple ADJ (O) ` 1 NUM (O) loss NOUN (O) for ADP (O) both DET (O) [seq2seq NOUN (B) decoder NOUN (I)] ([mel ADJ - scale NOUN (B) spectrogram NOUN (I)]) and CCONJ (O) [post NOUN - processing ADJ (B) net NOUN (I)] ([linear ADJ - scale NOUN (B) spectrogram NOUN (I)]) . 
The DET (O) two NUM (O) losses NOUN (O) have AUX (O) equal ADJ (O) weights NOUN . (O) 
We PRON (O) train NOUN (O) using VERB (O) a NOUN (O) [batch NOUN (B) size NOUN (I)] of ADP (O) 32 NUM , (O) where ADV (O) all DET (O) sequences NOUN (O) are AUX (O) padded ADJ (O) to ADP (O) a NOUN (O) max NOUN (O) length NOUN . (O) 
It PRON ’s PUNCT (O) a NOUN (O) common ADJ (O) practice NOUN (O) to ADP (O) train NOUN (O) sequence NOUN (O) models NOUN (O) with ADP (O) a NOUN (O) loss NOUN (O) mask NOUN , (O) which DET (O) masks NOUN (O) loss NOUN (O) on ADP (O) zero NUM - padded NOUN (O) frames NOUN . (O) 
However ADV , (O) we PRON (O) found VERB (O) that SCONJ (O) models NOUN (O) trained VERB (O) this DET (O) way NOUN (O) do AUX (O) n’t PROPN (O) know VERB (O) when ADV (O) to ADP (O) stop VERB (O) emitting NOUN (O) outputs NOUN , (O) causing VERB (O) repeated VERB (O) sounds VERB (O) towards ADP (O) the DET (O) end NOUN . (O) 
One NUM (O) simple ADJ (O) trick NOUN (O) to ADP (O) get AUX (O) around ADP (O) this DET (O) problem NOUN (O) is AUX (O) to ADP (O) also ADV (O) reconstruct NOUN (O) the DET (O) zero NUM - padded NOUN (O) frames NOUN . (O) 

 EXPERIMENTS NOUN (O) 

 We PRON (O) train NOUN (O) [Tacotron PROPN (B)] on ADP (O) an DET (O) internal ADJ (O) North NOUN (O) American PROPN (O) [English PROPN (B) dataset NOUN (I)] , which DET (O) contains VERB (O) about ADP (O) 24.6 NUM (O) hours NOUN (O) of ADP (O) [speech NOUN (B) data NOUN (I)] spoken ADJ (O) by ADP (O) a NOUN (O) professional ADJ (O) [female NOUN (B) speaker NOUN (I)] . 
The DET (O) phrases VERB (O) are AUX (O) text NOUN (O) normalized ADJ , (O) e.g. ADV (O) “ PUNCT (O) 16 NUM (O) ” PUNCT (O) is AUX (O) converted VERB (O) to ADP (O) “ PUNCT (O) sixteen NUM (O) ” PUNCT . (O) 

 Figure NOUN (O) : [Attention NOUN (B) alignments VERB (I)] on ADP (O) a NOUN (O) test NOUN (O) phrase NOUN . (O) 
The DET (O) [decoder NOUN (B) length NOUN (I)] in ADP (O) [Tacotron PROPN (B)] is AUX (O) shorter ADJ (O) due ADJ (O) to ADP (O) the DET (O) use NOUN (O) of ADP (O) the DET (O) output NOUN (O) reduction NOUN (O) factor NOUN (O) r=5 NOUN . (O) 


 ABLATION PROPN (O) ANALYSIS PROPN (O) 

 We PRON (O) conduct NOUN (O) a NOUN (O) few ADJ (O) ablation NOUN (O) studies NOUN (O) to ADP (O) understand VERB (O) the DET (O) key NOUN (O) components NOUN (O) in ADP (O) our DET (O) model NOUN . (O) 
As SCONJ (O) is AUX (O) common ADJ (O) for ADP (O) generative PROPN (O) models NOUN , (O) it PRON ’s VERB (O) hard ADJ (O) to ADP (O) compare VERB (O) models NOUN (O) based VERB (O) on ADP (O) objective NOUN (O) metrics NOUN , (O) which DET (O) often ADV (O) do AUX (O) not PART (O) correlate NOUN (O) well INTJ (O) with ADP (O) perception NOUN . (O) 
We PRON (O) mainly ADV (O) rely VERB (O) on ADP (O) visual NOUN (O) comparisons NOUN (O) instead ADV . (O) 
We PRON (O) strongly ADV (O) encourage VERB (O) readers NOUN (O) to ADP (O) listen VERB (O) to ADP (O) the DET (O) provided VERB (O) samples NOUN . (O) 
First ADV , (O) we PRON (O) compare VERB (O) with ADP (O) a NOUN (O) [vanilla NOUN (B) seq2seq NOUN (I) model NOUN (I)] . 
Both DET (O) the DET (O) [encoder NOUN (B)] and CCONJ (O) [decoder NOUN (B)] use NOUN (O) 2 NUM (O) layers NOUN (O) of ADP (O) residual ADJ (O) [RNNs PROPN (B)] , where ADV (O) each DET (O) layer NOUN (O) has AUX (O) 256 NUM (O) [GRU PROPN (B) cells NOUN (I)] (we PRON (O) tried VERB (O) [LSTM PROPN (B)] and CCONJ (O) got VERB (O) similar ADJ (O) results NOUN) . (O) 
No NOUN (O) [pre ADJ - net ADJ (B)] or CCONJ (O) [post NOUN - processing ADJ (B) net NOUN (I)] is AUX (O) used VERB , (O) and CCONJ (O) the DET (O) [decoder NOUN (B)] directly ADV (O) predicts VERB (O) [linear ADJ - scale NOUN (B) log NOUN (I) magnitude NOUN (I) spectrogram PROPN (I)] . 
We PRON (O) found VERB (O) that SCONJ (O) scheduled VERB (O) sampling NOUN (O) (sampling VERB (O) rate NOUN (O) 0.5 NUM) (O) is AUX (O) required VERB (O) for ADP (O) this DET (O) model NOUN (O) to ADP (O) learn VERB (O) alignments VERB (O) and CCONJ (O) generalize VERB . (O) 
We PRON (O) show NOUN (O) the DET (O) learned VERB (O) [attention NOUN (B) alignment NOUN (I)] in ADP (O) Figure NOUN (O) 3 NUM . (O) 
Figure NOUN (O) 3(a PROPN) (O) reveals VERB (O) that SCONJ (O) the DET (O) [vanilla NOUN (B) seq2seq NOUN (I)] learns VERB (O) a NOUN (O) poor ADJ (O) alignment NOUN . (O) 
One NUM (O) problem NOUN (O) is AUX (O) that SCONJ (O) attention NOUN (O) tends VERB (O) to ADP (O) get AUX (O) stuck PROPN (O) for ADP (O) many ADJ (O) frames VERB (O) before ADP (O) moving VERB (O) forward ADV , (O) which DET (O) causes VERB (O) bad ADJ (O) [speech NOUN (B)] intelligibility NOUN (O) in ADP (O) the DET (O) synthesized VERB (O) signal NOUN . (O) 
The DET (O) naturalness NOUN (O) and CCONJ (O) overall NOUN (O) duration NOUN (O) are AUX (O) destroyed VERB (O) as SCONJ (O) a NOUN (O) result VERB . (O) 
In ADP (O) contrast NOUN , (O) our DET (O) model NOUN (O) learns VERB (O) a NOUN (O) clean ADJ (O) and CCONJ (O) smooth VERB (O) alignment NOUN , (O) as SCONJ (O) shown VERB (O) in ADP (O) Figure NOUN (O) 3(c X) . (O) 
Second PROPN , (O) we PRON (O) compare VERB (O) with ADP (O) a NOUN (O) model NOUN (O) with ADP (O) the DET (O) [CBHG PROPN (B) encoder NOUN (I)] replaced VERB (O) by ADP (O) a NOUN (O) 2-layer NUM (O) residual ADJ (O) [GRU PROPN (B) encoder NOUN (I)] . 
The DET (O) rest NOUN (O) of ADP (O) the DET (O) model NOUN , (O) including VERB (O) the DET (O) [encoder NOUN (B) pre ADJ - net ADJ (I)] , remain VERB (O) exactly ADV (O) the DET (O) same ADJ . (O) 
Comparing PROPN (O) Figure NOUN (O) 3(b X) (O) and CCONJ (O) 3(c PROPN) , (O) we PRON (O) can VERB (O) see VERB (O) that SCONJ (O) the DET (O) alignment NOUN (O) from ADP (O) the DET (O) [GRU PROPN (B) encoder NOUN (I)] is AUX (O) noisier NOUN . (O) 
Listening NOUN (O) to ADP (O) synthesized VERB (O) signals NOUN , (O) we PRON (O) found VERB (O) that SCONJ (O) noisy PROPN (O) alignment NOUN (O) often ADV (O) leads VERB (O) to ADP (O) mispronunciations NOUN . (O) 
The DET (O) [CBHG PROPN (B) encoder NOUN (I)] reduces VERB (O) overfitting VERB (O) and CCONJ (O) generalizes VERB (O) well INTJ (O) to ADP (O) long ADJ (O) and CCONJ (O) complex NOUN (O) phrases NOUN . (O) 
Figures NOUN (O) 4(a NUM) (O) and CCONJ (O) 4(b PROPN) (O) demonstrate NOUN (O) the DET (O) benefit NOUN (O) of ADP (O) using VERB (O) the DET (O) [post NOUN - processing ADJ (B) net NOUN (I)] . 
We PRON (O) trained VERB (O) a NOUN (O) model NOUN (O) without ADP (O) the DET (O) [post NOUN - processing ADJ (B) net NOUN (I)] while SCONJ (O) keeping NOUN (O) all DET (O) the DET (O) other ADJ (O) components NOUN (O) untouched ADJ (O) (except SCONJ (O) that SCONJ (O) the DET (O) [decoder NOUN (B) RNN PROPN (I)] predicts VERB (O) [linear ADJ - scale NOUN (B) spectrogram NOUN (I)]) . 
With ADP (O) more ADJ (O) contextual NOUN (O) information NOUN , (O) the DET (O) prediction NOUN (O) from ADP (O) the DET (O) [post NOUN - processing ADJ (B) net NOUN (I)] contains VERB (O) better ADJ (O) resolved VERB (O) harmonics NOUN (O) (e.g. ADV (O) higher ADJ (O) harmonics NOUN (O) between ADP (O) bins PROPN (O) 100 NUM (O) and CCONJ (O) 400 NUM) (O) and CCONJ (O) high ADJ (O) frequency NOUN (O) formant NOUN (O) structure NOUN , (O) which DET (O) reduces VERB (O) synthesis NOUN (O) artifacts NOUN . (O) 

 Figure NOUN (O) : Predicted VERB (O) [spectrograms NOUN (B)] with ADP (O) and CCONJ (O) without ADP (O) using VERB (O) the DET (O) [post NOUN - processing ADJ (B) net NOUN (I)] . 

 [MEAN PROPN (B) OPINION PROPN (I) SCORE PROPN (I) TESTS NOUN (I)] 

 We PRON (O) conduct NOUN (O) [mean NOUN (B) opinion NOUN (I) score NOUN (I) tests VERB (I)] , where ADV (O) the DET (O) subjects NOUN (O) were AUX (O) asked VERB (O) to ADP (O) rate NOUN (O) the DET (O) naturalness NOUN (O) of ADP (O) the DET (O) stimuli PROPN (O) in ADP (O) a NOUN (O) 5-point NUM (O) Likert PROPN (O) scale NOUN (O) score NOUN . (O) 
The DET (O) [MOS PROPN (B) tests NOUN (I)] were AUX (O) crowdsourced VERB (O) from ADP (O) [native NOUN (B) speakers NOUN (I)] . 
100 NUM (O) unseen ADJ (O) phrases NOUN (O) were AUX (O) used VERB (O) for ADP (O) the DET (O) tests VERB (O) and CCONJ (O) each DET (O) phrase NOUN (O) received VERB (O) 8 NUM (O) ratings NOUN . (O) 
When ADV (O) computing NOUN (O) [MOS PROPN (B)] , we PRON (O) only ADV (O) include VERB (O) ratings NOUN (O) where ADV (O) headphones NOUN (O) were AUX (O) used VERB . (O) 
We PRON (O) compare VERB (O) our DET (O) model NOUN (O) with ADP (O) a NOUN (O) parametric NOUN (O) (based VERB (O) on ADP (O) [LSTM PROPN (B)]) and CCONJ (O) a NOUN (O) concatenative ADJ (O) system NOUN , (O) both DET (O) of ADP (O) which DET (O) are AUX (O) in ADP (O) production NOUN . (O) 
As SCONJ (O) shown VERB (O) in ADP (O) Table NOUN (O) 2 NUM , (O) [Tacotron PROPN (B)] achieves VERB (O) an DET (O) [MOS PROPN (B)] of ADP (O) 3.82 NUM , (O) which DET (O) outperforms NOUN (O) the DET (O) [parametric NOUN (B) system NOUN (I)] . 
Given VERB (O) the DET (O) strong ADJ (O) baselines NOUN (O) and CCONJ (O) the DET (O) artifacts NOUN (O) introduced VERB (O) by ADP (O) the DET (O) [Griffin PROPN - Lim PROPN (B) synthesis NOUN (I)] , this DET (O) represents VERB (O) a NOUN (O) very ADV (O) promising NOUN (O) result VERB . (O) 

 Table NOUN (O) : 5-scale NUM (O) [mean NOUN (B) opinion NOUN (I) score NOUN (I) evaluation NOUN (I)] .                                        

 DISCUSSIONS PROPN (O) 

 We PRON (O) have AUX (O) proposed VERB (O) [Tacotron PROPN (B)] , an DET (O) integrated VERB (O) [end NOUN - to ADP - end NOUN (B) generative PROPN (I) TTS PROPN (I) model NOUN (I)] that DET (O) takes VERB (O) a NOUN (O) character NOUN (O) sequence NOUN (O) as SCONJ (O) input NOUN (O) and CCONJ (O) outputs NOUN (O) the DET (O) corresponding VERB (O) [spectrogram NOUN (B)] . 
With ADP (O) a NOUN (O) very ADV (O) simple ADJ (O) [waveform PROPN (B) synthesis NOUN (I)] module NOUN , (O) it PRON (O) achieves VERB (O) a NOUN (O) 3.82 NUM (O) [MOS PROPN (B) score NOUN (I)] on ADP (O) US PROPN (O) English PROPN , (O) outperforming VERB (O) a NOUN (O) production NOUN (O) [parametric NOUN (B) system NOUN (I)] in ADP (O) terms NOUN (O) of ADP (O) naturalness NOUN . (O) 
[Tacotron PROPN (B)] is AUX (O) frame NOUN - based VERB , (O) so CCONJ (O) the DET (O) inference NOUN (O) is AUX (O) substantially ADV (O) faster ADV (O) than SCONJ (O) sample NOUN - level NOUN (O) autoregressive ADJ (O) methods NOUN . (O) 
Unlike PROPN (O) previous ADJ (O) work NOUN , (O) [Tacotron PROPN (B)] does AUX (O) not PART (O) need NOUN (O) handengineered VERB (O) [linguistic NOUN (B) features VERB (I)] or CCONJ (O) complex NOUN (O) components NOUN (O) such ADJ (O) as SCONJ (O) an DET (O) [HMM PROPN (B) aligner NOUN (I)] . 
It PRON (O) can VERB (O) be AUX (O) trained VERB (O) from ADP (O) scratch NOUN (O) with ADP (O) random ADJ (O) initialization NOUN . (O) 
We PRON (O) perform VERB (O) simple ADJ (O) [text NOUN (B) normalization NOUN (I)] , though SCONJ (O) recent ADJ (O) advancements NOUN (O) in ADP (O) learned VERB (O) [text NOUN (B) normalization NOUN (I)] may VERB (O) render NOUN (O) this DET (O) unnecessary ADJ (O) in ADP (O) the DET (O) future NOUN . (O) 
We PRON (O) have AUX (O) yet CCONJ (O) to ADP (O) investigate VERB (O) many ADJ (O) aspects NOUN (O) of ADP (O) our DET (O) model NOUN (O) ; many ADJ (O) early ADV (O) design NOUN (O) decisions NOUN (O) have AUX (O) gone VERB (O) unchanged ADJ . (O) 
Our DET (O) [output NOUN (B) layer NOUN (I)] , attention NOUN (O) module PROPN , (O) [loss NOUN (B) function NOUN (I)] , and CCONJ (O) [Griffin PROPN - Lim PROPN (B)]-based VERB [waveform NOUN (B) synthesizer NOUN (I)] are AUX (O) all DET (O) ripe NOUN (O) for ADP (O) improvement NOUN . (O) 
For ADP (O) example NOUN , (O) it PRON ’s VERB (O) well INTJ (O) known VERB (O) that SCONJ (O) [Griffin PROPN - Lim PROPN (B) outputs NOUN (I)] may VERB (O) have AUX (O) audible ADJ (O) artifacts NOUN . (O) 
We PRON (O) are AUX (O) currently ADV (O) working VERB (O) on ADP (O) fast VERB (O) and CCONJ (O) [high ADJ - quality NOUN (B) neural ADJ - network NOUN - based VERB (I) spectrogram PROPN (I) inversion NOUN (I)] . 
