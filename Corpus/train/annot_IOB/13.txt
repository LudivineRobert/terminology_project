[Deep (B) Voice (I)] : Real-time (O) [Neural (B) Text-to-Speech (I)] 

Abstract (O) 

We (O) present (O) [Deep (B) Voice (I)], a (O) production-quality (O) [text-to-speech (B) system (I)] constructed (O) entirely (O) from (O) [deep (B) neural (I) networks (I)]. 
[Deep (B) Voice (I)] lays (O) the (O) groundwork (O) for (O) truly (O) [end-to-end (B) neural (I) speech (I) synthesis (I)]. 
The (O) system (O) comprises (O) five (O) major (O) building (O) blocks (O) : a (O) [segmentation (B) model (I)] for (O) locating (O) [phoneme (B) boundaries (I)], a (O) [grapheme-to-phoneme (B) conversion (I) model (I)], a (O) [phoneme (B) duration (I) prediction (I) model (I)], a (O) [fundamental (B) frequency (I) prediction (I) model (I)], and (O) an (O) [audio (B) synthesis (I) model (I)]. 
For (O) the (O) [segmentation (B) model (I)], we (O) propose (O) a (O) novel (O) way (O) of (O) performing (O) [phoneme (B) boundary (I)] detection (O) with (O) [deep (B) neural (I) networks (I)] using (O) [connectionist (B) temporal (I) classification (I)] (CTC) (O) loss. (O) 
For (O) the (O) [audio (B) synthesis (I) model (I)], we (O) implement (O) a (O) variant (O) of (O) [WaveNet (B)] that (O) requires (O) fewer (O) parameters (O) and (O) trains (O) faster (O) than (O) the (O) original. (O) 
By (O) using (O) a (O) [neural (B) network (I)] for (O) each (O) component, (O) our (O) system (O) is (O) simpler (O) and (O) more (O) flexible (O) than (O) traditional (O) [text-to-speech (B) systems (I)], where (O) each (O) component (O) requires (O) [laborious (B) feature (I) engineering (I)] and (O) extensive (O) domain (O) expertise. (O) 
Finally, (O) we (O) show (O) that (O) inference (O) with (O) our (O) system (O) can (O) be (O) performed (O) faster (O) than (O) real (O) time (O) and (O) describe (O) optimized (O) [WaveNet (B) inference (I)] kernels (O) on (O) both (O) [CPU (B)] and (O) [GPU (B)] that (O) achieve (O) up (O) to (O) 400x (O) speedups (O) over (O) existing (O) implementations. (O) 


Introduction (O) 

Synthesizing (O) [artificial (B) human (I) speech (I)] from (O) text, (O) commonly (O) known (O) as (O) [text-to-speech (B)] ([TTS (B)]), is (O) an (O) essential (O) component (O) in (O) many (O) applications (O) such (O) as (O) [speech-enabled (B) devices (I)], navigation (O) systems, (O) and (O) accessibility (O) for (O) the (O) visually-impaired. (O) 
Fundamentally, (O) it (O) allows (O) human-technology (O) interaction (O) without (O) requiring (O) visual (O) interfaces. (O) 
[Modern (B) TTS (I) systems (I)] are (O) based (O) on (O) complex, (O) multi-stage (O) processing (O) pipelines, (O) each (O) of (O) which (O) may (O) rely (O) on (O) hand-engineered (O) features (O) and (O) heuristics. (O) 
Due (O) to (O) this (O) complexity, (O) developing (O) new (O) [TTS (B) systems (I)] can (O) be (O) very (O) labor (O) intensive (O) and (O) difficult. (O) 
[Deep (B) Voice (I)] is (O) inspired (O) by (O) traditional (O) [text-to-speech (B) pipelines (I)] and (O) adopts (O) the (O) same (O) structure, (O) while (O) replacing (O) all (O) components (O) with (O) [neural (B) networks (I)] and (O) using (O) [simpler (B) features (I)] : first (O) we (O) convert (O) text (O) to (O) [phoneme (B)] and (O) then (O) use (O) an (O) [audio (B) synthesis (I) model (I)] to (O) convert (O) [linguistic (B) features (I)] into (O) [speech (B)] (Taylor, (O) 2009). (O) 
Unlike (O) prior (O) work (O) (which (O) uses (O) hand-engineered (O) features (O) such (O) as (O) [spectral (B) envelope (I)], [spectral (B) parameters (I)], aperiodic (O) parameters, (O) etc.), (O) our (O) [only (B) features (I)] are (O) [phonemes (B)] with (O) stress (O) annotations, (O) [phoneme (B) durations (I)], and (O) [fundamental (B) frequency (I)] (F0). (O) 
This (O) choice (O) of (O) features (O) makes (O) our (O) system (O) more (O) readily (O) applicable (O) to (O) [new (B) datasets (I)], voices, (O) and (O) domains (O) without (O) any (O) [manual (B) data (I)] annotation (O) or (O) [additional (B) feature (I) engineering (I)]. 
We (O) demonstrate (O) this (O) claim (O) by (O) retraining (O) our (O) entire (O) pipeline (O) without (O) any (O) [hyperparameter (B)] changes (O) on (O) an (O) entirely (O) [new (B) dataset (I)] that (O) contains (O) solely (O) [audio (B)] and (O) unaligned (O) textual (O) transcriptions (O) andgenerating (O) relatively (O) [high (B) quality (I) speech (I)]. 
In (O) a (O) conventional (O) [TTS (B) system (I)] this (O) adaptation (O) requires (O) days (O) to (O) weeks (O) of (O) tuning, (O) whereas (O) [Deep (B) Voice (I)] allows (O) you (O) to (O) do (O) it (O) in (O) only (O) a (O) fewhours (O) of (O) manual (O) effort (O) and (O) the (O) time (O) it (O) takes (O) models (O) to (O) train. (O) 
Realtime (O) inference (O) is (O) a (O) requirement (O) for (O) a (O) production-quality (O) [TTS (B) system (I)] ; without (O) it, (O) the (O) system (O) is (O) unusable (O) for (O) most (O) applications (O) of (O) [TTS (B)]. 
Prior (O) work (O) has (O) demonstrated (O) that (O) a (O) [WaveNet (B)] (van (O) den (O) Oord (O) et (O) al., (O) 2016) (O) can (O) generate (O) close (O) to (O) [human-level (B) speech (I)]. 
However, (O) [WaveNet (B) inference (I)] poses (O) adaunting (O) computational (O) problem (O) due (O) to (O) the (O) high-frequency, (O) autoregressive (O) nature (O) of (O) the (O) model, (O) and (O) it (O) has (O) been (O) hit (O) her (O) to (O) unknown (O) whether (O) such (O) models (O) can (O) be (O) used (O) in (O) a (O) production (O) system. (O) 
We (O) answer (O) this (O) question (O) in (O) the (O) affirmative (O) anddemonstrate (O) efficient, (O) faster-than-realtime (O) [WaveNet (B) inference (I)] kernels (O) that (O) produce (O) [high-quality (B)] 16 (O) kHz (O) [audio (B)] andrealize (O) a (O) 400X (O) speedup (O) over (O) previous (O) [WaveNet (B) inference (I)] implementations (O) (Paine (O) et (O) al., (O) 2016). (O) 

Related (O) Work (O) 

Previous (O) work (O) uses (O) [neural (B) networks (I)] as (O) substitutes (O) for (O) several (O) [TTS (B) system (I)] components, (O) including (O) [grapheme-to-phoneme (B) conversion (I) models (I)] (Rao (O) et (O) al., (O) 2015 (O) ; Yao (O) & Zweig, (O) 2015), (O) [phoneme (B) duration (I) prediction (I) models (I)] (Zen (O) & Sak, (O) 2015), (O) [fundamental (B) frequency (I) prediction (I) models (I)] (Pascual (O) & Bonafonte, (O) 2016 (O) ; Ronanki (O) et (O) al., (O) 2016), (O) and (O) [audio (B) synthesis (I) models (I)] (van (O) den (O) Oord (O) et (O) al., (O) 2016 (O) ; Mehri (O) et (O) al., (O) 2016). (O) 
Unlike (O) [Deep (B) Voice (I)], however, (O) none (O) of (O) these (O) systems (O) solve (O) the (O) entire (O) problem (O) of (O) [TTS (B)] and (O) many (O) of (O) them (O) use (O) specialized (O) hand-engineered (O) features (O) developed (O) specifically (O) for (O) their (O) domain. (O) 
Most (O) recently, (O) there (O) has (O) been (O) a (O) lot (O) of (O) work (O) in (O) parametric (O) [audio (B) synthesis (I)], notably (O) [WaveNet (B)], [SampleRNN (B)], and (O) Char2Wav (O) (van (O) den (O) Oord (O) et (O) al., (O) 2016 (O) ; Mehri (O) et (O) al., (O) 2016 (O) ; Sotelo (O) et (O) al., (O) 2017). (O) 
While (O) [WaveNet (B)] can (O) be (O) used (O) for (O) both (O) conditional (O) and (O) unconditional (O) [audio (B) generation (I)], [SampleRNN (B)] is (O) only (O) used (O) for (O) unconditional (O) [audio (B) generation (I)]. 
Char2Wav (O) extends (O) [SampleRNN (B)] with (O) an (O) [attention-based (B) phoneme (I) duration (I) model (I)] and (O) the (O) equivalent (O) of (O) an (O) F0 (O) [prediction (B) model (I)], effectively (O) providing (O) local (O) conditioning (O) information (O) to (O) a (O) [SampleRNN (B)]-based [vocoder (B)]. 
[Deep (B) Voice (I)] differs (O) from (O) these (O) systems (O) in (O) several (O) key (O) aspects (O) that (O) notably (O) increase (O) the (O) scope (O) of (O) the (O) problem. (O) 
First, (O) [Deep (B) Voice (I)] is (O) completely (O) standalone (O) ; training (O) a (O) new (O) [Deep (B) Voice (I) system (I)] does (O) not (O) require (O) a (O) pre-existing (O) [TTS (B) system (I)], and (O) can (O) be (O) done (O) from (O) scratch (O) using (O) a (O) dataset (O) of (O) short (O) [audio (B) clips (I)] and (O) corresponding (O) textual (O) transcripts. (O) 
In (O) contrast, (O) reproducing (O) either (O) of (O) the (O) aforementioned (O) systems (O) requires (O) access (O) and (O) understanding (O) of (O) a (O) pre-existing (O) [TTS (B) system (I)], because (O) they (O) use (O) features (O) from (O) another (O) [TTS (B) system (I)] either (O) at (O) training (O) or (O) inference (O) time. (O) 
Second, (O) [Deep (B) Voice (I)] minimizes (O) the (O) use (O) of (O) hand-engineered (O) features (O) ; it (O) uses (O) [one-hot (B) encoded (I) characters (I)] for (O) [grapheme (B) to (I) phoneme (I) conversion (I)], [one-hot (B) encoded (I) phonemes (I)] and (O) stresses, (O) [phoneme (B) durations (I)] in (O) milliseconds, (O) and (O) normalized (O) log (O) [fundamental (B) frequency (I)] that (O) can (O) be (O) computed (O) from (O) [waveforms (B)] using (O) any (O) F0 (O) estimation (O) algorithm. (O) 
All (O) of (O) these (O) can (O) easily (O) be (O) obtained (O) from (O) [audio (B)] and (O) transcripts (O) with (O) minimal (O) effort. (O) 
In (O) contrast, (O) prior (O) works (O) use (O) a (O) much (O) more (O) [complex (B) feature (I) representation (I)], that (O) effectively (O) makes (O) reproducing (O) the (O) system (O) impossible (O) without (O) a (O) pre-existing (O) [TTS (B) system (I)]. 
[WaveNet (B)] uses (O) [several (B) features (I)] from (O) a (O) [TTS (B) system (I)] (Zen (O) et (O) al., (O) 2013), (O) that (O) include (O) values (O) such (O) as (O) the (O) number (O) of (O) syllables (O) in (O) a (O) word, (O) position (O) of (O) syllables (O) in (O) the (O) phrase, (O) position (O) of (O) the (O) current (O) frame (O) in (O) the (O) [phoneme (B)], and (O) [dynamic (B) features (I)] of (O) the (O) [speech (B) spectrum (I)] like (O) [spectral (B)] and (O) excitation (O) parameters, (O) as (O) well (O) as (O) their (O) time (O) derivatives. (O) 
Char2Wav (O) relies (O) on (O) [vocoder (B) features (I)] from (O) the (O) WORLD (O) [TTS (B) system (I)] (Morise (O) et (O) al., (O) 2016) (O) for (O) pre-training (O) their (O) alignment (O) module (O) which (O) include (O) F0, (O) [spectral (B) envelope (I)], and (O) aperiodic (O) parameters. (O) 
Finally, (O) we (O) focus (O) on (O) creating (O) a (O) production-ready (O) system, (O) which (O) requires (O) that (O) our (O) models (O) run (O) in (O) real-time (O) for (O) inference. (O) 
[Deep (B) Voice (I)] can (O) synthesize (O) [audio (B)] in (O) fractions (O) of (O) a (O) second, (O) and (O) offers (O) a (O) tunable (O) trade-off (O) between (O) synthesis (O) speed (O) and (O) [audio (B) quality (I)]. 
In (O) contrast, (O) previous (O) results (O) with (O) [WaveNet (B)] require (O) several (O) minutes (O) of (O) runtime (O) to (O) synthesize (O) one (O) second (O) of (O) [audio (B)]. 
We (O) are (O) unaware (O) of (O) similar (O) benchmarks (O) for (O) [SampleRNN (B)], but (O) the (O) 3-tier (O) architecture (O) as (O) described (O) in (O) the (O) original (O) publication (O) requires (O) approximately (O) 4-5X (O) as (O) much (O) compute (O) during (O) inference (O) as (O) our (O) largest (O) [WaveNet (B) models (I)], so (O) running (O) the (O) model (O) in (O) real-time (O) may (O) prove (O) challenging. (O) 

[TTS (B) System (I)] Components (O) 

As (O) shown (O) in (O) Fig. (O) 1, (O) the (O) [TTS (B) system (I)] consists (O) of (O) five (O) major (O) building (O) blocks (O) : 
• (O) The (O) [grapheme-to-phoneme (B) model (I) converts (I)] from (O) written (O) text (O) (English (O) characters) (O) to (O) phonemes (O) (encoded (O) using (O) a (O) phonemic (O) alphabet (O) such (O) as (O) ARPABET). (O) 
• (O) The (O) [segmentation (B) model (I)] locates (O) [phoneme (B) boundaries (I)] in (O) the (O) [voice (B) dataset (I)]. Given (O) an (O) [audio (B) file (I)] and (O) a (O) [phoneme-by-phoneme (B) transcription (I)] of (O) the (O) [audio (B)], the (O) [segmentation (B) model (I)] identifies (O) where (O) in (O) the (O) [audio (B)] each (O) [phoneme (B)] begins (O) and (O) ends. (O) 
• (O) The (O) [phoneme (B) duration (I) model (I)] predicts (O) the (O) temporal (O) duration (O) of (O) every (O) [phoneme (B)] in (O) a (O) [phoneme (B) sequence (I)] (an (O) utterance). (O) 
• (O) The (O) [fundamental (B) frequency (I) model (I)] predicts (O) whether (O) a (O) [phoneme (B)] is (O) voiced. (O) If (O) it (O) is, (O) the (O) model (O) predicts (O) the (O) [fundamental (B) frequency (I)] (F0) (O) throughout (O) the (O) [phoneme (B)]’s duration. (O) 
• (O) The (O) [audio (B) synthesis (I) model (I)] combines (O) the (O) outputs (O) of (O) the (O) [grapheme-to-phoneme (B)], [phoneme (B) duration (I)], and (O) [fundamental (B) frequency (I) prediction (I) models (I)] and (O) synthesizes (O) [audio (B)] at (O) a (O) high (O) sampling (O) rate, (O) corresponding (O) to (O) the (O) desired (O) text. (O) 

During (O) inference, (O) text (O) is (O) fed (O) through (O) the (O) [grapheme-to (B) phoneme (I) model (I)] or (O) a (O) [phoneme (B) dictionary (I)] to (O) generate (O) [phonemes (B)]. 
Next, (O) the (O) [phonemes (B)] are (O) provided (O) as (O) inputs (O) to (O) the (O) [phoneme (B) duration (I) model (I)] and (O) F0 (O) [prediction (B) model (I)] to (O) assign (O) durations (O) to (O) each (O) [phoneme (B)] and (O) generate (O) an (O) F0 (O) contour. (O) 
Finally, (O) the (O) [phonemes (B)], [phoneme (B) durations (I)], and (O) F0 (O) are (O) used (O) as (O) local (O) conditioning (O) [input (B) features (I)] to (O) the (O) [audio (B) synthesis (I) model (I)], which (O) generates (O) the (O) final (O) utterance. (O) 
Unlike (O) the (O) other (O) models, (O) the (O) [segmentation (B) model (I)] is (O) not (O) used (O) during (O) inference. (O) 
Instead, (O) it (O) is (O) used (O) to (O) annotate (O) the (O) [training (B) voice (I) data (I)] with (O) [phoneme (B) boundaries (I)]. 
The (O) [phoneme (B) boundaries (I)] imply (O) durations, (O) which (O) can (O) be (O) used (O) to (O) train (O) the (O) [phoneme (B) duration (I) model (I)]. 
The (O) [audio (B)], annotated (O) with (O) [phonemes (B)] and (O) [phoneme (B) durations (I)] as (O) well (O) as (O) [fundamental (B) frequency (I)], is (O) used (O) to (O) train (O) the (O) [audio (B) synthesis (I) model (I)]. 
In (O) the (O) following (O) sections, (O) we (O) describe (O) all (O) the (O) building (O) blocks (O) in (O) detail. (O) 

[Grapheme-to-Phoneme (B) Model (I)] 

Our (O) [grapheme-to-phoneme (B) model (I)] is (O) based (O) on (O) the (O) [encoder-decoder (B) architecture (I)] developed (O) by (O) (Yao (O) & Zweig, (O) 2015). (O) 
However, (O) we (O) use (O) a (O) [multi-layer (B) bidirectional (I) encoder (I)] with (O) a (O) [gated (B) recurrent (I) unit (I) (GRU) (I) nonlinearity (I)] and (O) an (O) equally (O) [deep (B) unidirectional (I) GRU (I) decoder (I)] (Chung (O) et (O) al., (O) 2014). (O) 
The (O) initial (O) state (O) of (O) every (O) [decoder (B) layer (I)] is (O) initialized (O) to (O) the (O) final (O) [hidden (B) state (I)] of (O) the (O) corresponding (O) [encoder (B) forward (I) layer (I)]. 
The (O) architecture (O) is (O) trained (O) with (O) teacher (O) forcing (O) and (O) decoding (O) is (O) performed (O) using (O) beam (O) search. (O) 
We (O) use (O) 3 (O) bidirectional (O) layers (O) with (O) 1024 (O) units (O) each (O) in (O) the (O) [encoder (B)] and (O) 3 (O) unidirectional (O) layers (O) of (O) the (O) same (O) size (O) in (O) the (O) [decoder (B)] and (O) a (O) beam (O) search (O) with (O) a (O) width (O) of (O) 5 (O) candidates. (O) 
During (O) training, (O) we (O) use (O) [dropout (B)] with (O) probability (O) 0.95 (O) after (O) each (O) [recurrent (B) layer (I)]. 
For (O) training, (O) we (O) use (O) the (O) [Adam (B) optimization (I) algorithm (I)] with (O) β (O) 1 (O) = 0.9, (O) β (O) 2 (O) = 0. (O) 
999, (O) ε (O) = 10 (O) −8, (O) a (O) [batch (B) size (I)] of (O) 64, (O) a (O) [learning (B) rate (I)] of (O) 10 (O) −3, (O) and (O) an (O) annealing (O) rate (O) of (O) 0.85 (O) applied (O) every (O) 1000 (O) iterations (O) ([Kingma (B)] & Ba, (O) 2014). (O) 

[Segmentation (B) Model (I)] 

Our (O) [segmentation (B) model (I)] is (O) trained (O) to (O) output (O) the (O) alignment (O) between (O) a (O) given (O) utterance (O) and (O) a (O) sequence (O) of (O) [target (B) phonemes (I)]. 
This (O) task (O) is (O) similar (O) to (O) the (O) problem (O) of (O) aligning (O) [speech (B)] to (O) written (O) output (O) in (O) [speech (B) recognition (I)]. 
In (O) that (O) domain, (O) the (O) [connectionist (B) temporal (I) classification (I)] (CTC) (O) [loss (B) function (I)] has (O) been (O) shown (O) to (O) focus (O) on (O) character (O) alignments (O) to (O) learn (O) a (O) mapping (O) between (O) sound (O) and (O) text (O) (Graves (O) et (O) al., (O) 2006). (O) 
We (O) adapt (O) the (O) convolutional (O) [recurrent (B) neural (I) network (I) architecture (I)] from (O) a (O) state-of-the-art (O) [speech (B) recognition (I) system (I)] (Amodei (O) et (O) al., (O) 2015) (O) for (O) [phoneme (B) boundary (I)] detection. (O) 
A (O) network (O) trained (O) with (O) CTC (O) to (O) generate (O) [sequences (B) of (I) phonemes (I)] will (O) produce (O) brief (O) peaks (O) for (O) every (O) output (O) [phoneme (B)]. 
Although (O) this (O) is (O) sufficient (O) to (O) roughly (O) align (O) the (O) [phonemes (B)] to (O) the (O) [audio (B)], it (O) is (O) insufficient (O) to (O) detect (O) precise (O)   [phoneme (B) boundaries (I)]. 
To (O) overcome (O) this, (O) we (O) train (O) to (O) predict (O) [sequences (B) of (I) phoneme (I)] pairs (O) rather (O) than (O) single (O) [phonemes (B)]. 
The (O) network (O) will (O) then (O) tend (O) to (O) output (O) [phoneme (B) pairs (I)] at (O) timesteps (O) close (O) to (O) the (O) boundary (O) between (O) two (O) [phonemes (B)] in (O) a (O) pair. (O) 
To (O) illustrate (O) our (O) label (O) encoding, (O) consider (O) the (O) string (O) “ (O) Hello (O) ! ”. (O) 
To (O) convert (O) this (O) to (O) a (O) [sequence (B) of (I) phoneme (I)] pair (O) labels, (O) convert (O) the (O) utterance (O) to (O) phonemes (O) (using (O) a (O) [pronunciation (B) dictionary (I)] such (O) as (O) [CMUDict (B)] or (O) a (O) [grapheme-to-phoneme (B) model (I)]) and (O) pad (O) the (O) [phoneme (B) sequence (I)] on (O) either (O) end (O) with (O) the (O) silence (O) [phoneme (B)] to (O) get (O) “ (O) sil (O) HH (O) EH (O) L (O) OW (O) sil (O) ”. (O) 
Finally, (O) construct (O) consecutive (O) [phoneme (B) pairs (I)] and (O) get (O) “ (O) (sil, (O) HH), (O) (HH, (O) EH), (O) (EH, (O) L), (O) (L, (O) OW), (O) (OW, (O) sil) (O) ”. (O) 
Input (O) [audio (B)] is (O) featurized (O) by (O) computing (O) 20 (O) [Mel (B)]-frequency [cepstral (B) coefficients (I)] (MFCCs) (O) with (O) a (O) ten (O) millisecond (O) stride. (O) 
On (O) top (O) of (O) the (O) input (O) layer, (O) there (O) are (O) two (O) convolution (O) layers (O) (2D (O) convolutions (O) in (O) time (O) and (O) frequency), (O) three (O) bidirectional (O) recurrent (O) [GRU (B) layers (I)], and (O) finally (O) a (O) [softmax (B) output (I) layer (I)]. 
The (O) convolution (O) layers (O) use (O) kernels (O) with (O) unit (O) stride, (O) height (O) nine (O) (in (O) frequency (O) bins), (O) and (O) width (O) five (O) (in (O) time) (O) and (O) the (O) [recurrent (B) layers (I)] use (O) 512 (O) [GRU (B) cells (I)] (for (O) each (O) direction). (O) 
[Dropout (B)] with (O) a (O) probability (O) of (O) 0.95 (O) is (O) applied (O) after (O) the (O) last (O) convolution (O) and (O) [recurrent (B) layers (I)]. 
To (O) compute (O) the (O) [phoneme (B) pair (I) error (I) rate (I)] ([PPER (B)]), we (O) decode (O) using (O) beam (O) search. (O) 
To (O) decode (O) [phoneme (B) boundaries (I)], we (O) perform (O) a (O) beam (O) search (O) with (O) width (O) 50 (O) with (O) the (O) constraint (O) that (O) neighboring (O) [phoneme (B) pairs (I)] overlap (O) by (O) at (O) least (O) one (O) [phoneme (B)] and (O) keep (O) track (O) of (O) the (O) positions (O) in (O) the (O) utterance (O) of (O) each (O) [phoneme (B) pair (I)]. 
For (O) training, (O) we (O) use (O) the (O) [Adam (B) optimization (I) algorithm (I)] with (O) β (O) 1 (O) = 0.9, (O) β (O) 2 (O) = 0. (O) 
999, (O) ε (O) = 10 (O) −8, (O) a (O) [batch (B) size (I)] of (O) 128, (O) a (O) [learning (B) rate (I)] of (O) 10 (O) −4, (O) and (O) an (O) annealing (O) rate (O) of (O) 0.95 (O) applied (O) every (O) 500 (O) iterations (O) ([Kingma (B)] & Ba, (O) 2014). (O) 

[Phoneme (B) Duration (I) and (I) Fundamental (I) Frequency (I) Model (I)] 

We (O) use (O) a (O) single (O) architecture (O) to (O) jointly (O) predict (O) [phoneme (B) duration (I)] and (O) [time-dependent (B) fundamental (I) frequency (I)]. 
The (O) input (O) to (O) the (O) model (O) is (O) a (O) [sequence (B) of (I) phonemes (I)] with (O) stresses, (O) with (O) each (O) [phoneme (B)] and (O) stress (O) being (O) encoded (O) as (O) a (O) [one-hot (B) vector (I)]. 
The (O) architecture (O) comprises (O) two (O) fully (O) connected (O) layers (O) with (O) 256 (O) units (O) each (O) followed (O) by (O) two (O) unidirectional (O) [recurrent (B) layers (I)] with (O) 128 (O) [GRU (B) cells (I)] each (O) and (O) finally (O) a (O) fullyconnected (O) [output (B) layer (I)]. 
[Dropout (B)] with (O) a (O) probability (O) of (O) 0.8 (O) is (O) applied (O) after (O) the (O) initial (O) fully-connected (O) layers (O) and (O) the (O) last (O) [recurrent (B) layer (I)]. 
The (O) final (O) layer (O) produces (O) three (O) estimations (O) for (O) every (O) input (O) [phoneme (B)] : the (O) [phoneme (B) duration (I)], the (O) probability (O) that (O) the (O) [phoneme (B)] is (O) voiced (O) (i.e. (O) has (O) a (O) [fundamental (B) frequency (I)]), and (O) 20 (O) [time-dependent (B) F0 (I) values (I)], which (O) are (O) sampled (O) uniformly (O) over (O) the (O) predicted (O) duration. (O) 
The (O) model (O) is (O) optimized (O) by (O) minimizing (O) a (O) joint (O) loss (O) that (O) combines (O) [phoneme (B) duration (I) error (I)], [fundamental (B) frequency (I) error (I)], the (O) negative (O) [log (B) likelihood (I)] of (O) the (O) probability (O) that (O) the (O) [phoneme (B)] is (O) voiced, (O) and (O) a (O) penalty (O) term (O) proportional (O) to (O) the (O) absolute (O) change (O) of (O) F0 (O) with (O) respect (O) to (O) time (O) to (O) impose (O) smoothness. (O) 
The (O) specific (O) functional (O) form (O) of (O) the (O) [loss (B) function (I)] is (O) described (O) in (O) Appendix (O) B. (O) 
For (O) training, (O) we (O) use (O) the (O) [Adam (B) optimization (I) algorithm (I)] with (O) β (O) 1 (O) = 0.9, (O) β (O) 2 (O) = 0.999, (O) ε (O) = 10 (O) −8, (O) a (O) [batch (B) size (I)] of (O) 128, (O) a (O) [learning (B) rate (I)] of (O) 3 (O) × (O) 10 (O) −4, (O) and (O) an (O) annealing (O) rate (O) of (O) 0.9886 (O) applied (O) every (O) 400 (O) iterations (O) ([Kingma (B)] & Ba, (O) 2014). (O) 

[Audio (B) Synthesis (I) Model (I)] 

Our (O) [audio (B) synthesis (I) model (I)] is (O) a (O) variant (O) of (O) [WaveNet (B)]. 
[WaveNet (B)] consists (O) of (O) a (O) conditioning (O) network, (O) which (O) upsamples (O) [linguistic (B) features (I)] to (O) the (O) desired (O) frequency, (O) and (O) an (O) autoregressive (O) network, (O) which (O) generates (O) a (O) probability (O) distribution (O) P(y) (O) over (O) discretized (O) [audio (B) samples (I)] y (O) ∈ (O) { 0, (O) 1,..., (O) 255 (O) }. (O) 
We (O) vary (O) the (O) number (O) of (O) layers (O) `, (O) the (O) number (O) of (O) residual (O) channels (O) r (O) (dimension (O) of (O) the (O) [hidden (B) state (I)] of (O) every (O) layer), (O) and (O) the (O) number (O) of (O) skip (O) channels (O) s (O) (the (O) dimension (O) to (O) which (O) layer (O) outputs (O) are (O) projected (O) prior (O) to (O) the (O) [output (B) layer (I)]). 
[WaveNet (B)] consists (O) of (O) an (O) upsampling (O) and (O) conditioning (O) network, (O) followed (O) by (O) ` 2×1 (O) convolution (O) layers (O) with (O) r (O) residual (O) output (O) channels (O) and (O) gated (O) [tanh (B)] nonlinearities. (O) 
We (O) break (O) the (O) convolution (O) into (O) two (O) matrix (O) multiplies (O) per (O) timestep (O) with (O) W (O) prev (O) and (O) W (O) cur. (O) 
These (O) layers (O) are (O) connected (O) with (O) [residual (B) connections (I)]. 
The (O) hidden (O) state (O) of (O) every (O) layer (O) is (O) concatenated (O) to (O) an (O) ` r (O) [vector (B)] and (O) projected (O) to (O) s (O) skip (O) channels (O) with (O) W (O) skip, (O) followed (O) by (O) two (O) layers (O) of (O) 1 (O) × (O) 1 (O) convolutions (O) (with (O) weights (O) W (O) [relu (B)] and (O) W (O) out) (O) with (O) [relu (B)] nonlinearities. (O) 
[WaveNet (B)] uses (O) transposed (O) convolutions (O) for (O) upsampling (O) and (O) conditioning. (O) 
We (O) find (O) that (O) our (O) models (O) perform (O) better, (O) train (O) faster, (O) and (O) require (O) fewer (O) parameters (O) if (O) we (O) instead (O) first (O) encode (O) the (O) inputs (O) with (O) a (O) stack (O) of (O) bidirectional (O) quasi-[RNN (B) (QRNN) (I) layers (I)] (Bradbury (O) et (O) al., (O) 2016) (O) and (O) then (O) perform (O) upsampling (O) by (O) repetition (O) to (O) the (O) desired (O) frequency. (O) 
Our (O) [highest-quality (B) final (I) model (I)] uses (O) ` = 40 (O) layers, (O) r (O) = 64 (O) residual (O) channels, (O) and (O) s (O) = 256 (O) skip (O) channels. (O) 
For (O) training, (O) we (O) use (O) the (O) [Adam (B) optimization (I) algorithm (I)] with (O) β (O) 1 (O) = 0.9, (O) β (O) 2 (O) = 0.999, (O) ε (O) = 10 (O) −8, (O) a (O) [batch (B) size (I)] of (O) 8, (O) a (O) [learning (B) rate (I)] of (O) 10 (O) −3, (O) and (O) an (O) annealing (O) rate (O) of (O) 0.9886 (O) applied (O) every (O) 1,000 (O) iterations (O) ([Kingma (B)] & Ba, (O) 2014). (O) 
Please (O) refer (O) to (O) Appendix (O) A (O) for (O) full (O) details (O) of (O) our (O) [WaveNet (B) architecture (I)] and (O) the (O) [QRNN (B) layers (I)] we (O) use. (O) 

Results (O) 

We (O) train (O) our (O) models (O) on (O) an (O) internal (O) English (O) [speech (B) database (I)] containing (O) approximately (O) 20 (O) hours (O) of (O) [speech (B) data (I)] segmented (O) into (O) 13,079 (O) utterances. (O) 
In (O) addition, (O) we (O) present (O) [audio (B) synthesis (I) results (I)] for (O) our (O) models (O) trained (O) on (O) a (O) subset (O) of (O) the (O) Blizzard (O) 2013 (O) data (O) (Prahallad (O) et (O) al., (O) 2013). (O) 
Both (O) datasets (O) are (O) spoken (O) by (O) a (O) professional (O) [female (B) speaker (I)]. 
All (O) of (O) our (O) models (O) are (O) implemented (O) using (O) the (O) TensorFlow (O) framework (O) (Abadi (O) et (O) al., (O) 2015). (O) 

Segmentation (O) Results (O) 

We (O) train (O) on (O) 8 (O) TitanX (O) Maxwell (O) [GPUs (B)], splitting (O) each (O) batch (O) equally (O) among (O) the (O) [GPUs (B)] and (O) using (O) a (O) ring (O) all-reduce (O) to (O) average (O) gradients (O) computed (O) on (O) different (O) [GPUs (B)], with (O) each (O) iteration (O) taking (O) approximately (O) 1300 (O) milliseconds. (O) 
After (O) approximately (O) 14,000 (O) iterations, (O) the (O) model (O) converges (O) to (O) a (O) [phoneme (B) pair (I) error (I) rate (I)] of (O) 7 (O) %. (O) We (O) also (O) find (O) that (O) [phoneme (B) boundaries (I)] do (O) not (O) have (O) to (O) be (O) precise, (O) and (O) randomly (O) shifting (O) [phoneme (B) boundaries (I)] by (O) 10-30 (O) milliseconds (O) makes (O) no (O) difference (O) in (O) the (O) [audio (B) quality (I)], and (O) so (O) suspect (O) that (O) [audio (B) quality (I)] is (O) insensitive (O) to (O) the (O) [phoneme (B) pair (I) error (I) rate (I)] past (O) a (O) certain (O) point. (O) 

[Grapheme-to-Phoneme (B) Results (I)] 

We (O) train (O) a (O) [grapheme-to-phoneme (B) model (I)] on (O) data (O) obtained (O) from (O) [CMUDict (B)] (Weide, (O) 2008). (O) We (O) strip (O) out (O) all (O) words (O) that (O) do (O) not (O) start (O) with (O) a (O) letter, (O) contain (O) numbers, (O) or (O) have (O) multiple (O) pronunciations, (O) which (O) leaves (O) 124,978 (O) out (O) of (O) the (O) original (O) 133,854 (O) [grapheme-phoneme (B) sequence (I) pairs (I)]. 
We (O) train (O) on (O) a (O) single (O) TitanX (O) Maxwell (O) [GPU (B)] with (O) each (O) iteration (O) taking (O) approximately (O) 150 (O) milliseconds. (O) 
After (O) approximately (O) 20,000 (O) iterations, (O) the (O) model (O) converges (O) to (O) a (O) [phoneme (B) error (I) rate (I)] of (O) 5.8 (O) % and (O) a (O) [word (B) error (I) rate (I)] of (O) 28.7 (O) %, (O) which (O) are (O) on (O) par (O) with (O) previous (O) reported (O) results (O) (Yao (O) & Zweig, (O) 2015). (O) 
Unlike (O) prior (O) work, (O) we (O) do (O) not (O) use (O) a (O) language (O) model (O) during (O) decoding (O) and (O) do (O) not (O) include (O) words (O) with (O) multiple (O) pronunciations (O) in (O) our (O) data (O) set. (O) 

[Phoneme (B) Duration (I) and (I) Fundamental (I) Frequency (I) Results (I)] 

We (O) train (O) on (O) a (O) single (O) TitanX (O) Maxwell (O) [GPU (B)] with (O) each (O) iteration (O) taking (O) approximately (O) 120 (O) milliseconds. (O) 
After (O) approximately (O) 20,000 (O) iterations, (O) the (O) model (O) converges (O) to (O) a (O) mean (O) absolute (O) error (O) of (O) 38 (O) milliseconds (O) (for (O) [phoneme (B) duration (I)]) and (O) 29.4 (O) Hz (O) (for (O) [fundamental (B) frequency (I)]). 

[Audio (B) Synthesis (I) Results (I)] 

We (O) divide (O) the (O) utterances (O) in (O) our (O) [audio (B) dataset (I)] into (O) one (O) second (O) chunks (O) with (O) a (O) quarter (O) second (O) of (O) context (O) for (O) each (O) chunk, (O) padding (O) each (O) utterance (O) with (O) a (O) quarter (O) second (O) of (O) silence (O) at (O) the (O) beginning. (O) 
We (O) filter (O) out (O) chunks (O) that (O) are (O) predominantly (O) silence (O) and (O) end (O) up (O) with (O) 74,348 (O) total (O) chunks. (O) 
We (O) trained (O) models (O) with (O) varying (O) depth, (O) including (O) 10, (O) 20, (O) 30, (O) and (O) 40 (O) layers (O) in (O) the (O) residual (O) layer (O) stack. (O) 
We (O) find (O) that (O) models (O) below (O) 20 (O) layers (O) result (O) in (O) poor (O) quality (O) [audio (B)]. 
The (O) 20, (O) 30, (O) and (O) 40 (O) [layer (B) models (I)] all (O) produce (O) high (O) quality (O) recognizable (O) [speech (B)], but (O) the (O) 40 (O) [layer (B) models (I)] have (O) less (O) noise (O) than (O) the (O) 20 (O) [layer (B) models (I)], which (O) can (O) be (O) detected (O) with (O) high (O) quality (O) over-ear (O) headphones. (O) 
Previous (O) work (O) has (O) emphasized (O) the (O) importance (O) of (O) receptive (O) field (O) size (O) in (O) determining (O) model (O) quality. (O) 
Indeed, (O) the (O) 20 (O) [layer (B) models (I)] have (O) half (O) the (O) receptive (O) field (O) as (O) the (O) 40 (O) [layer (B) models (I)]. 
However, (O) when (O) run (O) at (O) 48 (O) kHz, (O) models (O) with (O) 40 (O) layers (O) have (O) only (O) 83 (O) milliseconds (O) of (O) receptive (O) field, (O) but (O) still (O) generate (O) high (O) quality (O) [audio (B)]. 
This (O) suggests (O) the (O) receptive (O) field (O) of (O) the (O) 20 (O) [layer (B) models (I)] is (O) sufficient, (O) and (O) we (O) conjecture (O) the (O) difference (O) in (O) [audio (B) quality (I)] is (O) due (O) to (O) some (O) other (O) factor (O) than (O) receptive (O) field (O) size. (O) 
We (O) train (O) on (O) 8 (O) TitanX (O) Maxwell (O) [GPUs (B)] with (O) one (O) chunk (O) per (O) [GPU (B)], using (O) a (O) ring (O) allreduce (O) to (O) average (O) gradients (O) computed (O) on (O) different (O) [GPUs (B)]. 
Each (O) iteration (O) takes (O) approximately (O) 450 (O) milliseconds. (O) 
Our (O) model (O) converges (O) after (O) approximately (O) 300,000 (O) iterations. (O) 
We (O) find (O) that (O) a (O) single (O) 1.25s (O) chunk (O) is (O) sufficient (O) to (O) saturate (O) the (O) compute (O) on (O) the (O) [GPU (B)] and (O) that (O) batching (O) does (O) not (O) increase (O) training (O) efficiency. (O) 
As (O) is (O) common (O) with (O) high-dimensional (O) generative (O) models (O) (Theis (O) et (O) al., (O) 2015), (O) model (O) loss (O) is (O) somewhat (O) uncorrelated (O) with (O) perceptual (O) quality (O) of (O) individual (O) samples. (O) 
While (O) models (O) with (O) unusually (O) high (O) loss (O) sound (O) distinctly (O) noisy, (O) models (O) that (O) optimize (O) below (O) a (O) certain (O) threshold (O) do (O) not (O) have (O) a (O) loss (O) indicative (O) of (O) their (O) quality. (O) 
In (O) addition, (O) changes (O) in (O) [model (B) architecture (I)] (such (O) as (O) depth (O) and (O) output (O) frequency) (O) can (O) have (O) a (O) significant (O) impact (O) on (O) model (O) loss (O) while (O) having (O) a (O) small (O) effect (O) on (O) [audio (B) quality (I)]. 
To (O) estimate (O) perceptual (O) quality (O) of (O) the (O) individual (O) stages (O) of (O) our (O) [TTS (B) pipeline (I)], we (O) crowdsourced (O) [mean (B) opinion (I) score (I)] ([MOS (B)]) ratings (O) (ratings (O) between (O) one (O) and (O) five, (O) higher (O) values (O) being (O) better) (O) from (O) Mechanical (O) Turk (O) using (O) the (O) CrowdMOS (O) [toolkit (B)] and (O) methodology (O) (Ribeiro (O) et (O) al., (O) 2011). (O) 
In (O) order (O) to (O) separate (O) the (O) effect (O) of (O) the (O) [audio (B)] preprocessing, (O) the (O) [WaveNet (B) model (I)] quality, (O) and (O) the (O) [phoneme (B) duration (I) and (I) fundamental (I) frequency (I) model (I)] quality, (O) we (O) present (O) [MOS (B) scores (I)] for (O) a (O) variety (O) of (O) utterance (O) types, (O) including (O) synthesis (O) results (O) where (O) the (O) [WaveNet (B) inputs (I)] (duration (O) and (O) F0) (O) are (O) extracted (O) from (O) [ground (B) truth (I) audio (I)] rather (O) than (O)   synthesized (O) by (O) other (O) models. (O) 
The (O) results (O) are (O) presented (O) in (O) Table (O) 1. (O) 
We (O) purposefully (O) include (O) [ground (B) truth (I) samples (I)] in (O) every (O) batch (O) of (O) samples (O) that (O) raters (O) evaluate (O) to (O) highlight (O) the (O) delta (O) from (O) human (O) [speech (B)] and (O) allow (O) raters (O) to (O) distinguish (O) finer (O) grained (O) differences (O) between (O) models (O) ; the (O) downside (O) of (O) this (O) approach (O) is (O) that (O) the (O) resulting (O) [MOS (B) scores (I)] will (O) be (O) significantly (O) lower (O) than (O) if (O) raters (O) are (O) presented (O) only (O) with (O) synthesized (O) [audio (B) samples (I)]. 
First (O) of (O) all, (O) we (O) find (O) a (O) significant (O) drop (O) in (O) [MOS (B)] when (O) simply (O) downsampling (O) the (O) [audio (B)] stream (O) from (O) 48 (O) kHz (O) to (O) 16 (O) kHz, (O) especially (O) in (O) combination (O) with (O) μ-law (O) companding (O) and (O) quantization, (O) likely (O) because (O) a (O) 48 (O) kHz (O) sample (O) is (O) presented (O) to (O) the (O) raters (O) as (O) a (O) baseline (O) for (O) a (O) 5 (O) score, (O) and (O) a (O) low (O) quality (O) noisy (O) synthesis (O) result (O) is (O) presented (O) as (O) a (O) 1. (O) 
When (O) used (O) with (O) ground (O) truth (O) durations (O) and (O) F0, (O) our (O) models (O) score (O) highly, (O) with (O) the (O) 95 (O) % confidence (O) intervals (O) of (O) our (O) models (O) intersecting (O) those (O) of (O) the (O) [ground (B) truth (I) samples (I)]. 
However, (O) using (O) synthesized (O) frequency (O) reduces (O) the (O) [MOS (B)], and (O) further (O) including (O) synthesized (O) durations (O) reduces (O) it (O) significantly. (O) 
We (O) conclude (O) that (O) the (O) main (O) barrier (O) to (O) progress (O) towards (O) natural (O) [TTS (B)] lies (O) with (O) [duration (B) and (I) fundamental (I) frequency (I) prediction (I)], and (O) our (O) systems (O) have (O) not (O) meaningfully (O) progressed (O) past (O) the (O) state (O) of (O) the (O) art (O) in (O) that (O) regard. (O) 
Finally, (O) our (O) best (O) models (O) run (O) slightly (O) slower (O) than (O) real-time (O) (see (O) Table (O) 2), (O) so (O) we (O) demonstrate (O) that (O) synthesis (O) quality (O) can (O) be (O) traded (O) for (O) inference (O) speed (O) by (O) adjusting (O) model (O) size (O) by (O) obtaining (O) scores (O) for (O) models (O) that (O) run (O) 1X (O) and (O) 2X (O) faster (O) than (O) real-time. (O) 
We (O) also (O) tested (O) [WaveNet (B) models (I)] trained (O) on (O) the (O) full (O) set (O) of (O) features (O) from (O) the (O) original (O) [WaveNet (B)] publication, (O) but (O) found (O) no (O) perceptual (O) difference (O) between (O) those (O) models (O) and (O) models (O) trained (O) on (O) our (O) reduced (O) [feature (B) set (I)]. 

Blizzard (O) Results (O) 

To (O) demonstrate (O) the (O) flexibility (O) of (O) our (O) system, (O) we (O) retrained (O) all (O) of (O) our (O) models (O) with (O) identical (O) [hyperparameters (B)] on (O) the (O) Blizzard (O) 2013 (O) dataset (O) (Prahallad (O) et (O) al., (O) 2013). (O) 
For (O) our (O) experiments, (O) we (O) used (O) a (O) 20.5 (O) hour (O) subset (O) of (O) the (O) dataset (O) segmented (O) into (O) 9,741 (O) utterances. (O) 
We (O) evaluated (O) the (O) model (O) using (O) the (O) procedure (O) described (O) in (O) Section (O) 4.4, (O) which (O) encourages (O) raters (O) to (O) compare (O) synthesized (O) [audio (B)] directly (O) with (O) the (O) 
ground (O) truth. (O) 
On (O) the (O) held (O) out (O) set, (O) 16 (O) kHz (O) companded (O) and (O) expanded (O) [audio (B)] receives (O) a (O) [MOS (B) score (I)] of (O) 4.65±0.13, (O) while (O) our (O) synthesized (O) [audio (B)] received (O) a (O) [MOS (B) score (I)] of (O) 2.67±0.37. (O) 

Optimizing (O) Inference (O) 

Although (O) [WaveNet (B)] has (O) shown (O) promise (O) in (O) generating (O) highquality (O) [synthesized (B) speech (I)], initial (O) experiments (O) reported (O) generation (O) times (O) of (O) many (O) minutes (O) or (O) hours (O) for (O) short (O) utterances. (O) 
[WaveNet (B) inference (I)] poses (O) an (O) incredibly (O) challenging (O) computational (O) problem (O) due (O) to (O) the (O) high-frequency, (O) autoregressive (O) nature (O) of (O) the (O) model, (O) which (O) requires (O) orders (O) of (O) magnitude (O) more (O) timesteps (O) than (O) traditional (O) [recurrent (B) neural (I) networks (I)]. 
When (O) generating (O) [audio (B)], a (O) single (O) sample (O) must (O) be (O) generated (O) in (O) approximately (O) 60 (O) μs (O) (for (O) 16 (O) kHz (O) [audio (B)]) or (O) 20 (O) μs (O) (for (O) 48 (O) kHz (O) [audio (B)]). 
For (O) our (O) 40 (O) [layer (B) models (I)], this (O) means (O) that (O) a (O) single (O) layer (O) (consisting (O) of (O) several (O) matrix (O) multiplies (O) and (O) nonlinearities) (O) must (O) complete (O) in (O) approximately (O) 1. (O) 
5 (O) μs. (O) 
For (O) comparison, (O) accessing (O) a (O) value (O) that (O) resides (O) in (O) main (O) memory (O) on (O) a (O) [CPU (B)] can (O) take (O) 0.1 (O) μs. (O) 
In (O) order (O) to (O) perform (O) inference (O) at (O) real-time, (O) we (O) must (O) take (O) great (O) care (O) to (O) never (O) recompute (O) any (O) results, (O) store (O) the (O) entire (O) model (O) in (O) the (O) processor (O) cache (O) (as (O) opposed (O) to (O) main (O) memory), (O) and (O) optimally (O) utilize (O) the (O) available (O) computational (O) units. (O) 
These (O) same (O) techniques (O) could (O) be (O) used (O) to (O) accelerate (O) image (O) synthesis (O) with (O) PixelCNN (O) (Oord (O) et (O) al., (O) 2016) (O) to (O) fractions (O) of (O) a (O) second (O) per (O) image. (O) 
Synthesizing (O) one (O) second (O) of (O) [audio (B)] with (O) our (O) 40 (O) layer (O) [WaveNet (B) model (I)] takes (O) approximately (O) 55×10 (O) 9 (O) floating (O) point (O) operations (O) ([FLOPs (B)]). 
The (O) activations (O) in (O) any (O) given (O) layer (O) depend (O) on (O) the (O) activations (O) in (O) the (O) previous (O) layer (O) and (O) the (O) previous (O) timestep, (O) so (O) inference (O) must (O) be (O) done (O) one (O) timestep (O) and (O) one (O) layer (O) at (O) a (O) time. (O) 
A (O) single (O) layer (O) requires (O) only (O) 42 (O) × (O) 10 (O) 3 (O) [FLOPs (B)], which (O) makes (O) achieving (O) meaningful (O) parallelism (O) difficult. (O) 
In (O) addition (O) to (O) the (O) compute (O) requirements, (O) the (O) model (O) has (O) approximately (O) 1.6 (O) × (O) 10 (O) 6 (O) parameters, (O) which (O) equate (O) to (O) about (O) 6.4 (O) MB (O) if (O) represented (O) in (O) single (O) precision. (O) 
(See (O) Appendix (O) E (O) for (O) a (O) complete (O) performance (O) model.) (O) 
On (O) [CPU (B)], a (O) single (O) Haswell (O) or (O) Broadwell (O) core (O) has (O) a (O) peak (O) single-precision (O) throughput (O) of (O) approximately (O) 77 (O) × (O) 10 (O) 9 (O) [FLOPs (B)] and (O) an (O) L2-to-L1 (O) cache (O) bandwidth (O) of (O) approximately (O) 140 (O) GB (O) / s (O) (1). (O) 
The (O) model (O) must (O) be (O) loaded (O) from (O) cache (O) once (O) per (O) timestep, (O) which (O) requires (O) a (O) bandwidth (O) of (O) 100 (O) GB (O) / s. (O) 
Even (O) if (O) the (O) model (O) were (O) to (O) fit (O) in (O) L2 (O) cache, (O) the (O) implementation (O) would (O) need (O) to (O) utilize (O) 70 (O) % of (O) the (O) maximum (O) bandwidth (O) and (O) 70 (O) % of (O) the (O) peak (O) [FLOPS (B)] in (O) order (O) to (O) do (O) inference (O) in (O) realtime (O) on (O) a (O) single (O) core. (O) 
Splitting (O) the (O) calculations (O) across (O) multiple (O) cores (O) reduces (O) the (O) difficulty (O) of (O) the (O) problem, (O) but (O) nonetheless (O) it (O) remains (O) challenging (O) as (O) inference (O) must (O) operate (O) at (O) a (O) significant (O) fraction (O) of (O) maximum (O) memory (O) bandwidth (O) and (O) peak (O) [FLOPs (B)] and (O) while (O) keeping (O) threads (O) synchronized. (O) 
A (O) [GPU (B)] has (O) higher (O) memory (O) bandwidth (O) and (O) peak (O) [FLOPs (B)] than (O) a (O) [CPU (B)] but (O) provides (O) a (O) more (O) specialized (O) and (O) hence (O) restrictive (O) computational (O) model. (O) 
A (O) naive (O) implementation (O) that (O) launches (O) a (O) single (O) kernel (O) for (O) every (O) layer (O) or (O) timestep (O) is (O) untenable, (O) but (O) an (O) implementation (O) based (O) on (O) the (O) persistent (O) [RNN (B) technique (I)] (Diamos (O) et (O) al., (O) 2016) (O) may (O) be (O) able (O) to (O) take (O) advantage (O) of (O) the (O) throughput (O) offered (O) by (O) [GPUs (B)]. 
We (O) implement (O) high-speed (O) optimized (O) inference (O) kernels (O) for (O) both (O) [CPU (B)] and (O) [GPU (B)] and (O) demonstrate (O) that (O) [WaveNet (B) inference (I)] at (O) faster-than-real-time (O) speeds (O) is (O) achievable. (O) 
Table (O) 2 (O) lists (O) the (O) [CPU (B)] and (O) [GPU (B) inference (I)] speeds (O) for (O) different (O) models. (O) 
In (O) both (O) cases, (O) the (O) benchmarks (O) include (O) only (O) the (O) autoregressive, (O) high-frequency (O) [audio (B) generation (I)] and (O) do (O) not (O) include (O) the (O) generation (O) of (O) linguistic (O) [conditioning (B) features (I)] (which (O) can (O) be (O) done (O) in (O) parallel (O) for (O) the (O) entire (O) utterance). (O) 
Our (O) [CPU (B) kernels (I)] run (O) at (O) real-time (O) or (O) faster-than-real-time (O) for (O) a (O) subset (O) of (O) models, (O) while (O) the (O) [GPU (B) models (I)] do (O) not (O) yet (O) match (O) this (O) performance. (O) 

Assuming (O) two (O) 8-wide (O) AVX (O) FMA (O) instructions (O) every (O) cycle (O) and (O) an (O) L2-to-L1 (O) bandwidth (O) of (O) 64 (O) [bytes (B)] per (O) cycle. (O) 


Table. (O) [Mean (B) Opinion (I) Scores (I)] ([MOS (B)]) and (O) 95 (O) % confidence (O) intervals (O) (CIs) (O) for (O) utterances. (O) 
This (O) [MOS (B) score (I)] is (O) a (O) relative (O) [MOS (B) score (I)] obtained (O) by (O) showing (O) raters (O) the (O) same (O) utterance (O) across (O) all (O) the (O) model (O) types (O) (which (O) encourages (O) comparative (O) rating (O) and (O) allows (O) the (O) raters (O) to (O) distinguish (O) finer (O) grained (O) differences). (O) 
Every (O) batch (O) of (O) samples (O) also (O) includes (O) the (O) ground (O) truth (O) 48 (O) kHz (O) recording, (O) which (O) makes (O) all (O) our (O) ratings (O) comparative (O) to (O) natural (O) [human (B) voices (I)]. 474 (O) ratings (O) were (O) collected (O) for (O) every (O) sample. (O) 
Unless (O) otherwise (O) mentioned, (O) models (O) used (O) [phoneme (B) durations (I)] and (O) F0 (O) extracted (O) from (O) the (O) ground (O) truth, (O) rather (O) than (O) synthesized (O) by (O) the (O) duration (O) prediction (O) and (O) [frequency (B) prediction (I) models (I)], as (O) well (O) as (O) a (O) 16384 (O) Hz (O) [audio (B) sampling (I) rate (I)]. 

Table. (O) [CPU (B)] and (O) [GPU (B) inference (I)] kernel (O) benchmarks (O) for (O) different (O) models (O) in (O) float32 (O) and (O) int16. (O) 
At (O) least (O) one (O) main (O) and (O) one (O) auxiliary (O) thread (O) were (O) used (O) for (O) all (O) [CPU (B) kernels (I)]. These (O) kernels (O) operate (O) on (O) a (O) single (O) utterance (O) with (O) no (O) batching. (O) 
[CPU (B) results (I)] are (O) from (O) a (O) Intel (O) Xeon (O) E5-2660 (O) v3 (O) Haswell (O) processor (O) clocked (O) at (O) 2.6 (O) GHz (O) and (O) [GPU (B) results (I)] are (O) from (O) a (O) GeForce (O) GTX (O) Titan (O) X (O) Maxwell (O) [GPU (B)]. 

[CPU (B) Implementation (I)] 

We (O) achieve (O) real-time (O) [CPU (B) inference (I)] by (O) avoiding (O) any (O) recomputation, (O) doing (O) cache-friendly (O) memory (O) accesses, (O) parallelizing (O) work (O) via (O) multithreading (O) with (O) efficient (O) synchronization, (O) minimizing (O) nonlinearity (O) [FLOPs (B)], avoiding (O) cache (O) thrashing (O) and (O) thread (O) contention (O) via (O) thread (O) pinning, (O) and (O) using (O) custom (O) hardware-optimized (O) routines (O) for (O) matrix (O) multiplication (O) and (O) convolution. (O) 
For (O) the (O) [CPU (B) implementation (I)], we (O) split (O) the (O) computation (O) into (O) the (O) following (O) steps (O) : 

Sample (O) Embedding (O) : Compute (O) the (O) [WaveNet (B) input (I)] causal (O) convolution (O) by (O) doing (O) two (O) sample (O) embeddings, (O) one (O) for (O) the (O) current (O) timestep (O) and (O) one (O) for (O) the (O) previous (O)   timestep, (O) and (O) summing (O) them (O) with (O) a (O) bias. (O) That (O) is, (O) 

Layer (O) Inference (O) : For (O) every (O) layer (O) j (O) from (O) j (O) = 1 (O) to (O) ` with (O) dilation (O) width (O) d (O) : 
(a) (O) Compute (O) the (O) left (O) half (O) of (O) the (O) width-two (O) dilated (O) convolution (O) via (O) a (O) [matrix-vector (B) multiply (I)] : 
(b) (O) Compute (O) the (O) right (O) half (O) of (O) the (O) dilated (O) convolution (O) : 
(c) (O) Compute (O) the (O) hidden (O) state (O) h (O) (j) (O) given (O) the (O) conditioning (O) [vector (B)] L (O) h (O) : 
where (O) v (O) 0 (O) : r (O) denotes (O) the (O) first (O) r (O) elements (O) of (O) the (O) [vector (B)] v (O) and (O) v (O) r:2r (O) denotes (O) the (O) next (O) r (O) elements. (O) Then, (O) compute (O) the (O) input (O) to (O) the (O) next (O) layer (O) via (O) a (O) [matrix-vector (B) multiply (I)] : 
(d) (O) Compute (O) the (O) contribution (O) to (O) the (O) skip-channel (O) matrix (O) multiply (O) from (O) this (O) layer, (O) accumulating (O) over (O) all (O) layers, (O) with (O) q (O) (0) (O) = B (O) skip (O) : 

Output (O) : Compute (O) the (O) two (O) output (O) 1 (O) × (O) 1 (O) convolutions (O) : 
Finally, (O) sample (O) y (O) i+1 (O) randomly (O) from (O) the (O) distribution (O) p. (O) 

We (O) parallelize (O) these (O) across (O) two (O) groups (O) of (O) threads (O) as (O) depicted (O) in (O) Figure. (O) 
A (O) group (O) of (O) main (O) threads (O) computes (O) (j) (O) x (O) (0), (O) a (O) cur, (O) h (O) (j), (O) and (O) x (O) (j), (O) z (O) a, (O) and (O) p. (O) 
A (O) group (O) of (O) auxiliary (O) (j) (O) (j) (O) threads (O) computes (O) a (O) prev, (O) q (O) (j), (O) and (O) z (O) s, (O) with (O) the (O) a (O) prev (O) being (O) computed (O) for (O) the (O) next (O) upcoming (O) timestep (O) while (O) the (O) main (O) threads (O) compute (O) z (O) a (O) and (O) p. (O) 
Each (O) of (O) these (O) groups (O) can (O) consist (O) of (O) a (O) single (O) thread (O) or (O) of (O) multiple (O) threads (O) ; if (O) there (O) are (O) multiple (O) threads, (O) each (O) thread (O) computes (O) one (O) block (O) of (O) each (O) [matrix-vector (B) multiply (I)], binary (O) operation, (O) or (O) unary (O) operation, (O) and (O) thread (O) barriers (O) are (O) inserted (O) as (O) needed. (O) 
Splitting (O) the (O) model (O) across (O) multiple (O) threads (O) both (O) splits (O) up (O) the (O) compute (O) and (O) can (O) also (O) be (O) used (O) to (O) ensure (O) that (O) the (O) model (O) weights (O) fit (O) into (O) the (O) processor (O) L2 (O) cache. (O) 
Pinning (O) threads (O) to (O) physical (O) cores (O) (or (O) disabling (O) hyperthreading) (O) is (O) important (O) for (O) avoiding (O) thread (O) contention (O) and (O) cache (O) thrashing (O) and (O) increases (O) performance (O) by (O) approximately (O) 30 (O) %. (O) 
Depending (O) on (O) model (O) size, (O) the (O) nonlinearities (O) ([tanh (B)], [sigmoid (B)], and (O) [softmax (B)]) can (O) also (O) take (O) a (O) significant (O) fraction (O) of (O) inference (O) time, (O) so (O) we (O) replace (O) all (O) nonlinearities (O) with (O) high-accuracy (O) approximations, (O) which (O) are (O) detailed (O) in (O) Appendix (O) C. (O) 
The (O) maximum (O) absolute (O) error (O) arising (O) from (O) these (O) approximations (O) is (O) 1.5 (O) × (O) 10 (O) −3 (O) for (O) [tanh (B)], 2.5 (O) × (O) 10 (O) −3 (O) for (O) [sigmoid (B)], and (O) 2.4 (O) × (O) 10 (O) −5 (O) for (O) e (O) x. (O) 
With (O) approximate (O) instead (O) of (O) exact (O) nonlinearities, (O) performance (O) increases (O) by (O) roughly (O) 30 (O) %. (O) 
We (O) also (O) implement (O) inference (O) with (O) weight (O) matrices (O) quantized (O) to (O) int16 (O) and (O) find (O) no (O) change (O) in (O) perceptual (O) quality (O) when (O) using (O) quantization. (O) 
For (O) larger (O) models, (O) quantization (O) offers (O) a (O) significant (O) speedup (O) when (O) using (O) fewer (O) threads, (O) but (O) overhead (O) of (O) thread (O) synchronization (O) prevents (O) it (O) from (O) being (O) useful (O) with (O) a (O) larger (O) number (O) of (O) threads. (O) 
Finally, (O) we (O) write (O) custom (O) AVX (O) assembly (O) kernels (O) for (O) matrix (O) [vector (B)] multiplication (O) using (O) PeachPy (O) (Dukhan, (O) 2015) (O) specialized (O) to (O) our (O) matrix (O) sizes. (O) 
Inference (O) using (O) our (O) custom (O) assembly (O) kernels (O) is (O) up (O) to (O) 1.5X (O) faster (O) than (O) Intel (O) MKL (O) and (O) 3.5X (O) faster (O) than (O) OpenBLAS (O) when (O) using (O) float32. (O) 
Neither (O) library (O) provides (O) the (O) equivalent (O) int16 (O) operations. (O) 

[GPU (B) Implementation (I)] 

Due (O) to (O) their (O) computational (O) intensity, (O) many (O) [neural (B) models (I)] are (O) ultimately (O) deployed (O) on (O) [GPUs (B)], which (O) can (O) have (O) a (O) much (O) higher (O) computational (O) throughput (O) than (O) CPUs. (O) 
Since (O) our (O) model (O) is (O) memory (O) bandwidth (O) and (O) [FLOP (B)] bound, (O) it (O) may (O) seem (O) like (O) a (O) natural (O) choice (O) to (O) run (O) inference (O) on (O) a (O) [GPU (B)], but (O) it (O) turns (O) out (O) that (O) comes (O) with (O) a (O) different (O) set (O) of (O) challenges. (O) 
Usually, (O) code (O) is (O) run (O) on (O) the (O) [GPU (B)] in (O) a (O) sequence (O) of (O) kernel (O) invocations, (O) with (O) every (O) matrix (O) multiply (O) or (O) [vector (B)] operation (O) being (O) its (O) own (O) kernel. (O) 
However, (O) the (O) latency (O) for (O) a (O) [CUDA (B) kernel (I)] launch (O) (which (O) may (O) be (O) up (O) to (O) 50 (O) μs) (O) combined (O) with (O) the (O) time (O) needed (O) to (O) load (O) the (O) entire (O) model (O) from (O) [GPU (B)] memory (O) are (O) prohibitively (O) large (O) for (O) an (O) approach (O) like (O) this. (O) 
An (O) inference (O) kernel (O) in (O) this (O) style (O) ends (O) up (O) being (O) approximately (O) 1000X (O) slower (O) than (O) real-time. (O) 
To (O) get (O) close (O) to (O) real-time (O) on (O) a (O) [GPU (B)], we (O) instead (O) build (O) a (O) kernel (O) using (O) the (O) techniques (O) of (O) persistent (O) [RNNs (B)] (Diamos (O) et (O) al., (O) 2016) (O) which (O) generates (O) all (O) samples (O) in (O) the (O) output (O) [audio (B)] in (O) a (O) single (O) kernel (O) launch. (O) 
The (O) weights (O) for (O) the (O) model (O) are (O) loaded (O) to (O) registers (O) once (O) and (O) then (O) used (O) without (O) unloading (O) them (O) for (O) the (O) entire (O) duration (O) of (O) inference. (O) 
Due (O) to (O) the (O) mismatch (O) between (O) the (O) [CUDA (B)] programming (O) model (O) and (O) such (O) persistent (O) kernels, (O) the (O) resulting (O) kernels (O) are (O) specialized (O) to (O) particular (O) model (O) sizes (O) and (O) are (O) incredibly (O) labor-intensive (O) to (O) write. (O) 
Although (O) our (O) [GPU (B) inference (I)] speeds (O) are (O) not (O) quite (O) real-time (O) (Table (O) 2), (O) we (O) believe (O) that (O) with (O) these (O) techniques (O) and (O) a (O) better (O) implementation (O) we (O) can (O) achieve (O) real-time (O) [WaveNet (B) inference (I)] on (O) [GPUs (B)] as (O) well (O) as (O) CPUs. (O) 
Implementation (O) details (O) for (O) the (O) persistent (O) [GPU (B) kernels (I)] are (O) available (O) in (O) Appendix (O) D. (O) 

Conclusion (O) 

In (O) this (O) work, (O) we (O) demonstrate (O) that (O) current (O) [Deep (B) Learning (I) approaches (I)] are (O) viable (O) for (O) all (O) the (O) components (O) of (O) a (O) highquality (O) [text-to-speech (B) engine (I)] by (O) building (O) a (O) fully (O) neural (O) system. (O) 
We (O) optimize (O) inference (O) to (O) faster-than-real-time (O) speeds, (O) showing (O) that (O) these (O) techniques (O) can (O) be (O) applied (O) to (O) generate (O) [audio (B)] in (O) real-time (O) in (O) a (O) streaming (O) fashion. (O) 
Our (O) system (O) is (O) trainable (O) without (O) any (O) human (O) involvement, (O) dramatically (O) simplifying (O) the (O) process (O) of (O) creating (O) [TTS (B) systems (I)]. 
Our (O) work (O) opens (O) many (O) new (O) possible (O) directions (O) for (O) exploration. (O) 
Inference (O) performance (O) can (O) be (O) further (O) improved (O) through (O) careful (O) optimization, (O) model (O) quantization (O) on (O) [GPU (B)], and (O) int8 (O) quantization (O) on (O) [CPU (B)], as (O) well (O) as (O) experimenting (O) with (O) other (O) architectures (O) such (O) as (O) the (O) Xeon (O) Phi. (O) 
Another (O) natural (O) direction (O) is (O) removing (O) the (O) separation (O) between (O) stages (O) and (O) merging (O) the (O) segmentation, (O) duration (O) prediction, (O) and (O) [fundamental (B) frequency (I) prediction (I) models (I)] directly (O) into (O) the (O) [audio (B) synthesis (I) model (I)], thereby (O) turning (O) the (O) problem (O) into (O) a (O) full (O) [sequence-to-sequence (B) model (I)], creating (O) a (O) single (O) [end-to-end (B) trainable (I) TTS (I) system (I)], and (O) allowing (O) us (O) to (O) train (O) the (O) entire (O) system (O) with (O) no (O) intermediate (O) supervision. (O) 
In (O) lieu (O) of (O) fusing (O) the (O) models, (O) improving (O) the (O) duration (O) and (O) frequency (O) models (O) via (O) larger (O) [training (B) datasets (I)] or (O) generative (O) modeling (O) techniques (O) may (O) have (O) an (O) impact (O) on (O) [voice (B) naturalness (I)]. 

