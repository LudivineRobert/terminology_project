Transfer (O) Learning (O) from (O) [Speaker (B) Verification (I)] to (O) [Multispeaker (B) Text-To-Speech (I) Synthesis (I)] 


Abstract (O) 

We (O) describe (O) a (O) [neural (B) network-based (I) system (I)] for (O) [text-to-speech (B) (TTS) (I) synthesis (I)] that (O) is (O) able (O) to (O) generate (O) [speech (B) audio (I)] in (O) the (O) voice (O) of (O) [different (B) speakers (I)], including (O) those (O) unseen (O) during (O) training. (O) 
Our (O) system (O) consists (O) of (O) three (O) independently (O) trained (O) components (O) : a (O) [speaker (B) encoder (I) network (I)], trained (O) on (O) a (O) [speaker (B) verification (I)] task (O) using (O) an (O) [independent (B) dataset (I)] of (O) noisy (O) [speech (B)] without (O) transcripts (O) from (O) thousands (O) of (O) speakers, (O) to (O) generate (O) a (O) fixed-dimensional (O) embedding (O) [vector (B)] from (O) only (O) seconds (O) of (O) reference (O) [speech (B)] from (O) a (O) [target (B) speaker (I)] ; a (O) [sequence-to-sequence (B) synthesis (I) network (I)] based (O) on (O) [Tacotron (B) 2 (I)] that (O) generates (O) a (O) [mel (B) spectrogram (I)] from (O) text, (O) conditioned (O) on (O) the (O) [speaker (B) embedding (I)] ; an (O) auto-regressive (O) [WaveNet-based (B) vocoder (I) network (I)] that (O) converts (O) the (O) [mel (B) spectrogram (I)] into (O) time (O) domain (O) [waveform (B) samples (I)]. 
We (O) demonstrate (O) that (O) the (O) proposed (O) model (O) is (O) able (O) to (O) transfer (O) the (O) knowledge (O) of (O) [speaker (B) variability (I)] learned (O) by (O) the (O) discriminatively-trained (O) [speaker (B) encoder (I)] to (O) the (O) [multispeaker (B) TTS (I) task (I)], and (O) is (O) able (O) to (O) synthesize (O) natural (O) [speech (B)] from (O) speakers (O) unseen (O) during (O) training. (O) 
We (O) quantify (O) the (O) importance (O) of (O) training (O) the (O) [speaker (B) encoder (I)] on (O) a (O) large (O) and (O) [diverse (B) speaker (I)] set (O) in (O) order (O) to (O) obtain (O) the (O) best (O) generalization (O) performance. (O) 
Finally, (O) we (O) show (O) that (O) randomly (O) sampled (O) [speaker (B) embeddings (I)] can (O) be (O) used (O) to (O) [synthesize (B) speech (I)] in (O) the (O) voice (O) of (O) [novel (B) speakers (I)] dissimilar (O) from (O) those (O) used (O) in (O) training, (O) indicating (O) that (O) the (O) model (O) has (O) learned (O) a (O) high (O) [quality (B) speaker (I)] representation. (O) 

Introduction (O) 

The (O) goal (O) of (O) this (O) work (O) is (O) to (O) build (O) a (O) [TTS (B) system (I)] which (O) can (O) generate (O) natural (O) [speech (B)] for (O) a (O) variety (O) of (O) speakers (O) in (O) a (O) data (O) efficient (O) manner. (O) 
We (O) specifically (O) address (O) a (O) zero-shot (O) learning (O) setting, (O) where (O) a (O) few (O) seconds (O) of (O) untranscribed (O) reference (O) [audio (B)] from (O) a (O) [target (B) speaker (I)] is (O) used (O) to (O) synthesize (O) new (O) [speech (B)] in (O) that (O) [speaker’s (B) voice (I)], without (O) updating (O) any (O) model (O) parameters. (O) 
Such (O) systems (O) have (O) accessibility (O) applications, (O) such (O) as (O) restoring (O) the (O) ability (O) to (O) communicate (O) naturally (O) to (O) users (O) who (O) have (O) lost (O) their (O) voice (O) and (O) are (O) therefore (O) unable (O) to (O) provide (O) many (O) new (O) training (O) examples. (O) 
They (O) could (O) also (O) enable (O) new (O) applications, (O) such (O) as (O) transferring (O) a (O) voice (O) across (O) languages (O) for (O) more (O) natural (O) [speech-to-speech (B) translation (I)], or (O) generating (O) realistic (O) [speech (B)] from (O) text (O) in (O) low (O) resource (O) settings. (O) 
However, (O) it (O) is (O) also (O) important (O) to (O) note (O) the (O) potential (O) for (O) misuse (O) of (O) this (O) technology, (O) for (O) example (O) impersonating (O) someone’s (O) voice (O) without (O) their (O) consent. (O) 
In (O) order (O) to (O) address (O) safety (O) concerns (O) consistent (O) with (O) principles (O) such (O) as, (O) we (O) verify (O) that (O) voices (O) generated (O) by (O) the (O) proposed (O) model (O) can (O) easily (O) be (O) distinguished (O) from (O) [real (B) voices (I)]. 
Synthesizing (O) natural (O) [speech (B)] requires (O) training (O) on (O) a (O) large (O) number (O) of (O) [high (B) quality (I) speech (I)]-transcript pairs, (O) and (O) supporting (O) [many (B) speakers (I)] usually (O) uses (O) tens (O) of (O) minutes (O) of (O) [training (B) data (I)] per (O) speaker. (O) 
Recording (O) a (O) large (O) amount (O) of (O) high (O) [quality (B) data (I)] for (O) [many (B) speakers (I)] is (O) impractical. (O) 
Our (O) approach (O) is (O) to (O) [decouple (B) speaker (I)] modeling (O) from (O) [speech (B) synthesis (I)] by (O) independently (O) training (O) a (O) speaker-discriminative (O) embedding (O) network (O) that (O) captures (O) the (O) space (O) of (O) [speaker (B) characteristics (I)] and (O) training (O) a (O) high (O) quality (O) [TTS (B) model (I)] on (O) a (O) [smaller (B) dataset (I)] conditioned (O) on (O) the (O) representation (O) learned (O) by (O) the (O) first (O) network. (O) 
Decoupling (O) the (O) networks (O) enables (O) them (O) to (O) be (O) trained (O) on (O) [independent (B) data (I)], which (O) reduces (O) the (O) need (O) to (O) obtain (O) high (O) quality (O) [multispeaker (B) training (I) data (I)]. 
We (O) train (O) the (O) speaker (O) embedding (O) network (O) on (O) a (O) [speaker (B) verification (I)] task (O) to (O) determine (O) if (O) two (O) different (O) utterances (O) were (O) spoken (O) by (O) the (O) [same (B) speaker (I)]. 
In (O) contrast (O) to (O) the (O) subsequent (O) [TTS (B) model (I)], this (O) network (O) is (O) trained (O) on (O) [untranscribed (B) speech (I)] containing (O) reverberation (O) and (O) background (O) noise (O) from (O) a (O) large (O) number (O) of (O) speakers. (O) 
We (O) demonstrate (O) that (O) the (O) [speaker (B) encoder (I)] and (O) synthesis (O) networks (O) can (O) be (O) trained (O) on (O) unbalanced (O) and (O) disjoint (O) sets (O) of (O) speakers (O) and (O) still (O) generalize (O) well. (O) 
We (O) train (O) the (O) synthesis (O) network (O) on. (O) 
2000 (O) speakers (O) and (O) show (O) that (O) training (O) the (O) [encoder (B)] on (O) a (O) much (O) larger (O) set (O) of (O) 18000 (O) speakers (O) improves (O) adaptation (O) quality, (O) and (O) further (O) enables (O) synthesis (O) of (O) completely (O) [novel (B) speakers (I)] by (O) sampling (O) from (O) the (O) embedding (O) prior. (O) 
There (O) has (O) been (O) significant (O) interest (O) in (O) [end-to-end (B) training (I)] of (O) [TTS (B) models (I)], which (O) are (O) trained (O) directly (O) from (O) text-[audio (B) pairs (I)], without (O) depending (O) on (O) hand (O) crafted (O) intermediate (O) representations. (O) 
[Tacotron (B) 2 (I)] used (O) [WaveNet (B)] as (O) a (O) [vocoder (B)] to (O) invert (O) [spectrograms (B)] generated (O) by (O) an (O) [encoder-decoder (B) architecture (I)] with (O) attention, (O) obtaining (O) naturalness (O) approaching (O) that (O) of (O) human (O) [speech (B)] by (O) combining (O) Tacotron’sprosody (O) with (O) [WaveNet (B)]’s [audio (B) quality (I)]. 
It (O) only (O) supported (O) a (O) [single (B) speaker (I)]. 
Gibiansky (O) et (O) al. (O) ntroduced (O) a (O) multispeaker (O) variation (O) of (O) [Tacotron (B)] which (O) learned (O) [low-dimensional (B) speaker (I)] embedding (O) for (O) each (O) [training (B) speaker (I)]. 
[Deep (B) Voice (I)] 3proposed (O) a (O) fully (O) [convolutional (B) encoder-decoder (I) architecture (I)] which (O) scaled (O) up (O) to (O) support (O) over (O) 2,400 (O) speakers (O) from (O) [LibriSpeech (B)]. 
These (O) systems (O) learn (O) a (O) fixed (O) set (O) of (O) [speaker (B) embeddings (I)] and (O) therefore (O) only (O) support (O) synthesis (O) of (O) voices (O) seen (O) during (O) training. (O) 
In (O) contrast, (O) VoiceLoop (O) proposed (O) a (O) novel (O) architecture (O) based (O) on (O) a (O) fixed (O) size (O) memory (O) buffer (O) which (O) can (O) generate (O) [speech (B)] from (O) voices (O) unseen (O) during (O) training. (O) 
Obtaining (O) good (O) results (O) required (O) tens (O) of (O) minutes (O) of (O) enrollment (O) [speech (B)] and (O) transcripts (O) for (O) a (O) [new (B) speaker (I)]. 
Recent (O) extensions (O) have (O) enabled (O) few-[shot (B) speaker (I)] adaptation (O) where (O) only (O) a (O) few (O) seconds (O) of (O) [speech (B)] per (O) speaker (O) (without (O) transcripts) (O) can (O) be (O) used (O) to (O) generate (O) new (O) [speech (B)] in (O) that (O) speaker’s (O) voice. (O) 
extends (O) [Deep (B) Voice (I)] 3, (O) comparing (O) a (O) [speaker (B) adaptation (I)] method (O) similar (O) to (O) where (O) the (O) model (O) parameters (O) (including (O) [speaker (B) embedding (I)]) are (O) fine-tuned (O) on (O) a (O) small (O) amount (O) of (O) [adaptation (B) data (I)] to (O) a (O) [speaker (B) encoding (I)] method (O) which (O) uses (O) a (O) [neural (B) network (I)] to (O) predict (O) speaker (O) embedding (O) directly (O) from (O) a (O) [spectrogram (B)]. 
The (O) latter (O) approach (O) is (O) significantly (O) [more (B) data (I)] efficient, (O) obtaining (O) higher (O) naturalness (O) using (O) small (O) amounts (O) of (O) [adaptation (B) data (I)], in (O) as (O) few (O) as (O) one (O) or (O) two (O) utterances. (O) 
It (O) is (O) also (O) significantly (O) more (O) computationally (O) efficient (O) since (O) it (O) does (O) not (O) require (O) hundreds (O) of (O) backpropagation (O) iterations. (O) 
Nachmani (O) et (O) al. (O)   similarly (O) extended (O) VoiceLoop (O) to (O) utilize (O) a (O) [target (B) speaker (I)] encoding (O) network (O) to (O) predict (O) a (O) [speaker (B) embedding (I)]. 
This (O) network (O) is (O) trained (O) jointly (O) with (O) the (O) synthesis (O) network (O) using (O) a (O) contrastive (O) triplet (O) loss (O) to (O) ensure (O) that (O) embeddings (O) predicted (O) from (O) utterances (O) by (O) the (O) [same (B) speaker (I)] are (O) closer (O) than (O) embeddings (O) computed (O) from (O) [different (B) speakers (I)]. 
In (O) addition, (O) a (O) cycle-consistency (O) loss (O) is (O) used (O) to (O) ensure (O) that (O) the (O) [synthesized (B) speech (I)] encodes (O) to (O) a (O) similar (O) embedding (O) as (O) the (O) adaptation (O) utterance. (O) 
A (O) similar (O) [spectrogram (B) encoder (I) network (I)], trained (O) without (O) a (O) triplet (O) loss, (O) was (O) shown (O) to (O) work (O) for (O) transferring (O) [target (B) prosody (I)] to (O) [synthesized (B) speech (I)]. 
In (O) this (O) paper (O) we (O) demonstrate (O) that (O) training (O) a (O) similar (O) [encoder (B)] to (O) discriminate (O) between (O) speakers (O) leads (O) to (O) reliable (O) transfer (O) of (O) [speaker (B) characteristics (I)]. 
Our (O) work (O) is (O) most (O) similar (O) to (O) the (O) [speaker (B) encoding (I)] models (O) in, (O) except (O) that (O) we (O) utilize (O) a (O) network (O) independently-trained (O) for (O) a (O) [speaker (B) verification (I)] task (O) on (O) a (O) [large (B) dataset (I)] of (O) [untranscribed (B) audio (I)] from (O) tens (O) of (O) thousands (O) of (O) speakers, (O) using (O) a (O) state-of-the-art (O) generalized (O) [end-to-end (B) loss (I)]. 
incorporated (O) a (O) [similar (B) speaker (I)]-discriminative representation (O) into (O) their (O) model, (O) however (O) all (O) components (O) were (O) trained (O) jointly. (O) 
In (O) contrast, (O) we (O) explore (O) transfer (O) learning (O) from (O) a (O) pre-trained (O) [speaker (B) verification (I)] model. (O) 
Doddipatla (O) et (O) al. (O) used (O) a (O) similar (O) transfer (O) learning (O) configuration (O) where (O) a (O) [speaker (B) embedding (I)] computed (O) from (O) a (O) pre-trained (O) [speaker (B) classifier (I)] was (O) used (O) to (O) condition (O) a (O) [TTS (B) system (I)]. 
In (O) this (O) paper (O) we (O) utilize (O) an (O) [end-to-end (B) synthesis (I) network (I)] which (O) does (O) not (O) rely (O) on (O) intermediate (O) [linguistic (B) features (I)], and (O) a (O) substantially (O) [different (B) speaker (I) embedding (I) network (I)] which (O) is (O) not (O) limited (O) to (O) a (O) closed (O) set (O) of (O) speakers. (O) 
Furthermore, (O) we (O) analyze (O) how (O) quality (O) varies (O) with (O) the (O) number (O) of (O) speakers (O) in (O) the (O) training (O) set, (O) and (O) find (O) that (O) zero-shot (O) transfer (O) requires (O) training (O) on (O) thousands (O) of (O) speakers, (O) many (O) more (O) than (O) were (O) used (O) in. (O) 

[Multispeaker (B) speech (I) synthesis (I) model (I)] 

Our (O) system (O) is (O) composed (O) of (O) three (O) independently (O) trained (O) [neural (B) networks (I)], illustrated (O) in (O) Figure (O) 1 (O) : (1) (O) a (O) [recurrent (B) speaker (I) encoder (I)], based (O) on, (O) which (O) computes (O) a (O) fixed (O) dimensional (O) [vector (B)] from (O) a (O) [speech (B) signal (I)], (2) (O) a (O) [sequence-to-sequence (B) synthesizer (I)], based (O) on, (O) which (O) predicts (O) a (O) [mel (B) spectrogram (I)] from (O) a (O) [sequence (B) of (I) grapheme (I)] or (O) [phoneme (B)] inputs, (O) conditioned (O) on (O) the (O) speaker (O) embedding (O) [vector (B)], and (O) (3) (O) an (O) [autoregressive (B) WaveNet (I) vocoder (I)], which (O) converts (O) the (O) [spectrogram (B)] into (O) time (O) domain (O) [waveforms (B)]. 
                                                   
Figure (O) : Model (O) overview. (O) Each (O) of (O) the (O) three (O) components (O) are (O) trained (O) independently. (O) 

[Speaker (B) encoder (I)] 

The (O) [speaker (B) encoder (I)] is (O) used (O) to (O) condition (O) the (O) synthesis (O) network (O) on (O) a (O) reference (O) [speech (B) signal (I)] from (O) the (O) desired (O) [target (B) speaker (I)]. 
Critical (O) to (O) good (O) generalization (O) is (O) the (O) use (O) of (O) a (O) representation (O) which (O) captures (O) the (O) characteristics (O) of (O) [different (B) speakers (I)], and (O) the (O) ability (O) to (O) identify (O) these (O) characteristics (O) using (O) only (O) a (O) short (O) adaptation (O) signal, (O) independent (O) of (O) its (O) [phonetic (B) content (I)] and (O) background (O) noise. (O) 
These (O) requirements (O) are (O) satisfied (O) using (O) a (O) speaker-discriminative (O) model (O) trained (O) on (O) a (O) text-[independent (B) speaker (I)] verification (O) task. (O) 
We (O) follow, (O) which (O) proposed (O) a (O) highly (O) scalable (O) and (O) accurate (O) [neural (B) network (I) framework (I)] for (O) [speaker (B) verification (I)]. 
The (O) network (O) maps (O) a (O) sequence (O) of (O) [log-mel (B) spectrogram (I) frames (I)] computed (O) from (O) a (O) [speech (B)] utterance (O) of (O) arbitrary (O) length, (O) to (O) a (O) fixed-dimensional (O) embedding (O) [vector (B)], known (O) as (O) d-[vector (B)]. 
The (O) network (O) is (O) trained (O) to (O) optimize (O) a (O) generalized (O) [end-to-end (B) speaker (I) verification (I)] loss, (O) so (O) that (O) embeddings (O) of (O) utterances (O) from (O) the (O) [same (B) speaker (I)] have (O) high (O) cosine (O) similarity, (O) while (O) those (O) of (O) utterances (O) from (O) [different (B) speakers (I)] are (O) far (O) apart (O) in (O) the (O) embedding (O) space. (O) 
The (O) [training (B) dataset (I)] consists (O) of (O) [speech (B) audio (I)] examples (O) segmented (O) into (O) 1.6 (O) seconds (O) and (O) [associated (B) speaker (I)] identity (O) labels (O) ; no (O) transcripts (O) are (O) used. (O) 
Input (O) 40-channel (O) [log-mel (B) spectrograms (I)] are (O) passed (O) to (O) a (O) network (O) consisting (O) of (O) a (O) stack (O) of (O) 3 (O) [LSTM (B) layers (I)] of (O) 768 (O) cells, (O) each (O) followed (O) by (O) a (O) projection (O) to (O) 256 (O) dimensions. (O) 
The (O) final (O) embedding (O) is (O) created (O) by (O) L2 (O) -normalizing (O) the (O) output (O) of (O) the (O) top (O) layer (O) at (O) the (O) final (O) frame. (O) 
During (O) inference, (O) an (O) arbitrary (O) length (O) utterance (O) is (O) broken (O) into (O) 800ms (O) windows, (O) overlapped (O) by (O) 50 (O) %. (O) 
The (O) network (O) is (O) run (O) independently (O) on (O) each (O) window, (O) and (O) the (O) outputs (O) are (O) averaged (O) and (O) normalized (O) to (O) create (O) the (O) final (O) utterance (O) embedding. (O) 
Although (O) the (O) network (O) is (O) not (O) optimized (O) directly (O) to (O) learn (O) a (O) representation (O) which (O) captures (O) [speaker (B) characteristics (I)] relevant (O) to (O) synthesis, (O) we (O) find (O) that (O) training (O) on (O) a (O) [speaker (B) discrimination (I)] task (O) leads (O) to (O) an (O) embedding (O) which (O) is (O) directly (O) suitable (O) for (O) conditioning (O) the (O) synthesis (O) network (O) on (O) [speaker (B) identity (I)]. 

Synthesizer (O) 

We (O) extend (O) the (O) recurrent (O) [sequence-to-sequence (B)] with (O) attention (O) [Tacotron (B) 2 (I) architecture (I)] to (O) support (O) [multiple (B) speakers (I)] following (O) a (O) scheme (O) similar (O) to. (O) 
An (O) embedding (O) [vector (B)] for (O) the (O) [target (B) speaker (I)] is (O) concatenated (O) with (O) the (O) synthesizer (O) [encoder (B) output (I)] at (O) each (O) time (O) step. (O) 
In (O) contrast (O) to, (O) we (O) find (O) that (O) simply (O) passing (O) embeddings (O) to (O) the (O) [attention (B) layer (I)], as (O) in (O) Figure (O) 1, (O) converges (O) across (O) [different (B) speakers (I)]. 
We (O) compare (O) two (O) variants (O) of (O) this (O) model, (O) one (O) which (O) computes (O) the (O) embedding (O) using (O) the (O) [speaker (B) encoder (I)], and (O) a (O) baseline (O) which (O) optimizes (O) a (O) fixed (O) embedding (O) for (O) each (O) speaker (O) in (O) the (O) training (O) set, (O) essentially (O) learning (O) a (O) lookup (O) table (O) of (O) [speaker (B) embeddings (I)] similar (O) to. (O) 
The (O) synthesizer (O) is (O) trained (O) on (O) pairs (O) of (O) text (O) transcript (O) and (O) [target (B) audio (I)]. 
At (O) the (O) input, (O) we (O) map (O) the (O) text (O) to (O) a (O) [sequence (B) of (I) phonemes (I)], which (O) leads (O) to (O) faster (O) convergence (O) and (O) improved (O) pronunciation (O) of (O) rare (O) words (O) and (O) proper (O) nouns. (O) 
The (O) network (O) is (O) trained (O) in (O) a (O) transfer (O) learning (O) configuration, (O) using (O) a (O) pretrained (O) [speaker (B) encoder (I)] (whose (O) parameters (O) are (O) frozen) (O) to (O) extract (O) a (O) speaker (O) embedding (O) from (O) the (O) [target (B) audio (I)], i.e. (O) the (O) [speaker (B) reference (I)] signal (O) is (O) the (O) same (O) as (O) the (O) [target (B) speech (I)] during (O) training. (O) 
No (O) [explicit (B) speaker (I)] identifier (O) labels (O) are (O) used (O) during (O) training. (O) 
[Target (B) spectrogram (I) features (I)] are (O) computed (O) from (O) 50ms (O) windows (O) computed (O) with (O) a (O) 12.5ms (O) step, (O) passed (O) through (O) an (O) 80-channel (O) [mel-scale (B) filterbank (I)] followed (O) by (O) log (O) dynamic (O) range (O) compression. (O) 
We (O) extend (O) by (O) augmenting (O) the (O) L2 (O) loss (O) on (O) the (O) predicted (O) [spectrogram (B)] with (O) an (O) additional (O) L1 (O) loss. (O) 
In (O) practice, (O) we (O) found (O) this (O) combined (O) loss (O) to (O) be (O) more (O) robust (O) on (O) noisy (O) [training (B) data (I)]. 
In (O) contrast (O) to, (O) we (O) do (O) n’t (O) introduce (O) additional (O) loss (O) terms (O) based (O) on (O) the (O) [speaker (B) embedding (I)]. 

See (O) https://google.github.io/tacotron/publications/speaker_adaptation (O) for (O) samples. (O) 

Figure (O) : Example (O) synthesis (O) of (O) a (O) sentence (O) in (O) [different (B) voices (I)] using (O) the (O) proposed (O) system. (O) [Mel (B) spectrograms (I)] are (O) visualized (O) for (O) reference (O) utterances (O) used (O) to (O) generate (O) [speaker (B) embeddings (I)] (left), (O) and (O) the (O) corresponding (O) synthesizer (O) outputs (O) (right). (O) 
The (O) [text-to-spectrogram (B) alignment (I)] is (O) shown (O) in (O) red. (O) 
Three (O) speakers (O) held (O) out (O) of (O) the (O) train (O) sets (O) are (O) used (O) : one (O) male (O) (top) (O) and (O) two (O) female (O) (center (O) and (O) bottom). (O) 

[Neural (B) vocoder (I)] 

We (O) use (O) the (O) sample-by-sample (O) [autoregressive (B) WaveNet (I)] as (O) a (O) [vocoder (B)] to (O) invert (O) synthesized (O) [mel (B) spectrograms (I)] emitted (O) by (O) the (O) synthesis (O) network (O) into (O) time-domain (O) [waveforms (B)]. 
The (O) architecture (O) is (O) the (O) same (O) as (O) that (O) described (O) in, (O) composed (O) of (O) 30 (O) dilated (O) convolution (O) layers. (O) 
The (O) network (O) is (O) not (O) directly (O) conditioned (O) on (O) the (O) output (O) of (O) the (O) [speaker (B) encoder (I)]. 
The (O) [mel (B) spectrogram (I)] predicted (O) by (O) the (O) [synthesizer (B) network (I)] captures (O) all (O) of (O) the (O) relevant (O) detail (O) needed (O) for (O) high (O) quality (O) synthesis (O) of (O) a (O) variety (O) of (O) voices, (O) allowing (O) a (O) [multispeaker (B) vocoder (I)] to (O) be (O) constructed (O) by (O) simply (O) training (O) on (O) data (O) from (O) [many (B) speakers (I)]. 

Inference (O) and (O) zero-[shot (B) speaker (I)] adaptation (O) 

During (O) inference (O) the (O) model (O) is (O) conditioned (O) using (O) arbitrary (O) [untranscribed (B) speech (I) audio (I)], which (O) does (O) not (O) need (O) to (O) match (O) the (O) text (O) to (O) be (O) synthesized. (O) 
Since (O) the (O) speaker (O) characteristics (O) to (O) use (O) for (O) synthesis (O) are (O) inferred (O) from (O) [audio (B)], it (O) can (O) be (O) conditioned (O) on (O) [audio (B)] from (O) speakers (O) that (O) are (O) outside (O) the (O) training (O) set. (O) 
In (O) practice (O) we (O) find (O) that (O) using (O) a (O) single (O) [audio (B) clip (I)] of (O) a (O) few (O) seconds (O) duration (O) is (O) sufficient (O) to (O) synthesize (O) new (O) [speech (B)] with (O) the (O) corresponding (O) [speaker (B) characteristics (I)], representing (O) zero-shot (O) adaptation (O) to (O) [novel (B) speakers (I)]. 
In (O) Section (O) 3 (O) we (O) evaluate (O) how (O) well (O) this (O) process (O) generalizes (O) to (O) previously (O) [unseen (B) speakers (I)]. 
An (O) example (O) of (O) the (O) inference (O) process (O) is (O) visualized (O) in (O) Figure (O) 2, (O) which (O) shows (O) [spectrograms (B)] synthesized (O) using (O) several (O) different (O) 5 (O) [second (B) speaker (I)] reference (O) utterances. (O) 
Compared (O) to (O) those (O) of (O) the (O) female (O) (center (O) and (O) bottom) (O) speakers, (O) the (O) synthesized (O) male (O) (top) (O) [speaker (B) spectrogram (I)] has (O) noticeably (O) lower (O) [fundamental (B) frequency (I)], visible (O) in (O) the (O) denser (O) harmonic (O) spacing (O) (horizontal (O) stripes) (O) in (O) low (O) frequencies, (O) as (O) well (O) as (O) formants, (O) visible (O) in (O) the (O) mid-frequency (O) peaks (O) present (O) during (O) vowel (O) sounds (O) such (O) as (O) the (O) ‘ (O) i’ (O) at (O) 0.3 (O) seconds (O) – (O) the (O) top (O) male (O) F2 (O) is (O) in (O) [mel (B)] channel (O) 35, (O) whereas (O) the (O) F2 (O) of (O) the (O) [middle (B) speaker (I)] appears (O) closer (O) to (O) channel (O) 40. (O) 
Similar (O) differences (O) are (O) also (O) visible (O) in (O) sibilant (O) sounds, (O) e.g. (O) the (O) ‘s’ (O) at (O) 0.4 (O) seconds (O) contains (O) more (O) energy (O) in (O) lower (O) frequencies (O) in (O) the (O) [male (B) voice (I)] than (O) in (O) the (O) [female (B) voices (I)]. 
Finally, (O) the (O) characteristic (O) speaking (O) rate (O) is (O) also (O) captured (O) to (O) some (O) extent (O) by (O) the (O) speaker (O) embedding, (O) as (O) can (O) be (O) seen (O) by (O) the (O) longer (O) signal (O) duration (O) in (O) the (O) bottom (O) row (O) compared (O) to (O) the (O) top (O) two. (O) 
Similar (O) observations (O) can (O) be (O) made (O) about (O) the (O) corresponding (O) reference (O) utterance (O) [spectrograms (B)] in (O) the (O) right (O) column. (O) 

Table (O) : [Speech (B) naturalness (I) Mean (I) Opinion (I) Score (I)] ([MOS (B)]) with (O) 95 (O) % confidence (O) intervals. (O) 


Experiments (O) 

We (O) used (O) two (O) [public (B) datasets (I)] for (O) training (O) the (O) [speech (B) synthesis (I)] and (O) [vocoder (B) networks (I)]. 
VCTK (O) contains (O) 44 (O) hours (O) of (O) clean (O) [speech (B)] from (O) 109 (O) speakers, (O) the (O) majority (O) of (O) which (O) have (O) British (O) accents. (O) 
We (O) downsampled (O) the (O) [audio (B)] to (O) 24 (O) kHz, (O) trimmed (O) leading (O) and (O) trailing (O) silence (O) (reducing (O) the (O) median (O) duration (O) from (O) 3.3 (O) seconds (O) to (O) 1.8 (O) seconds), (O) and (O) split (O) into (O) three (O) subsets (O) : train, (O) validation (O) (containing (O) the (O) [same (B) speakers (I)] as (O) the (O) train (O) set) (O) and (O) test (O) (containing (O) 11 (O) speakers (O) held (O) out (O) from (O) the (O) train (O) and (O) validation (O) sets). (O) 
[LibriSpeech (B)] consists (O) of (O) the (O) union (O) of (O) the (O) two (O) “ (O) clean (O) ” (O) training (O) sets, (O) comprising (O) 436 (O) hours (O) of (O) [speech (B)] from (O) 1,172 (O) speakers, (O) sampled (O) at (O) 16 (O) kHz. (O) 
The (O) majority (O) of (O) [speech (B)] is (O) US (O) English, (O) however (O) since (O) it (O) is (O) sourced (O) from (O) [audio (B)] books, (O) the (O) tone (O) and (O) style (O) of (O) [speech (B)] can (O) differ (O) significantly (O) between (O) utterances (O) from (O) the (O) [same (B) speaker (I)]. 
We (O) resegmented (O) the (O) data (O) into (O) shorter (O) utterances (O) by (O) force (O) aligning (O) the (O) [audio (B)] to (O) the (O) transcript (O) using (O) an (O) [ASR (B) model (I)] and (O) breaking (O) segments (O) on (O) silence, (O) reducing (O) the (O) median (O) duration (O) from (O) 14 (O) to (O) 5 (O) seconds. (O) 
As (O) in (O) the (O) [original (B) dataset (I)], there (O) is (O) no (O) punctuation (O) in (O) transcripts. (O) 
The (O) [speaker (B) sets (I)] are (O) completely (O) disjoint (O) among (O) the (O) train, (O) validation, (O) and (O) test (O) sets. (O) 
Many (O) recordings (O) in (O) the (O) [LibriSpeech (B) clean (I) corpus (I)] contain (O) noticeable (O) environmental (O) and (O) stationary (O) background (O) noise. (O) 
We (O) preprocessed (O) the (O) [target (B) spectrogram (I)] using (O) a (O) simple (O) [spectral (B)] subtraction (O) denoising (O) procedure, (O) where (O) the (O) background (O) noise (O) [spectrum (B)] of (O) an (O) utterance (O) was (O) estimated (O) as (O) the (O) 10th (O) percentile (O) of (O) the (O) energy (O) in (O) each (O) frequency (O) band (O) across (O) the (O) full (O) signal. (O) 
This (O) process (O) was (O) only (O) used (O) on (O) the (O) synthesis (O) target (O) ; the (O) original (O) noisy (O) [speech (B)] was (O) passed (O) to (O) the (O) [speaker (B) encoder (I)]. 
We (O) trained (O) separate (O) synthesis (O) and (O) [vocoder (B) networks (I)] for (O) each (O) of (O) these (O) two (O) corpora. (O) 
Throughout (O) this (O) section, (O) we (O) used (O) synthesis (O) networks (O) trained (O) on (O) [phoneme (B)] inputs, (O) in (O) order (O) to (O) control (O) for (O) pronunciation (O) in (O) subjective (O) evaluations. (O) 
For (O) the (O) [VCTK (B) dataset (I)], whose (O) [audio (B)] is (O) quite (O) clean, (O) we (O) found (O) that (O) the (O) [vocoder (B)] trained (O) on (O) ground (O) truth (O) [mel (B) spectrograms (I)] worked (O) well. (O) 
However (O) for (O) [LibriSpeech (B)], which (O) is (O) noisier, (O) we (O) found (O) it (O) necessary (O) to (O) train (O) the (O) [vocoder (B)] on (O) [spectrograms (B)] predicted (O) by (O) the (O) [synthesizer (B) network (I)]. 
No (O) denoising (O) was (O) performed (O) on (O) the (O) [target (B) waveform (I)] for (O) [vocoder (B) training (I)]. 
The (O) [speaker (B) encoder (I)] was (O) trained (O) on (O) a (O) [proprietary (B) voice (I) search (I) corpus (I)] containing (O) 36 (O) M (O) utterances (O) with (O) median (O) duration (O) of (O) 3.9 (O) seconds (O) from (O) 18000 (O) [English (B) speakers (I)] in (O) the (O) United (O) States. (O) 
This (O) dataset (O) is (O) not (O) transcribed, (O) but (O) contains (O) anonymized (O) [speaker (B) identities (I)]. 
It (O) is (O) never (O) used (O) to (O) train (O) synthesis (O) networks. (O) 
We (O) primarily (O) rely (O) on (O) crowdsourced (O) [Mean (B) Opinion (I) Score (I) (MOS) (I) evaluations (I)] based (O) on (O) subjective (O) listening (O) tests. (O) 
All (O) our (O) [MOS (B) evaluations (I)] are (O) aligned (O) to (O) the (O) Absolute (O) Category (O) Rating (O) scale, (O) with (O) rating (O) scores (O) from (O) 1 (O) to (O) 5 (O) in (O) 0.5 (O) point (O) increments. (O) 
We (O) use (O) this (O) framework (O) to (O) evaluate (O) [synthesized (B) speech (I)] along (O) two (O) dimensions (O) : its (O) naturalness (O) and (O) similarity (O) to (O) real (O) [speech (B)] from (O) the (O) [target (B) speaker (I)]. 

[Speech (B) naturalness (I)] 

We (O) compared (O) the (O) naturalness (O) of (O) [synthesized (B) speech (I)] using (O) synthesizers (O) and (O) [vocoders (B)] trained (O) on (O) VCTK (O) and (O) [LibriSpeech (B)]. 
We (O) constructed (O) an (O) evaluation (O) set (O) of (O) 100 (O) phrases (O) which (O) do (O) not (O) appear (O) in (O) any (O) training (O) sets, (O) and (O) evaluated (O) two (O) sets (O) of (O) speakers (O) for (O) each (O) model (O) : one (O) composed (O) of (O) speakers (O) included (O) in (O) the (O) train (O) set (O) (Seen), (O) and (O) another (O) composed (O) of (O) those (O) that (O) were (O) held (O) out (O) (Unseen). (O) 
We (O) used (O) 11 (O) seen (O) and (O) [unseen (B) speakers (I)] for (O) VCTK (O) and (O) 10 (O) seen (O) and (O) [unseen (B) speakers (I)] for (O) [LibriSpeech (B)] (Appendix (O) D). (O) 
For (O) each (O) speaker, (O) we (O) randomly (O) chose (O) one (O) utterance (O) with (O) duration (O) of (O) about (O) 5 (O) seconds (O) to (O) use (O) to (O) compute (O) the (O) [speaker (B) embedding (I)] (see (O) Appendix (O) C). (O) 
Each (O) phrase (O) was (O) synthesized (O) for (O) each (O) speaker, (O) for (O) a (O) total (O) of (O) about (O) 1,000 (O) synthesized (O) utterances (O) per (O) evaluation. (O) 
Each (O) sample (O) was (O) rated (O) by (O) a (O) single (O) rater, (O) and (O) each (O) evaluation (O) was (O) conducted (O) independently (O) : the (O) outputs (O) of (O) different (O) models (O) were (O) not (O) compared (O) directly. (O) 
Results (O) are (O) shown (O) in (O) Table (O) 1, (O) comparing (O) the (O) proposed (O) model (O) to (O) baseline (O) multispeaker (O) models (O) that (O) utilize (O) a (O) lookup (O) table (O) of (O) [speaker (B) embeddings (I)] similar (O) to, (O) but (O) otherwise (O) have (O) identical (O) architectures (O) to (O) the (O) proposed (O) [synthesizer (B) network (I)]. 
The (O) proposed (O) model (O) achieved (O) about (O) 4.0 (O) [MOS (B)] in (O) all (O) datasets, (O) with (O) the (O) VCTK (O) model (O) obtaining (O) a (O) [MOS (B)] about (O) 0.2 (O) points (O) higher (O) than (O) the (O) [LibriSpeech (B) model (I)] when (O) evaluated (O) on (O) seen (O) speakers. (O) 
This (O) is (O) the (O) consequence (O) of (O) two (O) drawbacks (O) of (O) the (O) [LibriSpeech (B) dataset (I)] : (i) (O) the (O) lack (O) of (O) punctuation (O) in (O) transcripts, (O) which (O) makes (O) it (O) difficult (O) for (O) the (O) model (O) to (O) learn (O) to (O) pause (O) naturally, (O) and (O) (ii) (O) the (O) higher (O) level (O) of (O) background (O) noise (O) compared (O) to (O) VCTK, (O) some (O) of (O) which (O) the (O) synthesizer (O) has (O) learned (O) to (O) reproduce, (O) despite (O) denoising (O) the (O) training (O) targets (O) as (O) described (O) above. (O) 
Most (O) importantly, (O) the (O) [audio (B)] generated (O) by (O) our (O) model (O) for (O) [unseen (B) speakers (I)] is (O) deemed (O) to (O) be (O) at (O) least (O) as (O) natural (O) as (O) that (O) generated (O) for (O) seen (O) speakers. (O) 
Surprisingly, (O) the (O) [MOS (B)] on (O) [unseen (B) speakers (I)] is (O) higher (O) than (O) that (O) of (O) seen (O) speakers, (O) by (O) as (O) much (O) as (O) 0.2 (O) points (O) on (O) [LibriSpeech (B)]. 
This (O) is (O) a (O) consequence (O) of (O) the (O) randomly (O) selected (O) reference (O) utterance (O) for (O) each (O) speaker, (O) which (O) sometimes (O) contains (O) uneven (O) and (O) non-neutral (O) [prosody (B)]. 
In (O) informal (O) listening (O) tests (O) we (O) found (O) that (O) the (O) [prosody (B)] of (O) the (O) [synthesized (B) speech (I)] sometimes (O) mimics (O) that (O) of (O) the (O) reference, (O) similar (O) to. (O) 
This (O) effect (O) is (O) larger (O) on (O) [LibriSpeech (B)], which (O) contains (O) more (O) varied (O) [prosody (B)]. 
This (O) suggests (O) that (O) additional (O) care (O) must (O) be (O) taken (O) to (O) [disentangle (B) speaker (I)] identity (O) from (O) [prosody (B)] within (O) the (O) synthesis (O) network, (O) perhaps (O) by (O) integrating (O) a (O) [prosody (B) encoder (I)] as (O) in, (O) or (O) by (O) training (O) on (O) randomly (O) paired (O) reference (O) and (O) target (O) utterances (O) from (O) the (O) [same (B) speaker (I)]. 

Table (O) : [Speaker (B) similarity (I) Mean (I) Opinion (I) Score (I)] ([MOS (B)]) with (O) 95 (O) % confidence (O) intervals. (O) 



[Speaker (B) similarity (I)] 

To (O) evaluate (O) how (O) well (O) the (O) [synthesized (B) speech (I)] matches (O) that (O) from (O) the (O) [target (B) speaker (I)], we (O) paired (O) each (O) synthesized (O) utterance (O) with (O) a (O) randomly (O) selected (O) ground (O) truth (O) utterance (O) from (O) the (O) [same (B) speaker (I)]. 
Each (O) pair (O) is (O) rated (O) by (O) one (O) rater (O) with (O) the (O) following (O) instructions (O) : “ (O) You (O) should (O) not (O) judge (O) the (O) content, (O) grammar, (O) or (O) [audio (B) quality (I)] of (O) the (O) sentences (O) ; instead, (O) just (O) focus (O) on (O) the (O) similarity (O) of (O) the (O) speakers (O) to (O) one (O) another. (O) ” (O) 
Results (O) are (O) shown (O) in (O) Table. (O) 
The (O) scores (O) for (O) the (O) VCTK (O) model (O) tend (O) to (O) be (O) higher (O) than (O) those (O) for (O) [LibriSpeech (B)], reflecting (O) the (O) cleaner (O) nature (O) of (O) the (O) dataset. (O) 
This (O) is (O) also (O) evident (O) in (O) the (O) higher (O) ground (O) truth (O) baselines (O) on (O) VCTK. (O) 
For (O) [seen (B) speakers (I)] on (O) VCTK, (O) the (O) proposed (O) model (O) performs (O) about (O) as (O) well (O) as (O) the (O) baseline (O) which (O) uses (O) an (O) embedding (O) lookup (O) table (O) for (O) [speaker (B) conditioning (I)]. 
However, (O) on (O) [LibriSpeech (B)], the (O) proposed (O) model (O) obtained (O) a (O) lower (O) similarity (O) [MOS (B)] than (O) the (O) baseline, (O) which (O) is (O) likely (O) due (O) to (O) the (O) wider (O) degree (O) of (O) within-[speaker (B) variation (I)] (Appendix (O) B), (O) and (O) background (O) noise (O) level (O) in (O) the (O) dataset. (O) 
On (O) [unseen (B) speakers (I)], the (O) proposed (O) model (O) obtains (O) lower (O) similarity (O) between (O) ground (O) truth (O) and (O) [synthesized (B) speech (I)]. 
On (O) VCTK, (O) the (O) similarity (O) score (O) of (O) 3.28 (O) is (O) between (O) “ (O) moderately (O) similar (O) ” (O) and (O) “ (O) very (O) similar (O) ” (O) on (O) the (O) evaluation (O) scale. (O) 
Informally, (O) it (O) is (O) clear (O) that (O) the (O) proposed (O) model (O) is (O) able (O) to (O) transfer (O) the (O) broad (O) strokes (O) of (O) the (O) [speaker (B) characteristics (I)] for (O) [unseen (B) speakers (I)], clearly (O) reflecting (O) the (O) correct (O) gender, (O) [pitch (B)], and (O) formant (O) ranges (O) (as (O) also (O) visualized (O) in (O) Figure (O) 2). (O) 
But (O) the (O) significantly (O) reduced (O) similarity (O) scores (O) on (O) [unseen (B) speakers (I)] suggests (O) that (O) some (O) nuances, (O) e.g. (O) related (O) to (O) characteristic (O) [prosody (B)], are (O) lost. (O) 
The (O) [speaker (B) encoder (I)] is (O) trained (O) only (O) on (O) North (O) American (O) [accented (B) speech (I)]. 
As (O) a (O) result, (O) accent (O) mismatch (O) constrains (O) our (O) performance (O) on (O) [speaker (B) similarity (I)] on (O) VCTK (O) since (O) the (O) rater (O) instructions (O) did (O) not (O) specify (O) how (O) to (O) judge (O) accents, (O) so (O) raters (O) may (O) consider (O) a (O) pair (O) to (O) be (O) from (O) [different (B) speakers (I)] if (O) the (O) accents (O) do (O) not (O) match. (O) 
Indeed, (O) examination (O) of (O) rater (O) comments (O) shows (O) that (O) our (O) model (O) sometimes (O) produced (O) a (O) different (O) accent (O) than (O) the (O) ground (O) truth, (O) which (O) led (O) to (O) lower (O) scores. (O) 
However, (O) a (O) few (O) raters (O) commented (O) that (O) the (O) tone (O) and (O) inflection (O) of (O) the (O) voices (O) sounded (O) very (O) similar (O) despite (O) differences (O) in (O) accent. (O) 
As (O) an (O) initial (O) evaluation (O) of (O) the (O) ability (O) to (O) generalize (O) to (O) out (O) of (O) [domain (B) speakers (I)], we (O) used (O) synthesizers (O) trained (O) on (O) VCTK (O) and (O) [LibriSpeech (B)] to (O) synthesize (O) speakers (O) from (O) the (O) [other (B) dataset (I)]. 
We (O) only (O) varied (O) the (O) train (O) set (O) of (O) the (O) synthesizer (O) and (O) [vocoder (B) networks (I)] ; both (O) models (O) used (O) an (O) [identical (B) speaker (I) encoder (I)]. 
As (O) shown (O) in (O) Table (O) 3, (O) the (O) models (O) were (O) able (O) to (O) generate (O) [speech (B)] with (O) the (O) same (O) degree (O) of (O) naturalness (O) as (O) on (O) unseen, (O) but (O) in-domain, (O) speakers (O) shown (O) in (O) Table (O) 1. (O) 
However, (O) the (O) [LibriSpeech (B) model (I)] synthesized (O) [VCTK (B) speakers (I)] with (O) significantly (O) [higher (B) speaker (I)] similarity (O) than (O) the (O) VCTK (O) model (O) is (O) able (O) to (O) synthesize (O) [LibriSpeech (B) speakers (I)]. 
The (O) better (O) generalization (O) of (O) the (O) [LibriSpeech (B) model (I)] suggests (O) that (O) training (O) the (O) synthesizer (O) on (O) only (O) 100 (O) speakers (O) is (O) insufficient (O) to (O) enable (O) high (O) [quality (B) speaker (I)] transfer. (O) 

Table (O) : Cross (O) [- (B) dataset (I)] evaluation (O) on (O) naturalness (O) and (O) [speaker (B) similarity (I)] for (O) [unseen (B) speakers (I)]. 



[Speaker (B) verification (I)] 

As (O) an (O) objective (O) metric (O) of (O) the (O) degree (O) of (O) [speaker (B) similarity (I)] between (O) synthesized (O) and (O) ground (O) truth (O) [audio (B)] for (O) [unseen (B) speakers (I)], we (O) evaluated (O) the (O) ability (O) of (O) a (O) [limited (B) speaker (I)] verification (O) system (O) to (O) distinguish (O) synthetic (O) from (O) real (O) [speech (B)]. 
We (O) trained (O) a (O) new (O) eval-only (O) [speaker (B) encoder (I)] with (O) the (O) same (O) network (O) topology (O) as (O) Section, (O) but (O) using (O) a (O) different (O) training (O) set (O) of (O) 28 (O) M (O) utterances (O) from (O) 113000 (O) speakers. (O) 
Using (O) a (O) different (O) model (O) for (O) evaluation (O) ensured (O) that (O) metrics (O) were (O) not (O) only (O) valid (O) on (O) a (O) [specific (B) speaker (I)] embedding (O) space. (O) 
We (O) enroll (O) the (O) voices (O) of (O) 21 (O) [real (B) speakers (I)] : 11 (O) speakers (O) from (O) VCTK, (O) and (O) 10 (O) from (O) [LibriSpeech (B)], and (O) score (O) synthesized (O) [waveforms (B)] against (O) the (O) set (O) of (O) enrolled (O) speakers. (O) 
All (O) enrollment (O) and (O) [verification (B) speakers (I)] are (O) unseen (O) during (O) synthesizer (O) training. (O) 
[Speaker (B) verification (I)] equal (O) error (O) rates (O) (SV-EERs) (O) are (O) estimated (O) by (O) pairing (O) each (O) test (O) utterance (O) with (O) each (O) [enrollment (B) speaker (I)]. 
We (O) synthesized (O) 100 (O) test (O) utterances (O) for (O) each (O) speaker, (O) so (O) 21,000 (O) or (O) 23,100 (O) trials (O) were (O) performed (O) for (O) each (O) evaluation. (O) 
As (O) shown (O) in (O) Table, (O) as (O) long (O) as (O) the (O) synthesizer (O) was (O) trained (O) on (O) a (O) sufficiently (O) large (O) set (O) of (O) speakers, (O) i.e. (O) on (O) [LibriSpeech (B)], the (O) [synthesized (B) speech (I)] is (O) typically (O) most (O) similar (O) to (O) the (O) ground (O) [truth (B) voices (I)]. 
The (O) [LibriSpeech (B) synthesizer (I)] obtains (O) similar (O) EERs (O) of (O) 5-6 (O) % using (O) [reference (B) speakers (I)] from (O) both (O) datasets, (O) whereas (O) the (O) one (O) trained (O) on (O) VCTK (O) performs (O) much (O) worse, (O) especially (O) on (O) out-of-domain (O) [LibriSpeech (B) speakers (I)]. 
These (O) results (O) are (O) consistent (O) with (O) the (O) subjective (O) evaluation (O) in (O) Table. (O) 
To (O) measure (O) the (O) difficulty (O) of (O) discriminating (O) between (O) real (O) and (O) [synthetic (B) speech (I)] for (O) the (O) [same (B) speaker (I)], we (O) performed (O) an (O) additional (O) evaluation (O) with (O) an (O) expanded (O) set (O) of (O) enrolled (O) speakers (O) including (O) 10 (O) synthetic (O) versions (O) of (O) the (O) 10 (O) real (O) [LibriSpeech (B) speakers (I)]. 
On (O) this (O) 20 (O) [voice (B) discrimination (I)] task (O) we (O) obtain (O) an (O) EER (O) of (O) 2.86 (O) %, (O) demonstrating (O) that, (O) while (O) the (O) [synthetic (B) speech (I)] tends (O) to (O) be (O) close (O) to (O) the (O) [target (B) speaker (I)] (cosine (O) similarity (O) > 0.6, (O) and (O) as (O) in (O) Table (O) 4), (O) it (O) is (O) nearly (O) always (O) even (O) closer (O) to (O) other (O) synthetic (O) utterances (O) for (O) the (O) [same (B) speaker (I)] (similarity (O) > 0.7). (O) 
From (O) this (O) we (O) can (O) conclude (O) that (O) the (O) proposed (O) model (O) can (O) generate (O) [speech (B)] that (O) resembles (O) the (O) [target (B) speaker (I)], but (O) not (O) well (O) enough (O) to (O) be (O) confusable (O) with (O) a (O) [real (B) speaker (I)]. 

Speaker (O) embedding (O) space (O) 

Visualizing (O) the (O) speaker (O) embedding (O) space (O) further (O) contextualizes (O) the (O) quantitive (O) results (O) described (O) in (O) Section (O) and. (O) 
As (O) shown (O) in (O) Figure (O) 3, (O) [different (B) speakers (I)] are (O) well (O) separated (O) from (O) each (O) other (O) in (O) the (O) speaker (O) embedding (O) space. (O) 
The (O) PCA (O) visualization (O) (left) (O) shows (O) that (O) synthesized (O) utterances (O) tend (O) to (O) lie (O) very (O) close (O) to (O) real (O) [speech (B)] from (O) the (O) [same (B) speaker (I)] in (O) the (O) embedding (O) space. (O) 
However, (O) synthetic (O) utterances (O) are (O) still (O) easily (O) distinguishable (O) from (O) the (O) real (O) human (O) [speech (B)] as (O) demonstrated (O) by (O) the (O) t-SNE (O) visualization (O) (right) (O) where (O) utterances (O) from (O) each (O) [synthetic (B) speaker (I)] form (O) a (O) distinct (O) cluster (O) adjacent (O) to (O) a (O) cluster (O) of (O) real (O) utterances (O) from (O) the (O) corresponding (O) speaker. (O) 
Speakers (O) appear (O) to (O) be (O) well (O) separated (O) by (O) gender (O) in (O) both (O) the (O) PCA (O) and (O) t-SNE (O) visualizations, (O) with (O) all (O) [female (B) speakers (I)] appearing (O) on (O) the (O) left, (O) and (O) all (O) [male (B) speakers (I)] appearing (O) on (O) the (O) right. (O) 
This (O) is (O) an (O) indication (O) that (O) the (O) [speaker (B) encoder (I)] has (O) learned (O) a (O) reasonable (O) representation (O) of (O) [speaker (B) space (I)]. 

Figure (O) : Visualization (O) of (O) [speaker (B) embeddings (I)] extracted (O) from (O) [LibriSpeech (B)] utterances. (O) 
Each (O) color (O) corresponds (O) to (O) a (O) [different (B) speaker (I)]. 
Real (O) and (O) synthetic (O) utterances (O) appear (O) nearby (O) when (O) they (O) are (O) from (O) the (O) [same (B) speaker (I)], however (O) real (O) and (O) synthetic (O) utterances (O) consistently (O) form (O) distinct (O) clusters. (O) 

Table (O) : Performance (O) using (O) [speaker (B) encoders (I)] (SEs) (O) trained (O) on (O) [different (B) datasets (I)]. 
Synthesizers (O) are (O) all (O) trained (O) on (O) [LibriSpeech (B)] Clean (O) and (O) evaluated (O) on (O) held (O) out (O) speakers. (O) 

Number (O) of (O) [speaker (B) encoder (I) training (I) speakers (I)] 

It (O) is (O) likely (O) that (O) the (O) ability (O) of (O) the (O) proposed (O) model (O) to (O) generalize (O) well (O) across (O) a (O) wide (O) variety (O) of (O) speakers (O) is (O) based (O) on (O) the (O) quality (O) of (O) the (O) representation (O) learned (O) by (O) the (O) [speaker (B) encoder (I)]. 
We (O) therefore (O) explored (O) the (O) effect (O) of (O) the (O) [speaker (B) encoder (I) training (I)] set (O) on (O) synthesis (O) quality. (O) 
We (O) made (O) use (O) of (O) three (O) additional (O) training (O) sets (O) : [LibriSpeech (B)] Other, (O) which (O) contains (O) 461 (O) hours (O) of (O) [speech (B)] from (O) a (O) set (O) of (O) 1,166 (O) [speakers (B) disjoint (I)] from (O) those (O) in (O) the (O) clean (O) subsets, (O) [VoxCeleb (B)], and (O) VoxCeleb2 (O) which (O) contain (O) 139000 (O) utterances (O) from (O) 1,211 (O) speakers, (O) and (O) 1.09 (O) M (O) utterances (O) from (O) 5,994 (O) speakers, (O) respectively. (O) 
Table (O)   compares (O) the (O) performance (O) of (O) the (O) proposed (O) model (O) as (O) a (O) function (O) of (O) the (O) number (O) of (O) speakers (O) used (O) to (O) train (O) the (O) [speaker (B) encoder (I)]. 
This (O) measures (O) the (O) importance (O) of (O) [speaker (B) diversity (I)] when (O) training (O) the (O) [speaker (B) encoder (I)]. 
To (O) avoid (O) overfitting, (O) the (O) [speaker (B) encoders (I)] trained (O) on (O) [small (B) datasets (I)] (top (O) two (O) rows) (O) use (O) a (O) smaller (O) [network (B) architecture (I)] (256-dim (O) [LSTM (B) cells (I)] with (O) 64-dim (O) projections) (O) and (O) output (O) 64 (O) [dimensional (B) speaker (I) embeddings (I)]. 
We (O) first (O) evaluate (O) the (O) [speaker (B) encoder (I)] trained (O) on (O) [LibriSpeech (B)] Clean (O) and (O) Other (O) sets, (O) each (O) of (O) which (O) contain (O) a (O) similar (O) number (O) of (O) speakers. (O) 
In (O) Clean, (O) the (O) [speaker (B) encoder (I)] and (O) synthesizer (O) are (O) trained (O) on (O) the (O) [same (B) data (I)], a (O) baseline (O) similar (O) to (O) the (O) non-fine (O) tuned (O) [speaker (B) encoder (I)] from, (O) except (O) that (O) it (O) is (O) trained (O) discriminatively (O) as (O) in. (O) 
This (O) matched (O) condition (O) gives (O) a (O) slightly (O) better (O) naturalness (O) and (O) a (O) similar (O) similarity (O) score. (O) 
As (O) the (O) number (O) of (O) [training (B) speakers (I)] increases, (O) both (O) naturalness (O) and (O) similarity (O) improve (O) significantly. (O) 
The (O) objective (O) EER (O) results (O) also (O) improve (O) alongside (O) the (O) subjective (O) evaluations. (O) 
These (O) results (O) have (O) an (O) important (O) implication (O) for (O) [multispeaker (B) TTS (I) training (I)]. 
The (O) [data (B) requirement (I)] for (O) the (O) [speaker (B) encoder (I)] is (O) much (O) cheaper (O) than (O) full (O) [TTS (B) training (I)] since (O) no (O) transcripts (O) are (O) necessary, (O) and (O) the (O) [audio (B) quality (I)] can (O) be (O) lower (O) than (O) for (O) [TTS (B) training (I)]. 
We (O) have (O) shown (O) that (O) it (O) is (O) possible (O) to (O) synthesize (O) very (O) natural (O) [TTS (B)] by (O) combining (O) a (O) [speaker (B) encoder (I) network (I)] trained (O) on (O) large (O) amounts (O) of (O) [untranscribed (B) data (I)] with (O) a (O) [TTS (B) network (I)] trained (O) on (O) a (O) smaller (O) set (O) of (O) high (O) [quality (B) data (I)]. 

Table (O) : [Speech (B)] from (O) [fictitious (B) speakers (I)] compared (O) to (O) their (O) nearest (O) neighbors (O) in (O) the (O) train (O) sets. (O) 

[Fictitious (B) speakers (I)] 

Bypassing (O) the (O) [speaker (B) encoder (I) network (I)] and (O) conditioning (O) the (O) synthesizer (O) on (O) random (O) points (O) in (O) the (O) speaker (O) embedding (O) space (O) results (O) in (O) [speech (B)] from (O) [fictitious (B) speakers (I)] which (O) are (O) not (O) present (O) in (O) the (O) train (O) or (O) test (O) sets (O) of (O) either (O) the (O) synthesizer (O) or (O) the (O) [speaker (B) encoder (I)]. 
This (O) is (O) demonstrated (O) in (O) Table, (O) which (O) compares (O) 10 (O) [such (B) speakers (I)], generated (O) from (O) uniformly (O) sampled (O) points (O) on (O) the (O) surface (O) of (O) the (O) unit (O) hypersphere, (O) to (O) their (O) nearest (O) neighbors (O) in (O) the (O) training (O) sets (O) of (O) the (O) component (O) networks. (O) 
SV-EERs (O) are (O) computed (O) using (O) the (O) same (O) setup (O) as (O) Section (O) after (O) enrolling (O) voices (O) of (O) the (O) 10 (O) nearest (O) neighbors. (O) 
Even (O) though (O) these (O) speakers (O) are (O) totally (O) fictitious, (O) the (O) synthesizer (O) and (O) the (O) [vocoder (B)] are (O) able (O) to (O) generate (O) [audio (B)] as (O) natural (O) as (O) for (O) seen (O) or (O) unseen (O) [real (B) speakers (I)]. 
The (O) low (O) cosine (O) similarity (O) to (O) the (O) nearest (O) neighbor (O) training (O) utterances (O) and (O) very (O) high (O) EER (O) indicate (O) that (O) they (O) are (O) indeed (O) distinct (O) from (O) the (O) [training (B) speakers (I)]. 

Conclusion (O) 

We (O) present (O) a (O) [neural (B) network-based (I) system (I)] for (O) [multispeaker (B) TTS (I) synthesis (I)]. 
The (O) system (O) combines (O) an (O) independently (O) trained (O) [speaker (B) encoder (I) network (I)] with (O) a (O) [sequence-to-sequence (B) TTS (I) synthesis (I)] network (O) and (O) [neural (B) vocoder (I)] based (O) on (O) [Tacotron (B) 2 (I)]. 
By (O) leveraging (O) the (O) knowledge (O) learned (O) by (O) the (O) [discriminative (B) speaker (I) encoder (I)], the (O) synthesizer (O) is (O) able (O) to (O) generate (O) [high (B) quality (I) speech (I)] not (O) only (O) for (O) speakers (O) seen (O) during (O) training, (O) but (O) also (O) for (O) speakers (O) never (O) seen (O) before. (O) 
Through (O) evaluations (O) based (O) on (O) a (O) [speaker (B) verification (I)] system (O) as (O) well (O) as (O) subjective (O) listening (O) tests, (O) we (O) demonstrated (O) that (O) the (O) [synthesized (B) speech (I)] is (O) reasonably (O) similar (O) to (O) real (O) [speech (B)] from (O) the (O) [target (B) speakers (I)], even (O) on (O) such (O) [unseen (B) speakers (I)]. 
We (O) ran (O) experiments (O) to (O) analyze (O) the (O) impact (O) of (O) the (O) amount (O) of (O) data (O) used (O) to (O) train (O) the (O) different (O) components, (O) and (O) found (O) that, (O) given (O) [sufficient (B) speaker (I)] diversity (O) in (O) the (O) synthesizer (O) training (O) set, (O) [speaker (B) transfer (I)] quality (O) could (O) be (O) significantly (O) improved (O) by (O) increasing (O) the (O) amount (O) of (O) [speaker (B) encoder (I) training (I) data (I)]. 
Transfer (O) learning (O) is (O) critical (O) to (O) achieving (O) these (O) results. (O) 
By (O) separating (O) the (O) training (O) of (O) the (O) [speaker (B) encoder (I)] and (O) the (O) synthesizer, (O) the (O) system (O) significantly (O) lowers (O) the (O) requirements (O) for (O) [multispeaker (B) TTS (I) training (I) data (I)]. 
It (O) requires (O) neither (O) [speaker (B) identity (I)] labels (O) for (O) the (O) synthesizer (O) [training (B) data (I)], nor (O) high (O) quality (O) clean (O) [speech (B)] or (O) transcripts (O) for (O) the (O) [speaker (B) encoder (I) training (I) data (I)]. 
In (O) addition, (O) training (O) the (O) components (O) independently (O) significantly (O) simplifies (O) the (O) training (O) configuration (O) of (O) the (O) [synthesizer (B) network (I)] compared (O) to (O) since (O) it (O) does (O) not (O) require (O) additional (O) triplet (O) or (O) contrastive (O) losses. (O) 
However, (O) modeling (O) [speaker (B) variation (I)] using (O) a (O) low (O) dimensional (O) [vector (B)] limits (O) the (O) ability (O) to (O) leverage (O) large (O) amounts (O) of (O) reference (O) [speech (B)]. 
Improving (O) [speaker (B) similarity (I)] given (O) more (O) than (O) a (O) few (O) seconds (O) of (O) reference (O) [speech (B)] requires (O) a (O) model (O) adaptation (O) approach (O) as (O) in, (O) and (O) more (O) recently (O) in. (O) 
Finally, (O) we (O) demonstrate (O) that (O) the (O) model (O) is (O) able (O) to (O) generate (O) realistic (O) [speech (B)] from (O) [fictitious (B) speakers (I)] that (O) are (O) dissimilar (O) from (O) the (O) training (O) set, (O) implying (O) that (O) the (O) model (O) has (O) learned (O) to (O) utilize (O) a (O) realistic (O) representation (O) of (O) the (O) space (O) of (O) [speaker (B) variation (I)]. 
The (O) proposed (O) model (O) does (O) not (O) attain (O) [human-level (B) naturalness (I)], despite (O) the (O) use (O) of (O) a (O) [WaveNet (B) vocoder (I)] (along (O) with (O) its (O) very (O) high (O) inference (O) cost), (O) in (O) contrast (O) to (O) the (O) [single (B) speaker (I)] results (O) from. (O) 
This (O) is (O) a (O) consequence (O) of (O) the (O) additional (O) difficulty (O) of (O) generating (O) [speech (B)] for (O) a (O) variety (O) of (O) speakers (O) given (O) significantly (O) [less (B) data (I)] per (O) speaker, (O) as (O) well (O) as (O) the (O) use (O) of (O) datasets (O) with (O) [lower (B) data (I)] quality. (O) 
An (O) additional (O) limitation (O) lies (O) in (O) the (O) model’s (O) inability (O) to (O) transfer (O) accents. (O) 
Given (O) sufficient (O) [training (B) data (I)], this (O) could (O) be (O) addressed (O) by (O) conditioning (O) the (O) synthesizer (O) on (O) [independent (B) speaker (I)] and (O) accent (O) embeddings. (O) 
Finally, (O) we (O) note (O) that (O) the (O) model (O) is (O) also (O) not (O) able (O) to (O) completely (O) isolate (O) the (O) [speaker (B) voice (I)] from (O) the (O) [prosody (B)] of (O) the (O) reference (O) [audio (B)], a (O) similar (O) trend (O) to (O) that (O) observed (O) in. (O) 
