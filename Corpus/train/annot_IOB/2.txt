Current (O) trends (O) in (O) [multilingual (B) speech (I) processing (I)] 

Abstract. (O) 
In (O) this (O) paper, (O) we (O) describe (O) recent (O) work (O) at (O) Idiap (O) Research (O) Institute (O) in (O) the (O) domain (O) of (O) [multilingual (B) speech (I) processing (I)] and (O) provide (O) some (O) insights (O) into (O) emerging (O) challenges (O) for (O) the (O) [research (B) community (I)]. 
[Multilingual (B) speech (I) processing (I)] has (O) been (O) a (O) topic (O) of (O) ongoing (O) interest (O) to (O) the (O) [research (B) community (I)] for (O) many (O) years (O) and (O) the (O) field (O) is (O) now (O) receiving (O) renewed (O) interest (O) owing (O) to (O) two (O) strong (O) driving (O) forces. (O) 
Firstly, (O) technical (O) advances (O) in (O) [speech (B) recognition (I)] and (O) synthesis (O) are (O) posing (O) new (O) challenges (O) and (O) opportunities (O) to (O) researchers. (O) 
For (O) example, (O) [discriminative (B) features (I)] are (O) seeing (O) wide (O) application (O) by (O) the (O) [speech (B) recognition (I)] community, (O) but (O) additional (O) issues (O) arise (O) when (O) using (O) [such (B) features (I)] in (O) a (O) [multilingual (B) setting (I)]. 
Another (O) example (O) is (O) the (O) apparent (O) convergence (O) of (O) [speech (B) recognition (I)] and (O) [speech (B) synthesis (I) technologies (I)] in (O) the (O) form (O) of (O) [statistical (B) parametric (I)] methodologies. (O) 
This (O) convergence (O) enables (O) the (O) investigation (O) of (O) new (O) approaches (O) to (O) unified (O) modelling (O) for (O) [automatic (B) speech (I) recognition (I)] and (O) [text-to-speech (B) synthesis (I)] ([TTS (B)]) as (O) well (O) as (O) [cross-lingual (B) speaker (I) adaptation (I)] for (O) [TTS (B)]. 
The (O) second (O) driving (O) force (O) is (O) the (O) impetus (O) being (O) provided (O) by (O) both (O) government (O) and (O) industry (O) for (O) technologies (O) to (O) help (O) break (O) down (O) domestic (O) and (O) international (O) language (O) barriers, (O) these (O) also (O) being (O) barriers (O) to (O) the (O) expansion (O) of (O) policy (O) and (O) commerce. (O) 
[Speech-to-speech (B)] and (O) [speech-to (B)]-text translation (O) are (O) thus (O) emerging (O) as (O) key (O) technologies (O) at (O) the (O) heart (O) of (O) which (O) lies (O) [multilingual (B) speech (I) processing (I)]. 

Keywords. (O) [Multilingual (B) speech (I) processing (I)] ; [speech (B) synthesis (I)] ; [speech (B) recognition (I)] ; [speech-to-speech (B) translation (I)] ; language (O) identification. (O) 


Introduction (O) 

[Multilingual (B) speech (I) processing (I)] ([MLSP (B)]) is (O) a (O) distinct (O) field (O) of (O) research (O) in (O) [speech (B) and (I) language (I) technology (I)] that (O) combines (O) many (O) of (O) the (O) techniques (O) developed (O) for (O) monolingual (O) systems (O) with (O) new (O) approaches (O) that (O) address (O) specific (O) challenges (O) of (O) the (O) multilingual (O) domain. (O) 
Research (O) in (O) [MLSP (B)] should (O) be (O) of (O) particular (O) interest (O) to (O) countries (O) such (O) as (O) India (O) and (O) Switzerland (O) where (O) there (O) are (O) several (O) officially (O) recognized (O) languages (O) and (O) many (O) more (O) additional (O) languages (O) are (O) commonly (O) spoken. (O) 
In (O) such (O) multilingual (O) environments, (O) the (O) language (O) barrier (O) can (O) pose (O) significant (O) difficulties (O) in (O) both (O) commerce (O) and (O) government (O) administration (O) and (O) technological (O) advances (O) that (O) could (O) help (O) break (O) down (O) this (O) barrier (O) would (O) be (O) of (O) great (O) cultural (O) and (O) economic (O) value. (O) 
In (O) this (O) paper, (O) we (O) discuss (O) current (O) trends (O) in (O) [MLSP (B)] and (O) how (O) these (O) relate (O) to (O) advances (O) in (O) the (O) general (O) domain (O) of (O) [speech (B) and (I) language (I) technology (I)]. 
Examples (O) of (O) current (O) advances (O) and (O) trends (O) in (O) the (O) field (O) of (O) [MLSP (B)] are (O) provided (O) in (O) the (O) form (O) of (O) case (O) studies (O) of (O) research (O) being (O) conducted (O) at (O) Idiap (O) Research (O) Institute (O) in (O) the (O) framework (O) of (O) international (O) and (O) national (O) research (O) programmes. (O) 
The (O) first (O) case (O) study (O) presents (O) work (O) in (O) personalized (O) [speech-to-speech (B) translation (I)], in (O) which (O) we (O) have (O) made (O) significant (O) advances (O) in (O) developing (O) methods (O) for (O) [unsupervised (B) cross-lingual (I) adaptation (I)] of (O) [hidden (B) Markov (I) model (I)] (HMM)-based (O) [speech (B) synthesis (I)]. 
In (O) addition, (O) this (O) work (O) has (O) been (O) closely (O) linked (O) with (O) efforts (O) to (O) develop (O) unified (O) models (O) for (O) [automatic (B) speech (I) recognition (I)] ([ASR (B)]) and (O) [text-to-speech (B) synthesis (I)] ([TTS (B)]), thus (O) highlighting (O) the (O) potential (O) of (O) [MLSP (B)] to (O) impact (O) broader (O) research (O) topics. (O) 
The (O) second (O) case (O) study (O) looks (O) at (O) discriminative (O) methods (O) in (O) [ASR (B)] and (O) how (O) these (O) have (O) been (O) applied (O) to (O) problems (O) in (O) [MLSP (B)]. 
This (O) includes (O) a (O) study (O) of (O) [multilingual (B) acoustic (I) models (I)], in (O) particular (O) in (O) the (O) context (O) of (O) [multilayer (B) perceptron (I)] (MLP)-based (O) [discriminative (B) feature (I)] extraction. (O) 
The (O) major (O) component (O) of (O) this (O) study (O) is (O) the (O) use (O) of (O) hierarchical (O) classification (O) frameworks (O) that (O) have (O) the (O) potential (O) to (O) provide (O) more (O) robust (O) performance (O) while (O) simplifying (O) means (O) to (O) perform (O) crosslanguage (O) knowledge (O) transfer. (O) 
The (O) third (O) and (O) final (O) case (O) study (O) looks (O) at (O) the (O) related (O) tasks (O) of (O) language (O) identification (O) and (O) out-of-language (O) detection (O) using (O) [ASR (B)]. 
The (O) paper (O) is (O) organized (O) as (O) follows (O) : in (O) section, (O) we (O) present (O) a (O) brief (O) literature (O) review (O) of (O) [multilingual (B) speech (I) processing (I)], followed (O) by (O) a (O) study (O) of (O) more (O) recent (O) trends (O) in (O) [speech (B) and (I) language (I) technology (I)] and (O) how (O) these (O) impact (O) [MLSP (B)] in (O) section. (O) 
In (O) sections, (O) we (O) present (O) the (O) three (O) case (O) studies (O) on (O) work (O) being (O) conducted (O) at (O) Idiap (O) on (O) personalized (O) [speech-to-speech (B) translation (I)], [discriminative (B) features (I)] in (O) [multilingual (B) ASR (I)], and (O) language (O) identification (O) and (O) out-of-language (O) detection, (O) respectively. (O) 
Finally, (O) in (O) section (O) we (O) provide (O) some (O) insights (O) into (O) future (O) opportunities (O) and (O) challenges (O) in (O) [MLSP (B)]. 

[Multilingual (B) speech (I) processing (I)] 

In (O) language, (O) there (O) are (O) normally (O) two (O) forms (O) of (O) communication, (O) namely, (O) spoken (O) form (O) and (O) written (O) form. (O) 
Depending (O) upon (O) the (O) granularity (O) of (O) representation, (O) both (O) these (O) forms (O) can (O) have (O) different (O) or (O) common (O) representation (O) in (O) terms (O) of (O) basic (O) units. (O) 
For (O) instance, (O) in (O) spoken (O) form, (O) [phonemes (B)] / syllables (O) can (O) be (O) considered (O) as (O) the (O) basic (O) unit. (O) 
Similarly, (O) in (O) the (O) case (O) of (O) (most) (O) written (O) forms, (O) [graphemes (B)] / characters (O) are (O) the (O) smallest (O) basic (O) units. (O) 
However, (O) above (O) the (O) smaller (O) units, (O) a (O) word (O) can (O) be (O) seen (O) as (O) a (O) common (O) unit (O) for (O) both (O) spoken (O) and (O) written (O) forms. (O) 
The (O) word (O) then (O) in (O) turn (O) can (O) be (O) described (O) in (O) terms (O) of (O) the (O) smaller (O) basic (O) units (O) corresponding (O) to (O) the (O) spoken (O) form (O) or (O) written (O) form. (O) 
For (O) a (O) given (O) language, (O) there (O) can (O) be (O) a (O) consistent (O) relationship (O) between (O) spoken (O) form (O) and (O) written (O) form. (O) 
However, (O) the (O) relationship (O) may (O) not (O) be (O) same (O) across (O) languages. (O) 
Addressing (O) the (O) issue (O) of (O) a (O) common (O) basic (O) representation (O) across (O) languages (O) is (O) central (O) to (O) [MLSP (B)], although (O) this (O) can (O) raise (O) different (O) challenges (O) in (O) applications (O) such (O) as (O) [ASR (B)] and (O) [TTS (B)]. 
It (O) is (O) important (O) to (O) note (O) that (O) not (O) all (O) languages (O) spoken (O) in (O) the (O) world (O) have (O) both (O) forms. (O) 
There (O) are (O) languages (O) that (O) have (O) spoken (O)   form (O) but (O) no (O) written (O) form. (O) 
It (O) should (O) be (O) noted (O) that (O) in (O) case (O) of (O) some (O) languages, (O) the (O) written (O) form (O) of (O) word (O) and (O) the (O) character (O) representation (O) may (O) be (O)   considered (O) same (O) ; e.g. (O) Mandarin (O) language. (O) 
Most (O) commonly, (O) the (O) idea (O) of (O) a (O) unified (O) [phonetic (B) lexicon (I)] is (O) often (O) used (O) as (O) the (O) binding (O) element (O) in (O) [MLSP (B) systems (I)]. 
Such (O) a (O) [lexicon (B)] is (O) available (O) in (O) the (O) International (O) [Phonetic (B)] Association (O) [(IPA) (B) system (I)]. 
Most (O) [phonetic (B) lexicons (I)] are (O) in (O) practice (O) just (O) alternative (O) parameterizations (O) of (O) the (O) [IPA (B) symbols (I)]. 
Some (O) [IPA (B) symbols (I)] are (O) difficult (O) to (O) represent (O) in (O) computers, (O) and (O) are (O) catered (O) to (O) by (O) using (O) the (O) SAMPA3 (O) and (O) SAMPROSA4 (O) systems. (O) 
Although (O) [SAMPA (B)] was (O) originally (O) designed (O) for (O) just (O) European (O) languages, (O) it (O) has (O) occasionally (O) been (O) extended (O) to (O) others (O) (e.g., (O) Chinese). (O) 
The (O) worldbet (O) system (O) described (O) in (O) is (O) more (O) thorough. (O) 
There (O) is (O) a (O) one-to-one (O) mapping (O) between, (O) say, (O) ARPABET5 (O) symbols (O) and (O) [IPA (B) symbols (I)] ; notation (O) aside, (O) the (O) former (O) is (O) just (O) a (O) subset (O) of (O) the (O) latter. (O) 
A (O) drawback (O) of (O) such (O) a (O) common (O) representation (O) is (O) that (O) it (O) can (O) preclude (O) the (O) possibility (O) of (O) tuning (O) aspects (O) of (O) systems (O) to (O) the (O) particular (O) language. (O) 
For (O) instance, (O) the (O) C (O) – (O) V (O) syllable (O) structure (O) of (O) Japanese (O) and (O) tonal (O) nature (O) of (O) Chinese (O) would (O) normally (O) be (O) hardwired (O) in (O) a (O) monolingual (O) system, (O) but (O) this (O) may (O) not (O) be (O) feasible (O) in (O) the (O) multilingual (O) case. (O) 
In (O) the (O) remainder (O) of (O) this (O) section, (O) we (O) give (O) a (O) brief (O) survey (O) of (O) [MLSP (B)] in (O) the (O) context (O) of (O) specific (O) applications, (O) concentrating (O) on (O) [ASR (B)] and (O) [TTS (B)]. 

[Automatic (B) speech (I) recognition (I)] 

In (O) the (O) field (O) of (O) [ASR (B)], advances (O) have (O) been (O) largely (O) promoted (O) by (O) the (O) US (O) government (O) via (O) the (O) National (O) Institute (O) of (O) Science (O) and (O) Technology. (O) 
Besides (O) running (O) evaluations, (O) one (O) of (O) the (O) the (O) main (O) contributions (O) of (O) this (O) has (O) been (O) to (O) provide (O) databases. (O) 
This (O) has (O) led (O) naturally (O) to (O) an (O) English (O) language (O) focus (O) for (O) [ASR (B) research (I)] and (O) development. (O) 
That (O) is (O) not (O) to (O) say (O) that (O) [ASR (B) research (I)] is (O) English (O) centric (O) ; rather, (O) many (O) of (O) the (O) algorithmic (O) advances (O) have (O) been (O) made (O) initially (O) in (O) that (O) language. (O) 
Such (O) advances (O) have, (O) however, (O) translated (O) easily (O) to (O) different (O) languages, (O) largely (O) owing (O) to (O) the (O) robustness (O) of (O) [statistical (B) approaches (I)] to (O) the (O) different (O) specificities (O) of (O) languages. (O) 
State-of-the-art (O) [ASR (B) systems (I)] are (O) stochastic (O) in (O) nature (O) and (O) are (O) typically (O) based (O) on (O) [HMM (B)]. 
Figure (O)   illustrates (O) a (O) typical (O) [HMM-based (B) ASR (I) system (I)] for (O) English (O) language. (O) 
From (O) a (O) spoken (O) language (O) perspective, (O) the (O) different (O) components (O) of (O) the (O) [HMM-based (B) ASR (I) system (I)] are (O) as (O) follows. (O) 
(i) (O) [Feature (B) extractor (I)] that (O) extracts (O) the (O) relevant (O) information (O) from (O) the (O) [speech (B) signal (I)] yielding (O) a (O) sequence (O) of (O) [feature (B) observations (I)] / [vectors (B)]. 
[Feature (B) extraction (I)] is (O) typically (O) considered (O) a (O) [language (B) independent (I) process (I)] (i.e., (O) [common (B) feature (I)] extraction (O) algorithms (O) are (O) used (O) in (O) most (O) systems (O) regardless (O) of (O) language), (O) although (O) in (O) some (O) cases (O) (such (O) as (O) tonal (O) languages) (O) specific (O) processing (O) should (O) be (O) explored. (O) 
(ii) (O) [Acoustic (B) model (I)] that (O) models (O) the (O) relation (O) between (O) the (O) [feature (B) vector (I)] and (O) units (O) of (O) spoken (O) form (O) (sound (O) units (O) such (O) as (O) phones). (O) 
(iii) (O) [Lexicon (B)] that (O) integrates (O) lexical (O) constraints (O) on (O) top (O) of (O) spoken (O) unit (O) level (O) representation (O) yielding (O) a (O) unit (O) representation (O) that (O) is (O) typically (O) common (O) to (O) both (O) spoken (O) form (O) and (O) written (O) form (O) such (O) as, (O) word (O) or (O) [morpheme (B)]. 
(iv) (O) Language (O) model (O) that (O) tends (O) to (O) model (O) syntactical (O) / grammatical (O) constraints (O) of (O) the (O) spoken (O) language (O) using (O) the (O) unit (O) representation (O) resulting (O) after (O) integrating (O) lexical (O) constraints. (O) 

http://www.phon.ucl.ac.uk (O) / home (O) / sampa/ (O) 
http://www.phon.ucl.ac.uk (O) / home (O) / [sampa (B)] / samprosa.htm (O) 
http://en.wikipedia.org/wiki/Arpabet (O) 
http://www.nist.gov/index.html (O) 

Figure. (O) Illustration (O) of (O) a (O) typical (O) [HMM-based (B) ASR (I) system (I)]. 

The (O) [HMM-based (B) ASR (I) system (I)] using (O) Gaussians (O) to (O) model (O) the (O) [feature (B) observations (I)] is (O) referred (O) to (O) as (O) [HMM (B)] / [GMM (B) system (I)]. 
Similarly, (O) hybrid (O) [HMM (B)] / ANN (O) systems (O) refer (O) to (O) the (O) case (O) where (O) [artificial (B) neural (I) networks (I)] (ANNs), (O) typically, (O) [MLPs (B)], are (O) used (O) to (O) model (O) the (O) [feature (B) observations (I)] and (O) estimate (O) phone (O) posteriors (O) based (O) on (O) the (O) [input (B) features (I)] within (O) some (O) temporal (O) context (O) (typically (O) 90 (O) ms). (O) 
In (O) [ASR (B)], we (O) are (O) generally (O) attempting (O) to (O) address (O) one (O) of (O) two (O) issues (O) in (O) [MLSP (B)]. 
The (O) first (O) issue (O) is (O) that (O) of (O) building (O) [multilingual (B) models (I)] ; that (O) is, (O) models (O) that (O) can (O) recognize (O) [speech (B)] of (O) multiple (O) languages. (O) 
In (O) building (O) such (O) [multilingual (B) systems (I)], a (O) major (O) issue (O) is (O) the (O) availability (O) of (O) resources. (O) 
At (O) present, (O) there (O) are (O) various (O) resources (O) that (O) enable (O) [multilingual (B) speech (I) research (I)]. 
An (O) early (O) example (O) is (O) the (O) OGI (O) multi-language (O) telephone (O) [speech (B) corpus (I)]. 
More (O) recent (O) ones (O) are (O) [GlobalPhone (B)], SpeechDat(M) (O) and (O) SPEECON. (O) 
Complementary (O) to (O) the (O) issue (O) of (O) [data (B) resources (I)], the (O) second (O) major (O) area (O) of (O) research (O) in (O) [MLSP (B)] for (O) [ASR (B)] concerns (O) [cross-language (B) transfer (I)]. 
Specifically, (O) how (O) [data (B) resources (I)] for (O) a (O) given (O) language(s) (O) can (O) be (O) used (O) to (O) improve (O) [ASR (B)] in (O) another (O) language, (O) in (O) particular (O) when (O) resources (O) for (O) that (O) language (O) are (O) lacking. (O) 
Much (O) of (O) the (O) work (O) in (O) [cross-language (B) transfer (I)] has (O) built (O) upon (O) progress (O) already (O) made (O) in (O) multilingual (O) modelling. (O) 
A (O) trivial (O) approach (O) to (O) building (O) [multilingual (B) models (I)] is (O) to (O) construct (O) separate (O) acoustic (O) and (O) language (O) models (O) for (O) each (O) desired (O) language, (O) i.e., (O) a (O) monolingual (O) system (O) for (O) each (O) language. (O) 
During (O) recognition, (O) multiple (O) [decoders (B)] corresponding (O) to (O) the (O) different (O) monolingual (O) systems (O) are (O) run (O) in (O) parallel, (O) and (O) the (O) recognizer (O) output (O) yielding (O) the (O) [maximum (B) likelihood (I)] is (O) selected. (O) 
As (O) a (O) by-product, (O) the (O) identity (O) of (O) language (O) is (O) also (O) inferred. (O) 
Such (O) an (O) approach (O) is (O) simple (O) and (O) feasible. (O) 
However, (O) in (O) the (O) light (O) of (O) practical (O) issues (O) such (O) as (O) portability (O) to (O) new (O) languages (O) (especially (O) with (O) fewer (O) resources), (O) system (O) memory (O) and (O) computational (O) requirements, (O) it (O) may (O) not (O) be (O) the (O) best (O) approach. (O) 
Given (O) this, (O) there (O) has (O) been (O) considerable (O) effort (O) devoted (O) to (O) build (O) [acoustic (B) models (I)] and (O) language (O) models (O) that (O) are (O) shared (O) across (O) languages. (O) 
Along (O) the (O) direction (O) of (O) multilingual (O) acoustic (O) modelling, (O) the (O) emphasis (O) has (O) been (O) towards (O) finding (O) a (O) common (O) sound (O) unit (O) representation (O) that (O) is (O) shared (O) across (O) languages. (O) 
In (O) the (O) literature, (O) a (O) popular (O) approach (O) towards (O) this (O) is (O) the (O) creation (O) of (O) a (O) ‘ (O) universal (O) / global’ (O) [phone (B) set (I)] by (O) first (O) pooling (O) the (O) [phone (B) sets (I)] of (O) different (O) languages (O) together (O) and (O) then (O) merging (O) them (O) (i) (O) based (O) on (O) heuristics (O) / knowledge (O) such (O) as (O) [IPA-based (B)] (Köhler (O) 1996) (O) or (O) [SAMPA-based (B)] (Ackermann (O) et (O) al (O) 1996), (O) [articulatory (B) features (I)], (ii) (O) in (O) a (O) data-driven (O) manner (O) by (O) clustering (O) (Köhler (O) 1999) (O) or (O) by (O) measuring (O) [phoneme (B)] similarity (O) such (O) as (O) using (O) confusion (O) matrix (O) (Andersen (O) et (O) al (O) 1993), (O) or (O) (iii) (O) both (O) i.e., (O) knowledge-based (O) merging (O) followed (O) by (O) datadriven (O) clustering (O) (Schultz (O) & Waibel (O) 1998 (O) ; Köhler (O) 1999). (O) 
The (O) underlying (O) assumption (O) here (O) is (O) that (O) the (O) articulatory (O) representations (O) of (O) phones (O) are (O) similar (O) across (O) languages, (O) and (O) thus (O) their (O) acoustic (O) realizations (O) can (O) be (O) considered (O) [language (B) independent (I)]. 
These (O) studies (O) were (O) mostly (O) done (O) in (O) the (O) framework (O) of (O) modelling (O) [context-independent (B) phones (I)]. 
The (O) approach (O) of (O) knowledge-based (O) merging (O) followed (O) by (O) [data-driven (B) clustering (I)] for (O) [context-dependent (B) phone (I) models (I)] was (O) further (O) investigated (O) by. (O) 
In (O) the (O) literature, (O) different (O) studies (O) have (O) been (O) reported (O) for (O) [cross-language (B) transfer (I)], where (O) an (O) [acoustic (B) model (I)] trained (O) on (O) a (O) different (O) language (O) or (O) many (O) different (O) languages (O) (excluding (O) [target (B) language (I)]) is (O) (i) (O) used (O) directly (O) when (O) no (O) data (O) from (O) the (O) [target (B) language (I)] is (O) available, (O) (ii) (O) adapted (O) when (O) limited (O) data (O) from (O) [target (B) language (I)] is (O) available, (O) or (O) (iii) (O) used (O) as (O) seed (O) model (O) and (O) then (O) trained (O) with (O) large (O) amount (O) of (O) data. (O) 
When (O) compared (O) to (O) the (O) use (O) of (O) [monolingual (B) acoustic (I) models (I)], it (O) has (O) been (O) typically (O) observed (O) that (O) [multilingual (B) acoustic (I) models (I)] yield (O) a (O) better (O) starting (O) point (O) for (O) such (O) [cross-language (B) transfer (I)]. 
From (O) the (O) above (O) brief (O) literature (O) overview, (O) it (O) is (O) clear (O) that (O) there (O) has (O) been (O) reasonable (O) success (O) in (O) constructing (O) [multilingual (B) acoustic (I) models (I)]. 
However, (O) developing (O) multilingual (O) language (O) models (O) is (O) still (O) an (O) open (O) issue. (O) 
Standard (O) [ASR (B) systems (I)] typically (O) use (O) [N-gram (B) language (I) models (I)], where (O) word (O) sequences (O) are (O) modelled (O) by (O) a (O) N (O) th (O) order (O) [Markov (B)] chain, (O) i.e., (O) the (O) model (O) consists (O) of (O) probability (O) to (O) transit (O) to (O) a (O) word (O) given (O) a (O) word (O) sequence (O) of (O) length (O) N8. (O) 
When (O) estimating (O) [N-gram (B) language (I) models (I)] for (O) different (O) languages, (O) the (O) [language-specific (B)] characteristic (O) plays (O) a (O) greater (O) role. (O) 
For (O) instance, (O) at (O) the (O) word (O) level, (O) the (O) perplexity (O) of (O) the (O) language (O) can (O) vary (O) greatly (O) depending (O) on (O) the (O) degree (O) of (O) morphological (O) inflection. (O) 
As (O) a (O) result, (O) morphologically (O) rich (O) languages (O) can (O) have (O) many (O) more (O) words, (O) thus (O) leading (O) to (O) morphological-level (O) rather (O) than (O) word-level (O) representations. (O) 
Also, (O) some (O) morphologically (O) rich (O) languages (O) tend (O) to (O) allow (O) free (O) word (O) ordering, (O) which (O) is (O) clearly (O) not (O) well (O) handled (O) by (O) simple (O) [N-gram (B) models (I)]. 
Additionally, (O) [character-based (B) languages (I)], such (O) as (O) Chinese (O) and (O) Japanese, (O) do (O) not (O) explicitly (O) identify (O) word (O) boundaries. (O) 
In (O) such (O) cases, (O) a (O) system (O) for (O) word (O) segmentation (O) is (O) necessary. (O) 
In (O) spite (O) of (O) all (O) these (O) [language-specific (B) issues (I)], there (O) is (O) need (O) for (O) multilingual (O) language (O) models (O) to (O) handle, (O) for (O) example, (O) instances (O) of (O) ‘ (O) code (O) switching9’. (O) 
Research (O) in (O) the (O) direction (O) of (O) building (O) multilingual (O) language (O) models (O) has (O) mainly (O) focussed (O) on (O) (i) (O) estimation (O) of (O) a (O) single (O) language (O) model (O) using (O) the (O) data (O) from (O) all (O) languages, (O) or (O) (ii) (O) estimation (O) of (O) a (O) language (O) model (O) for (O) each (O) language (O) separately (O) including (O) the (O) words (O) from (O) other (O) languages (O) and (O) interpolation (O) of (O) these. (O) 
It (O) has (O) been (O) observed (O) that (O) the (O) latter (O) approach (O) yields (O) better (O) performance (O) than (O) the (O) first (O) approach. (O) 

[Text-to-speech (B) synthesis (I)] 

In (O) the (O) domain (O) of (O) [TTS (B)], issues (O) in (O) [multilingual (B) speech (I) processing (I)] have (O) been (O) dominated (O) by (O) two (O) main (O) areas. (O) 
This (O) is (O) partially (O) due (O) to (O) the (O) limitations (O) of (O) the (O) dominant (O) [unit (B) selection (I)] paradigm, (O) which (O) directly (O) uses (O) the (O) recordings (O) from (O) [voice (B) talents (I)] in (O) the (O) generation (O) of (O) [synthesized (B) speech (I)]. 
It (O) is (O) clear (O) that (O) multilingual (O) techniques (O) in (O) [unit (B) selection (I)] are (O) thus (O) bound (O) by (O) the (O) limited (O) availability (O) of (O) [multilingual (B) voice (I)] talents (O) and (O) more (O) so (O) by (O) the (O) availability (O) of (O) such (O) recordings (O) to (O) the (O) [research (B) community (I)], although (O) some (O) research (O) has (O) been (O) conducted (O) to (O) overcome (O) this. (O) 
The (O) first (O) area (O) concerns (O) the (O) construction (O) of (O) [TTS (B) voices (I)] in (O) multiple (O) languages, (O) which (O) is (O) dealt (O) with (O) in (O) detail (O) by. (O) 
This (O) area (O) of (O) research (O) is (O) largely (O) concerned (O) with (O) the (O) development (O) of (O) methodologies (O) that (O) can (O) be (O) made (O) portable (O) across (O) languages, (O) and (O) include (O) issues (O) in (O) text (O) parsing, (O) intonation (O) and (O) [waveform (B) generation (I)]. 
Many (O) of (O) these (O) general (O) issues (O) are (O) mirrored (O) in (O) challenges (O) faced (O) by (O) the (O) [multilingual (B) ASR (I)] community, (O) as (O) has (O) been (O) discussed (O) in (O) the (O) opening (O) of (O) this (O) section, (O) although (O) many (O) of (O) the (O) related (O) challenges (O) are (O) arguably (O) more (O) complex (O) in (O) [TTS (B)] due (O) to (O) the (O) extensive (O) use (O) of (O) supra-[segmental (B) features (I)] and (O) the (O) greater (O) degree (O) of (O) language (O) dependence (O) of (O) [such (B) features (I)] – (O) there (O) is (O) no (O) ‘ (O) international (O) prosodic (O) alphabet’. (O) 
Much (O) progress (O) in (O) this (O) direction (O) has (O) been (O) made (O) owing (O) to (O) efforts (O) of (O) the (O) [research (B) community (I)] towards (O) the (O) development (O) of (O) freely (O) available (O) corpora (O) and (O) [TTS (B) tools (I)] (for (O) example, (O) see (O) Festival, (O) Festvox (O) and (O) MBROLA), (O) which (O) has (O) enabled (O) the (O) development (O) of (O) synthesis (O) systems (O) in (O) many (O) languages. (O) 
Although, (O) the (O) systems (O) that (O) have (O) been (O) developed (O) using (O) these (O) resources (O) have (O) remained (O) largely (O) independent (O) of (O) one (O) another, (O) these (O) developments (O) have (O) laid (O) the (O) ground (O) work (O) for (O) future (O) multilingual (O) applications (O) of (O) [TTS (B)]. 
A (O) second (O) area (O) of (O) research (O) that (O) has (O) a (O) more (O) evident (O) emphasis (O) on (O) multilingual (O) capabilities (O) is (O) that (O) of (O) polyglot (O) synthesis. (O) 
In (O) polyglot (O) synthesis, (O) the (O) goal (O) is (O) the (O) synthesis (O) of (O) mixed (O) language (O) utterances, (O) hence, (O) a (O) major (O) challenge (O) is (O) in (O) proper (O) text (O) parsing (O) that (O) enables (O) correct (O) pronunciation (O) and (O) intonation (O) of (O) such (O) [speech (B)]. 
Given (O) appropriate (O) text (O) parsing, (O) the (O) main (O) concern (O) of (O) [unit (B) selection (I) synthesis (I)] is (O) the (O) efficient (O) design (O) of (O) a (O) mixed-language (O) inventory (O) of (O) [speech (B)]. 
A (O) related (O) field (O) of (O) research, (O) [voice (B) conversion (I)], has (O) helped (O) overcome (O) some (O) of (O) the (O) limitations (O) of (O) [unit (B) selection (I) methods (I)] with (O) respect (O) to (O) [MLSP (B)]. 
In (O) particular, (O) the (O) application (O) of (O) [voice (B) conversion (I)] to (O) [cross-lignual (B) scenarios (I)] has (O) been (O) investigated, (O) especially (O) in (O) the (O) context (O) of (O) [speech-to-speech (B) translation (I)]. Drawbacks (O) of (O) [voice (B) conversion (I)] techniques (O) lie (O) in (O) their (O) limited (O) ability (O) to (O) modify (O) supra-segmental (O) [speech (B)] characteristics (O) and (O) the (O) requirement (O) of (O) [parallel (B) data (I)] for (O) learning (O) the (O) conversion, (O) although (O) some (O) progress (O) in (O) this (O) direction (O) has (O) been (O) made. (O) 

refers (O) to (O) unigram, (O) bigram, (O) trigram, (O) respectively. (O) 
Code (O) switching (O) refers (O) to (O) the (O) case (O) where (O) the (O) user (O) switches (O) from (O) one (O) language (O) to (O) another (O) language. (O) 
The (O) switch (O) can (O) happen (O) within (O) an (O) utterance (O) or (O) across (O) utterances. (O) 

Automatic (O) language (O) recognition (O) 

The (O) objective (O) of (O) automatic (O) language (O) recognition (O) is (O) to (O) recognize (O) the (O) spoken (O) language (O) by (O) automatic (O) analysis (O) of (O) [speech (B) data (I)]. 
Automatic (O) language (O) recognition (O) can (O) be (O) classified (O) into (O) two (O) tasks (O) (i) (O) automatic (O) language (O) identification (O) and (O) (ii) (O) automatic (O) language (O) detection. (O) 
In (O) principle, (O) this (O) classification (O) is (O) similar (O) to (O) [speaker (B) identification (I)] and (O) [speaker (B) verification (I)] in (O) [speaker (B) recognition (I)] research. (O) 
The (O) goal (O) of (O) automatic (O) language (O) identification (O) (LID) (O) is (O) to (O) classify (O) a (O) given (O) input (O) [speech (B)] utterance (O) as (O) belonging (O) to (O) one (O) out (O) of (O) L (O) languages. (O) 
Various (O) possible (O) applications (O) of (O) LID (O) can (O) be (O) found (O) in (O) [multilingual (B) speech (I) processing (I)], call (O) routing, (O) [interactive (B) voice (I)] response (O) applications, (O) and (O) [front-end (B) processing (I)] for (O) [speech (B) translation (I) translation (I)]. 
There (O) are (O) a (O) variety (O) of (O) cues, (O) including (O) [phonological (B)], morphological, (O) syntactical (O) or (O) prosodic (O) cues, (O) that (O) can (O) be (O) exploited (O) by (O) an (O) LID (O) system. (O) 
In (O) the (O) literature, (O) different (O) approaches (O) have (O) been (O) proposed (O) to (O) perform (O) LID, (O) such (O) as (O) using (O) only (O) low-level (O) [spectral (B) information (I)], using (O) [phoneme (B)] recognizers (O) in (O) conjunction (O) with (O) [phonotactic (B) constraints (I)] or (O) using (O) mediumto (O) high-level (O) information (O) (e.g. (O) lexical (O) constraints, (O) language (O) models) (O) through (O) [speech (B) recognition (I)]. 
Among (O) these, (O) the (O) most (O) common (O) approach (O) is (O) to (O) use (O) [phoneme (B)] recognizers (O) along (O) with (O) [phonotactic (B) constraints (I)]. 
The (O) [phoneme (B)] recognizer (O) can (O) be (O) [language-dependent (B)] (using (O) a (O) [language-specific (B) phoneme (I) set (I)]) or (O) [language-independent (B)]   (using (O) a (O) [multilingual (B) phoneme (I) set (I)]). 
The (O) [phonotactic (B) constraints (I)] are (O) typically (O) modelled (O) by (O) a (O) [phoneme (B)] bigram (O) estimated (O) on (O) phonetically (O) labelled (O) data. (O) 
Given (O) a (O) segment (O) of (O) [speech (B) signal (I)] and (O) associated (O) claimed (O) language, (O) the (O) goal (O) of (O) automatic (O) language (O) detection (O) is (O) to (O) verify (O) the (O) claim (O) or, (O) in (O) other (O) words, (O) choose (O) one (O) of (O) the (O) two (O) possible (O) hypotheses. (O) 
A (O) null (O) hypothesis (O) that (O) the (O) given (O) [speech (B) segment (I)] belongs (O) to (O) the (O) claimed (O) language (O) or (O) the (O) alternative (O) hypothesis (O) that (O) the (O) given (O) segment (O) does (O) not (O) belong (O) to (O) the (O) claimed (O) language. (O) 
Usually, (O) this (O) is (O) achieved (O) by (O) training (O) a (O) model (O) corresponding (O) to (O) the (O) null (O) hypothesis (O) using (O) data (O) from (O) the (O) [target (B) language (I)], and (O) a (O) separate (O) model (O) corresponding (O) to (O) the (O) alternate (O) hypothesis (O) using (O) data (O) from (O) different (O) languages. (O) 

Recent (O) trends (O) in (O) [speech (B)] and (O) language (O) processing (O) 

We (O) can (O) identify (O) a (O) number (O) of (O) advances (O) in (O) [speech (B)] and (O) language (O) processing (O) that (O) have (O) significantly (O) impacted (O) [MLSP (B)]. 
One (O) of (O) the (O) most (O) important (O) developments (O) has (O) been (O) the (O) rise (O) of (O) statistical (O) [machine (B) translation (I)] that (O) has (O) resulted (O) in (O) substantial (O) funding (O) and, (O) consequently, (O) research (O) activity (O) being (O) directed (O) towards (O) [machine (B) translation (I)] and (O) its (O) related (O) multilingual (O) applications (O) ([speech-to-speech (B)], [speech-to (B)]-text translation, (O) etc.). (O) 
Several (O) notable (O) projects (O) have (O) been (O) pursued (O) in (O) recent (O) years, (O) to (O) mention (O) only (O) a (O) few (O) : Spoken (O) Language (O) Communication (O) and (O) Translation (O) System (O) for (O) Tactical (O) Use (O) (TRANSTAC (O) DARPA (O) initiative), (O) Technology (O) and (O) Corpora (O) for (O) [Speech (B)] to (O) [Speech (B) Translation (I)] (TC-STAR (O) FP6 (O) European (O) project), (O) and (O) the (O) Global (O) Autonomous (O) Language (O) Exploitation (O) ([GALE (B)] DARPA (O) initiative10). (O) 
Research (O) in (O) these (O) projects (O) not (O) only (O) needs (O) to (O) focus (O) on (O) the (O) optimization (O) of (O) individual (O) components, (O) but (O) also (O) on (O) how (O) the (O) components (O) can (O) be (O) integrated (O) together (O) effectively (O) to (O) provide (O) overall (O) improved (O) output. (O) 
This (O) is (O) not (O) a (O) trivial (O) task. (O) 
For (O) instance, (O) [speech (B) recognition (I) systems (I)] are (O) typically (O) optimized (O) to (O) reduce (O) [word (B) error (I) rate (I)] ([WER (B)]) (or (O) character (O) / letter (O) error (O) rate, (O) CER (O) / LER, (O) in (O) some (O) languages). (O) 
The (O) [WER (B)] measure (O) gives (O) equal (O) importance (O) to (O) the (O) different (O) types (O) of (O) error (O) that (O) can (O) be (O) committed (O) by (O) the (O) [ASR (B) system (I)], namely, (O) deletion, (O) insertion (O) and (O) substitution. (O) 
Suppose, (O) if (O) the (O) [ASR (B) system (I)] output (O) (i.e., (O) text (O) transcript) (O) is (O) processed (O) by (O) a (O) [machine (B) translation (I) system (I)], then (O) deletion (O) error (O) is (O) probably (O) more (O) expensive (O) compared (O) to (O) other (O) two (O) errors (O) as (O) all (O) information (O) is (O) lost. (O) 
It (O) then (O) follows (O) that (O) the (O) optimal (O) performance (O) with (O) respect (O) to (O) [WER (B)] may (O) not (O) provide (O) the (O) best (O) possible (O) translated (O) output (O) and (O) vice (O) versa. (O) 
Indeed, (O) in (O) the (O) [GALE (B)] project (O) (in (O) the (O) context (O) of (O) translation (O) of (O) Mandarin (O) language (O) [speech (B) data (I)] to (O) English), (O) it (O) was (O) observed (O) that (O) CER (O) is (O) less (O) correlated (O) with (O) translation (O) error (O) rate (O) when (O) compared (O) to (O) objective (O) functions (O) such (O) as (O) SParseval (O) (parsing-based (O) word (O) string (O) scoring (O) function). (O) 
Similarly, (O) the (O) relatively (O) recent (O) emergence (O) of (O) [statistical (B) parametric (I) speech (I) synthesis (I)]   has (O) resulted (O) in (O) a (O) flurry (O) of (O) new (O) research (O) activities (O) and (O) is (O) contributing (O) to (O) substantial (O) efforts (O) to (O) ‘ (O) cross-pollinate’ (O) ideas (O) between (O) [ASR (B)] and (O) [TTS (B)], as (O) techniques (O) in (O) these (O) two (O) fields (O) have (O) become (O) increasingly (O) interrelated. (O) 
[HMM-based (B) TTS (I)] has (O) helped (O) accelerate (O) efforts (O) in (O) the (O) development (O) of (O) [multilingual (B) TTS (I)] by (O) providing (O) a (O) means (O) to (O) easily (O) train (O) synthesis (O) systems (O) for (O) new (O) languages, (O) where (O) adaptive (O) techniques (O) may (O) prove (O) particularly (O) useful. (O) 
It (O) is (O) also (O) evident (O) that (O) the (O) dominant (O) source-filter (O) model (O) employed (O) in (O) [HMM-based (B) TTS (I)]   may (O) not (O) be (O)   ideal (O) for (O) all (O) languages (O) and (O) further (O) work (O) is (O) being (O) carried (O) out (O) to (O) address (O) this (O) problem. (O) 
There (O) has (O) also (O) been (O) increasing (O) effort (O) in (O) the (O) use (O) of (O) [lightly (B) supervised (I)] and (O) unsupervised (O) training (O) methods (O) for (O) [ASR (B)], which (O) has (O) more (O) recently (O) been (O) applied (O) to (O) [TTS (B)]. 
Methods (O) that (O) use (O) ‘ (O) found’ (O) data (O) from (O) the (O) internet (O) have (O) also (O) been (O) shown (O) to (O) be (O) useful. (O) 
Such (O) approaches (O) have (O) the (O) potential (O) to (O) have (O) a (O) great (O) impact (O) on (O) efforts (O) in (O) [MLSP (B)] for (O) poorer (O) resourced (O) languages (O) or (O) languages (O) with (O) little (O) transcribed (O) material, (O) as (O) has (O) been (O) demonstrated (O) by. (O) 
There (O) have (O) also (O) been (O) efforts (O) in (O) [cross-lingual (B) language (I) modelling (I)], where (O) [cross-lingual (B) information (I) retrieval (I)] and (O) [machine (B) translation (I)] have (O) been (O) used (O) with (O) (i) (O) sentence-aligned (O) [parallel (B) corpus (I)], (ii) (O) document-aligned (O) corpus, (O) and (O) (iii) (O) latent (O) semantic (O) analysis (O) to (O) improve (O) language (O) modelling (O) for (O) resource-deficient (O) language. (O) 
Finally, (O) relatively (O) recent (O) efforts (O) have (O) been (O) introduced (O) to (O) integrate (O) more (O) appropriate (O) training (O) criteria (O) in (O) machine (O) learning (O) algorithms (O) that (O) provide (O) more (O) discriminative (O) models (O) in (O) the (O) case (O) of (O) [ASR (B)] and (O) better (O) quality (O) synthesis (O) for (O) [TTS (B)]. 
This (O) has (O) not (O) only (O) been (O) restricted (O) to (O) the (O) estimation (O) of (O) model (O) parameters, (O) but (O) has (O) also (O) been (O) applied (O) to (O) [feature (B) extraction (I)], such (O) as (O) [MLP (B) features (I)]. 
These (O) methods (O) have (O) been (O) shown (O) to (O) provide (O) significant (O) improvements (O) for (O) [monolingual (B) ASR (I) systems (I)], but (O) generalization (O) to (O) the (O) multilingual (O) case (O) has (O) not (O) been (O) extensively (O) investigated. (O) 
There (O) are (O) different (O) ways (O) for (O) improvement (O) of (O) these (O) methods, (O) such (O) as (O) use (O) of (O) hierarchical (O) methods (O) based (O) on (O) [MLP (B)] or (O) [conditional (B) random (I) fields (I)], [MLP (B)] regularization (O) approach (O) to (O) handle (O) resource (O) (data) (O) differences (O) across (O) languages (O) through (O) [cross-language (B) transfer (I)]. 

See (O) respectively (O) : http://www.darpa.mil/ipto/programs/transtac/transtac.asp (O) ; http://www.tc-star.org (O) ; http://www.darpa.mil/ipto/programs/gale/gale.asp (O) 

Recent (O) and (O) ongoing (O) [MLSP (B) research (I)] at (O) Idiap (O) Research (O) Institute (O) 

At (O) Idiap, (O) [multilingual (B) speech (I) processing (I)] is (O) an (O) important (O) research (O) objective (O) given (O) Switzerland’s (O) location (O) within (O) Europe, (O) at (O) the (O) intersection (O) of (O) three (O) major (O) linguistic (O) cultures, (O) and (O) Swiss (O) society (O) itself (O) being (O) inherently (O) multilingual. (O) Towards (O) this (O) end, (O) we (O) have (O) been (O) conducting (O) research (O) in (O) [MLSP (B)] as (O) part (O) of (O) several (O) internationally (O) and (O) nationally (O) funded (O) projects, (O) including (O) for (O) instance (O) : 
  (i) (O) EMIME11 (O) (Effective (O) Multilingual (O) Interaction (O) in (O) Mobile (O) Environments) (O) : This (O) FP7 (O) EU (O) project (O) commenced (O) in (O) March (O) 2008 (O) and (O) is (O) investigating (O) the (O) personalization (O) of (O) [speech-to-speech (B) translation (I) systems (I)]. 
The (O) EMIME (O) project (O) aims (O) to (O) achieve (O) its (O) goal (O) through (O) the (O) use (O) of (O) [HMM-based (B) ASR (I)] and (O) [TTS (B)], more (O) specifically, (O) the (O) main (O) research (O) goal (O) is (O) the (O) development (O) of (O) techniques (O) that (O) enable (O) [unsupervised (B) cross-lingual (I) speaker (I) adaptation (I)] for (O) [TTS (B)]. 
 (ii) (O) GALE12 (O) (Global (O) Autonomous (O) Language (O) Exploitation) (O) : Idiap (O) was (O) involved (O) in (O) this (O) DARPA-funded (O) project (O) as (O) part (O) of (O) the (O) SRI-lead (O) team. (O) The (O) project (O) primarily (O) involved (O) [machine (B) translation (I)] and (O) information (O) distillation. (O) 
We (O) have (O) mostly (O) studied (O) the (O) development (O) of (O) new (O) discriminative (O) [MLP (B)]-based features (O) and (O) [MLP (B)] combination (O) methods (O) for (O) the (O) [ASR (B) components (I)]. 
Despite (O) cessation (O) of (O) the (O) project, (O) we (O) have (O) continued (O) our (O) research (O) of (O) this (O) topic. (O) 
(iii) (O) MULTI (O) (MULTImodal (O) Interaction (O) and (O) [MULTImedia (B) Data (I)] Mining) (O) is (O) a (O) Swiss (O) National (O) Science (O) Foundation (O) (SNSF) (O) project (O) carrying (O) out (O) fundamental (O) research (O) in (O) several (O) related (O) fields (O) of (O) multimodal (O) interaction (O) and (O) [multimedia (B) data (I)] mining. (O) 
A (O) recently (O) initiated (O) MULTI (O) sub-project (O) is (O) conducting (O) research (O) in (O) [MLP (B)]-based methods (O) for (O) language (O) identification (O) and (O) [multilingual (B) speech (I) recognition (I)] with (O) a (O) focus (O) on (O) Swiss (O) languages. (O) 
(iv) (O) COMTIS13 (O) (Improving (O) the (O) coherence (O) of (O) [machine (B) translation (I) output (I)] by (O) modelling (O) intersentential (O) relations) (O) is (O) a (O) new (O) [machine (B) translation (I)] project (O) funded (O) by (O) the (O) SNSF (O) that (O) was (O) started (O) on (O) March (O) 2010. (O) 
The (O) project (O) is (O) concerned (O) with (O) modelling (O) the (O) coherency (O) between (O) sentences (O) in (O) [machine (B) translation (I) output (I)], thereby (O) improving (O) overall (O) translation (O) performance. (O) 
In (O) the (O) remainder (O) of (O) the (O) paper, (O) we (O) present (O) three (O) case (O) studies (O) conducted (O) in (O) MSLP (O) at (O) Idiap (O) that (O) have (O) resulted (O) from (O) participation (O) in (O) the (O) above-mentioned (O) research (O) projects. (O) 

www.emime.org. (O) See (O)   for (O) an (O) overview (O) of (O) the (O) project. (O) 
http://www-speech.sri.com/projects/GALE (O) 

Personalizing (O) [speech-to-speech (B) translation (I)] 

One (O) aspect (O) which (O) we (O) take (O) for (O) granted (O) in (O) spoken (O) communication (O) that (O) is (O) largely (O) missing (O) from (O) current (O) [speech-to-speech (B) translation (I)] (SST) (O) technology (O) is (O) a (O) means (O) to (O) facilitate (O) the (O) personal (O) nature (O) of (O) spoken (O) dialogue. (O) 
That (O) is, (O) state-of-the-art (O) approaches (O) lack (O) or (O) are (O) limited (O) in (O) their (O) ability (O) to (O) be (O) personalized (O) in (O) an (O) effective (O) and (O) unobtrusive (O) manner, (O) thereby (O) acting (O) as (O) a (O) barrier (O) to (O) natural (O) communication. (O) 
The (O) use (O) of (O) a (O) common (O) framework (O) for (O) [ASR (B)] and (O) [TTS (B)] provides (O) several (O) interesting (O) research (O) opportunities (O) in (O) the (O) framework (O) of (O) SST, (O) including (O) the (O) development (O) of (O) unified (O) approaches (O) for (O) the (O) modelling (O) of (O) [speech (B)] for (O) recognition (O) and (O) synthesis (O) that (O) will (O) need (O) to (O) adapt (O) across (O) languages (O) to (O) each (O) user’s (O) speaking (O) characteristics. (O) 
In (O) this (O) section, (O) we (O) present (O) progress (O) that (O) has (O) recently (O) been (O) made (O) by (O) Idiap (O) in (O) the (O) EMIME (O) project, (O) but (O) firstly, (O) we (O) discuss (O) the (O) role (O) of (O) translation (O) in (O) our (O) study. (O) 

[Machine (B) translation (I)] as (O) the (O) glue (O) 

Unlike (O) in (O) most (O) other (O) [speech-to-speech (B) translation (I)] projects, (O) translation (O) plays (O) a (O) less (O) prominent (O) role (O) in (O) EMIME. (O) 
The (O) key (O) focus (O) of (O) research (O) is (O) the (O) personalization (O) of (O) SST, (O) which (O) essentially (O) requires (O) the (O) development (O) of (O) techniques (O) for (O) [unsupervised (B) cross-lingual (I) speaker (I) adaptation (I)] for (O) [HMM-based (B) TTS (I)]. 
Translation (O) acts (O) as (O) the (O) ‘ (O) glue’ (O) that (O) links (O) the (O) input (O) and (O) output (O) languages (O) for (O) [cross-lingual (B) adaptation (I)] and (O) also (O) links (O) [ASR (B)] and (O) [TTS (B)] for (O) unsupervised (O) adaptation. (O) 
Thus, (O) we (O) can (O) consider (O) our (O) goal (O) as (O) comprising (O) two (O) main (O) tasks (O) :   
• (O) To (O) bridge (O) the (O) gap (O) between (O) [ASR (B)] and (O) [TTS (B)] by (O) investigating (O) techniques (O) in (O) unsupervised (O) adaptation (O) for (O) [TTS (B)]. 
• (O) To (O) bridge (O) the (O) gap (O) between (O) languages (O) such (O) that (O) we (O) can (O) perform (O) [cross-lingual (B) adaptation (I)] of (O) [HMM-based (B) TTS (I)]. 
We (O) are (O) working (O) with (O) several (O) languages (O) that (O) encompass (O) a (O) range (O) of (O) language (O) families, (O) geographical (O) regions (O) and (O) partner (O) competencies (O) : English, (O) Finnish, (O) Japanese (O) and (O) Mandarin. (O) 
English (O) always (O) comprises (O) one (O) of (O) the (O) languages (O) in (O) each (O) SST (O) language (O) pair. (O) 
At (O) Idiap, (O) our (O) research (O) has (O) primarily (O) focused (O) on (O) the (O) English (O) – (O) Mandarin (O) language (O) pair. (O) 
English (O) – (O) Mandarin (O) is (O) arguably (O) the (O) most (O) disparate (O) of (O) the (O) language (O) pairs (O) under (O) consideration. (O) 
Although, (O) this (O) poses (O) a (O) greater (O) challenge, (O) it (O) may (O) better (O) enable (O) us (O) to (O) ascertain (O) and (O) analyse (O) differences (O) between (O) the (O) different (O) approaches (O) under (O) investigation. (O) 
In (O) performing (O) [unsupervised (B) cross-lingual (I) speaker (I) adaptation (I)] within (O) a (O) [speech-to-speech (B) translation (I) framework (I)], we (O) consider (O) two (O) possible (O) approaches (O) : a (O) ‘ (O) pipeline’ (O) framework, (O) in (O) which (O) individual (O) components (O) operate (O) largely (O) independent (O) of (O) one (O) another (O) ; and (O) a (O) ‘ (O) unified’ (O) framework (O) in (O) which (O) [ASR (B)] and (O) [TTS (B)] modules (O) share (O) common (O) components (O) such (O) as (O) [feature (B) extraction (I)] and (O) acoustic (O)   models. (O) 
It (O) should (O) be (O) clear (O) that, (O) while (O) the (O) pipeline (O) framework (O) contains (O) a (O) great (O) deal (O) of (O) redundancy, (O) it (O) allows (O) each (O) component (O) to (O) be (O) separately (O) optimized, (O) whereas (O) the (O) unified (O) framework (O) minimizes (O) redundancy, (O) but (O) possibly (O) at (O) a (O) cost (O) to (O) performance. (O) 
Figure (O)   illustrates (O) these (O) two (O) frameworks. (O) 

Figure. (O) Frameworks (O) for (O) [speech-to-speech (B) translation (I)]. 
‘ (O) [ASR (B)] ⇒ (O) [TTS (B)]’ denotes (O) mapping (O) of (O) [ASR (B)] (triphone (O) context) (O) labels (O) to (O) [TTS (B)] (full (O) context) (O) labels (O) via (O) a (O) [TTS (B) front-end (I)]. ‘ (O) SrcLang (O) ⇒ (O) rgLang’ (O) denotes (O) [cross-lingual (B) speaker (I) adaptation (I)] (CLSA) (O) from (O) the (O) input (O) language (O) to (O) the (O) output (O) language. (O) 
(a) (O) Pipeline (O) approach, (O) whereby (O) [ASR (B)] and (O) [TTS (B) feature (I)] extraction (O) and (O) models (O) do (O) not (O) share (O) common (O) components. (O) Separate (O) [TTS (B) models (I)] may (O) also (O) be (O) employed (O) in (O) input (O) and (O) output (O) languages. (O) 
(b) (O) Unified (O) approach, (O) whereby (O) [feature (B) extraction (I)] and (O) [acoustic (B) models (I)] are (O) shared (O) between (O) [ASR (B)] and (O) [TTS (B)] and (O) across (O) languages. (O) 

www.idiap.ch/comtis (O) 

Bridging (O) the (O) gap (O) between (O) [ASR (B)] and (O) [TTS (B)] 

[Statistical (B) parametric (I) approaches (I)] have (O) emerged (O) in (O) recent (O) years (O) as (O) a (O) dominant (O) paradigm (O) for (O) [TTS (B)]. 
Training (O) of (O) such (O) models (O) is (O) very (O) similar (O) to (O) the (O) training (O) of (O) models (O) for (O) [ASR (B)] – (O) [acoustic (B) features (I)] are (O) first (O) generated (O) that (O) are (O) used (O) to (O) train (O) [acoustic (B) models (I)] given (O) corresponding (O) labels. (O) 
During (O) synthesis, (O) the (O) label (O) sequence (O) is (O) generated (O) from (O) the (O) text (O) to (O) be (O) synthesized. (O) 
The (O) [acoustic (B) model (I)] is (O) then (O) used (O) to (O) generate (O) the (O) maximum (O) a (O) posteriori (O) probability (O) observation (O) sequence (O) for (O) the (O) given (O) labels, (O) taking (O) into (O) account (O) the (O) explicit (O) relationship (O) between (O) dynamic (O) and (O) static (O) components (O) of (O) the (O) [feature (B) vector (I)] (Zen (O) et (O) al (O) 2009). (O) 
This (O) can (O) be (O) considered (O) the (O) inverse (O) of (O) the (O) inference (O) procedure (O) carried (O) out (O) in (O) [ASR (B)]. 
Other (O) major (O) differences (O) between (O) [ASR (B)] and (O) [TTS (B)] include (O) 14 (O) the (O) following (O)    
• (O) [Acoustic (B) features (I)] : should (O) provide (O) necessary (O) information (O) to (O) reconstruct (O) the (O) [speech (B) signal (I)], normally (O) including (O) [pitch (B)] and (O) excitation (O) information (O)    
• (O) Labels (O) : take (O) into (O) account (O) a (O) much (O) broader (O) range (O) of (O) acoustic (O) and (O) prosodic (O) contexts. (O) 
Such (O) ‘ (O) full’ (O) context (O) labels (O) are (O) normally (O) produced (O) by (O) first (O) parsing (O) text (O) with (O) a (O) [TTS (B) front-end (I)] ; and (O)    
• (O) Models (O) : normally (O) differ (O) from (O) standard (O) [HMM (B)] by (O) including (O) explicit (O) duration (O) modelling (O) (hidden (O) semi-[Markov (B) model (I)] – (O) HSMM) (O) and (O) provide (O) appropriate (O) distributions (O) for (O) modelling (O) of (O) [discontinuous (B) features (I)] such (O) as (O) [pitch (B)] (multi-space (O) probability (O) distribution (O) – (O) MSD (O) (Tokuda (O) et (O) al (O) 2002)). (O) 
To (O) perform (O) [unsupervised (B) speaker (I)] adaptation (O) of (O) [TTS (B)], we (O) use (O) [ASR (B)] to (O) generate (O) transcriptions (O) that (O) are (O) necessary (O) to (O) adapt (O) the (O) [TTS (B) models (I)]. 
In (O) the (O) pipeline (O) approach, (O) [ASR (B) transcriptions (I)] form (O) the (O) only (O) link (O) between (O) the (O) [ASR (B)] and (O) [TTS (B)] modules. (O) 
These (O) erroneous (O) [ASR (B) transcriptions (I)] are (O) then (O) fed (O) through (O) a (O) [TTS (B) front-end (I)] that (O) is (O) used (O) to (O) generate (O) the (O) ‘ (O) full-context’ (O) labels (O) for (O) the (O) adaptation (O) of (O) the (O) [TTS (B) models (I)]. 
In (O) the (O) unified (O) approach, (O) [acoustic (B) models (I)] and (O) features (O) also (O) link (O) the (O) two (O) and (O) adaptation (O) of (O) [TTS (B)] is (O) carried (O) out (O) implicitly (O) during (O) the (O) adaptation (O) of (O) the (O) [ASR (B) models (I)] without (O) the (O) need (O) for (O) a (O) [TTS (B) front-end (I)]. 
As (O) noted (O) previously, (O) paradigms (O) for (O) [ASR (B)] and (O) [TTS (B)] have (O) undergone (O) a (O) degree (O) of (O) convergence (O) in (O) recent (O) years (O) and (O) now (O) the (O) [HMM-based (B) approach (I)] is (O) commonly (O) employed (O) for (O) both. (O) 
As (O) part (O) of (O) our (O) initial (O) studies, (O) we (O) conducted (O) a (O) comparison (O) of (O) [feature (B) extraction (I)], acoustic (O) modelling (O) and (O) adaptation (O) for (O) [ASR (B)] and (O) [TTS (B)]. 
Although (O) fully (O) unified (O) [ASR (B)] and (O) [TTS (B) models (I)] may (O) be (O) sub-optimal, (O) our (O) goal (O) was (O) to (O) quantify (O) the (O) differences (O) between (O) the (O) two (O) that (O) would (O) enable (O) us (O) to (O) better (O) determine (O) where (O) and (O) by (O) how (O) much (O) these (O) approaches (O) differ. (O) 
We (O) did (O) this (O) by (O) taking (O) [ASR (B)] and (O) [TTS (B)] baselines (O) built (O) on (O) a (O) [common (B) corpus (I)] and (O) then (O) systematically (O) interchanged (O) [ASR (B)] and (O) [TTS (B)] components (O) related (O) to (O) [lexicon (B)] and (O) [phone (B) set (I)], [feature (B) extraction (I)], model (O) topology (O) and (O) [speaker (B) adaptation (I)]. 
Our (O) findings (O) showed (O) that (O) many (O) of (O) the (O) techniques (O) used (O) in (O) [ASR (B)] and (O) [TTS (B)] can (O) not (O) be (O) simply (O) applied (O) to (O) [TTS (B)] and (O) [ASR (B)], respectively (O) without (O) negative (O) consequences. (O) 
Despite (O) this, (O) we (O) are (O) still (O) interested (O) in (O) unified (O) modelling (O) – (O) not (O) necessarily (O) as (O) a (O) means (O) to (O) explicitly (O) model (O) [speech (B)] jointly (O) for (O) both (O) [ASR (B)] and (O) [TTS (B)], but (O) rather (O) as (O) a (O) means (O) to (O) transfer (O) knowledge (O) and (O) approaches (O) between (O) the (O) two. (O) 
We (O) present (O) two (O) examples (O) of (O) this (O) study (O) which (O) relate (O) to (O) [feature (B) extraction (I)] and (O) acoustic (O) modelling. (O) 
[Voice (B) tract (I)] length (O) normalization (O) for (O) [speech (B) synthesis (I)] Vocal (O) tract (O) length (O) normalization (O) ([VTLN (B)]) is (O) a (O) [feature (B) transformation (I)] technique (O) that (O) has (O) been (O) used (O) extensively (O) in (O) [speech (B) recognition (I)] to (O) provide (O) robust (O) and (O) [rapid (B) speaker (I)] adaptation. (O) 
[VTLN (B)] operates (O) on (O) the (O) principle (O) that (O) the (O) vocal (O) tract (O) length (O) varies (O) across (O) [different (B) speakers (I)] and (O) the (O) formant (O) frequency (O)   positions (O) are (O) inversely (O) proportional (O) to (O) this. (O) 
Thus, (O) by (O) warping (O) the (O) frequency (O) scale (O) during (O) [feature (B) extraction (I)] we (O) are (O) able (O) to (O) approximately (O) account (O) for (O) this (O) variability (O) across (O) listeners. (O) 
An (O) additional (O) advantage (O) of (O) using (O) [VTLN (B)] in (O) the (O) context (O) of (O) [cross-lingual (B) adaptation (I)] is (O) that (O) it (O) should (O) be (O) inherently (O) independent (O) of (O) the (O) language (O) being (O) spoken. (O) 
Building (O) upon (O) earlier (O) studies (O) in (O) [ASR (B)], we (O) have (O) been (O) investigating (O) the (O) use (O) of (O) [VTLN (B)] in (O) [statistical (B) parametric (I) speech (I) synthesis (I)]. 
In (O) the (O) context (O) of (O) speaker (O) adaptive (O) [speech (B) synthesis (I)], the (O) most (O) commonly (O) employed (O) [feature (B) extraction (I)] technique (O) is (O) the (O) [mel (B)]-generalized cepstrum (O) (MGCEP). (O) 
MGCEP (O) analysis (O) uses (O) a (O) bilinear (O) transform (O) to (O) achieve (O) [spectral (B)] warping (O) that (O) can (O) approximate (O) that (O) of (O) the (O) [mel (B)] auditory (O) scale (O) (see (O) figure). (O) 
The (O) bilinear (O) transform (O) has (O) also (O) been (O) used (O) to (O) perform (O) [VTLN (B)]   and (O) it (O) follows (O) that (O) a (O) cascade (O) of (O) bilinear (O) transforms (O) simply (O) gives (O) another (O) bilinear (O) transform. (O) 
Hence, (O) [VTLN (B)] can (O) be (O) made (O) implicit (O) to (O) MGCEP (O) analysis (O) and (O) can (O) further (O) be (O) formulated (O) as (O) a (O) linear (O) transform (O) in (O) the (O) cepstral (O) domain, (O) which (O) permits (O) optimization (O) via (O) grid-search (O) or (O) expectation-maximization. (O) 
We (O) have (O) shown (O) that (O) [VTLN (B)] for (O) [TTS (B)] carries (O) with (O) it (O) additional (O) challenges. (O) 
Firstly, (O) the (O) highdimensional (O) nature (O) of (O) [TTS (B) features (I)] results (O) in (O) more (O) severe (O) impact (O) of (O) warping (O) on (O) the (O) [feature (B) space (I)], not (O) only (O) making (O) the (O) use (O) of (O) Jacobian (O) normalization (O) imperative, (O) but (O) also (O) requiring (O) care (O) with (O) the (O) initialization (O) of (O) the (O) model. (O) 
Secondly, (O) it (O) is (O) not (O) clear (O) that (O) the (O) ideal (O) criterion (O) for (O) [VTLN (B)] in (O) [TTS (B)] is (O) the (O) same (O) as (O) that (O) typically (O) used (O) in (O) [ASR (B)]. 
We (O) have (O) noted (O) that (O) the (O) warping (O) factors (O) inferred (O) using (O) the (O) usual (O) objective (O) function (O) do (O) not (O) result (O) in (O) perceptible (O) warping (O) of (O) the (O) voice, (O) while (O) a (O) modified (O) criterion (O) achieves (O) warping (O) that (O) is (O) subjectively (O) closer (O) to (O) the (O) [target (B) speaker (I)] (but, (O) in (O) contrast, (O) is (O) detrimental (O) to (O) [ASR (B) performance (I)]). 
This (O) result (O) suggests, (O) once (O) again, (O) the (O) divergent (O) nature (O) of (O) [ASR (B)] and (O) [TTS (B)] in (O) terms (O) of (O) direct (O) compatibility, (O) but (O) demonstrates (O) the (O) portability (O) of (O) fundamental (O) techniques (O) between (O) the (O) two. (O) 
[Decision (B) tree (I)] marginalization (O) In (O) [unsupervised (B) adaptation (I) of (I) TTS (I)], we (O) adapt (O) [synthesis (B) models (I)] from (O) the (O) noisy (O) [speech (B) recognition (I) output (I)]. 
If (O) we (O) wish (O) to (O) bypass (O) the (O) [TTS (B) front-end (I)], this (O) requires (O) a (O) means (O) to (O) adapt (O) the (O) full-context (O) [TTS (B) acoustic (I) models (I)] from (O) the (O) [triphone-context (B) labels (I)] generated (O) by (O) the (O) [ASR (B)]. 
Several (O) approaches (O) have (O) been (O) proposed (O) to (O) achieve (O) this (O) end, (O) including (O) a (O) method (O) that (O) transfers (O) regression (O) class (O) trees (O) from (O) triphone (O) models (O) to (O) full-context (O) models (O)   and (O) approaches (O) that (O) consider (O) triphone (O) and (O) full-context (O) models (O) using (O) a (O) shared (O) set (O) of (O) parameters. (O) 
We (O) have (O) proposed (O) the (O) [decision (B) tree (I)] marginalization (O) approach (O) that (O) takes (O) a (O) standard (O) set (O) of (O) fullcontext (O) models (O) trained (O) for (O) [TTS (B)] and (O) marginalizes (O) out (O) the (O) contexts (O) irrelevant (O) to (O) [ASR (B)] (e.g., (O) leaving (O) triphone (O) only (O) contexts) (O) (see (O) figure). (O) 
The (O) marginalized (O) models (O) can (O) then (O) be (O) used (O) to (O) estimate (O) adaptation (O) transforms (O) from (O) [ASR (B) transcriptions (I)] or (O) can (O) be (O) used (O) to (O) directly (O) perform (O) the (O) [ASR (B)], although (O) at (O) a (O) cost (O) to (O) performance. (O) 
We (O) have (O) shown (O) that (O) such (O) models (O) can (O) be (O) used (O) in (O) [unsupervised (B) adaptation (I) of (I) TTS (I)] with (O) minimal (O) impact (O) on (O) synthesis (O) quality (O) compared (O) to (O) supervised (O) adaptation (O) using (O) the (O) full-context (O) models. (O) 

Figure. (O) Illustration (O) of (O) [decision (B) tree (I)] marginalization. (O) 
In (O) the (O) example, (O) a (O) question (O) ‘ (O) Syllable_stressed (O) ?’ (O) is (O) marginalized (O) out (O) of (O) the (O) [decision (B) tree (I)]. 
Any (O) distribution (O) of (O) a (O) context (O) that (O) involves (O) traversal (O) of (O) the (O) ‘ (O) Syllable_stressed (O) ?’ (O) branch (O) will (O) become (O) the (O) weighted (O) sum (O) of (O) distributions (O) reached (O) by (O) following (O) both (O) children. (O) 

Figure. (O) Frequency (O) warping (O) with (O) bilinear (O) transform (O) (0.42 (O) approximates (O) the (O) [mel-scale (B)]). 

Further (O) details (O) can (O) be (O) found (O) in. (O) 

Bridging (O) the (O) gap (O) between (O) languages (O) 

Another (O) aspect (O) of (O) the (O) EMIME (O) project (O) concerns (O) the (O) adaptation (O) of (O) [speaker (B) identity (I)] across (O) languages (O) for (O) [HMM-based (B) TTS (I)]. 
As (O) discussed (O) earlier, (O) multi-linguality (O) for (O) [TTS (B)] has (O) previously (O) been (O) mostly (O) concerned (O) with (O) polyglot (O) synthesis. (O) 
Likewise, (O) [ASR (B)] multilingual (O) modelling (O) has (O) been (O) mostly (O) concerned (O) with (O) building (O) [acoustic (B) models (I)] that (O) can (O) recognize (O) multiple (O) languages (O) or (O) with (O) [cross-language (B) transfer (I)] in (O) mind. (O) 
Thus, (O) [cross-lingual (B) speaker (I) adaptation (I)] constitutes (O) something (O) of (O) a (O) new (O) task. (O) 
We (O) define (O) the (O) input (O) language (O) as (O) the (O) language (O) in (O) which (O) [speech (B) input (I)] is (O) provided (O) to (O) the (O) SST (O) system (O) and (O) the (O) output (O) language (O) as (O) the (O) language (O) in (O) which (O) spoken (O) output (O) is (O) generated (O) by (O) the (O) SST (O) system. (O) 
To (O) date, (O) the (O) main (O) approaches (O) that (O) have (O) been (O) considered (O) involve (O) a (O) mapping (O) from (O) models (O) / states (O) in (O) the (O) input (O) language (O) to (O) models (O) / states (O) in (O) the (O) output (O) language. (O) 
The (O) most (O) successful (O) approaches (O) have (O) involved (O) state-mappings (O) derived (O) from (O) KL-divergence (O) (KLD) (O) between (O) models (O) trained (O) on (O) input (O) and (O) output (O) [language (B) data (I)]. 
The (O) mappings (O) can (O) then (O) be (O) used (O) to (O) map (O) either (O) transforms (O) or (O) distributions (O) between (O) languages (O) (see (O) figure). (O) 
• (O) In (O) transform (O) mapping, (O) intralingual (O) adaptation (O) is (O) carried (O) out (O) in (O) the (O) input (O) language. (O) 
Generated (O) transforms (O) are (O) then (O) mapped (O) to (O) the (O) [output (B) language (I) acoustic (I) models (I)] by (O) associating (O) each (O) [HMM (B)] state (O) distribution (O) in (O) the (O) [output (B) language (I) acoustic (I) model (I)] with (O) a (O) state (O) distribu (O)     tion (O) in (O) the (O) input (O) language (O) [acoustic (B) model (I)] according (O) to (O) the (O) KLD (O) criterion. (O) 
Transforms (O) are (O) then (O) transferred (O) from (O) the (O) input (O) language (O) states (O) to (O) the (O) output (O) language (O) states (O) according (O) to (O) the (O) defined (O) mapping. (O) 
• (O) In (O) [data (B) mapping (I)], each (O) state (O) in (O) the (O) input (O) language (O) [acoustic (B) model (I)] is (O) associated (O) with (O) a (O) state (O) from (O) the (O) [output (B) language (I) acoustic (I) model (I)] according (O) to (O) KLD (O) criterion. (O) 
Output (O) language (O) states (O) are (O) then (O) substituted (O) for (O) input (O) language (O) states (O) according (O) to (O) this (O) mapping (O) and (O) intralingual (O) adaptation (O) is (O) performed. (O) 
The (O) generated (O) transforms (O) may (O) then (O) be (O) directly (O) applied (O) to (O) the (O) [output (B) language (I) acoustic (I) model (I)]. 
We (O) have (O) concentrated (O) on (O) studying (O) these (O) [HMM (B)] state (O) mapping (O) techniques (O) in (O) both (O) supervised (O) and (O) unsupervised (O) adaptation (O) (using (O) [decision (B) tree (I)] marginalization) (O) modes (O) of (O) operation. (O) 
We (O) also (O) proposed (O) an (O) alternative (O) stochastic (O) mapping (O) approach (O) that (O) uses (O) [decision (B) tree (I)] marginalization (O) on (O) the (O) [output (B) language (I) acoustic (I) models (I)] to (O) marginalize (O) out (O) all (O) contexts (O) that (O) are (O) unique (O) to (O) the (O) [output (B) language (I) acoustic (I) model (I)], such (O) that (O) the (O) resulting (O) [acoustic (B) model (I)] can (O) be (O) used (O) directly (O) on (O) the (O) input (O) language (O) (see (O) figure). (O) 
The (O) results (O) of (O) this (O) first (O) study (O) showed (O) that (O) unsupervised (O) and (O) supervised (O) adaptation (O) using (O) [maximum (B) likelihood (I)] linear (O) transformations (O) (MLLT) (O) gave (O) similar (O) performance (O) as (O) in (O) the (O) intralingual (O) adaptation (O) scenario. (O) 
This (O) was (O) a (O) good (O) result (O) as (O) it (O) demonstrated (O) the (O) robustness (O) of (O) adaptation (O) algorithms (O) vis-à-vis (O) unsupervised (O) and (O) [cross-lingual (B) scenarios (I)]. 
We (O) also (O) showed (O) that (O) the (O) language (O) of (O) the (O) reference (O) [speech (B)] plays (O) an (O) important (O) role (O) in (O) people’s (O) ability (O) to (O) evaluate (O) [speaker (B) similarity (I)], a (O) topic (O) which (O) is (O) now (O) under (O) investigation. (O) 
Finally, (O) of (O) the (O) different (O) [cross-lingual (B) adaptation (I) approaches (I)] that (O) were (O) investigated, (O) it (O) was (O) apparent (O) that (O) those (O) using (O) the (O) [HMM (B)] state (O) emission (O) distributions (O) of (O) the (O) [output (B) language (I) acoustic (I) model (I)] for (O) transform (O) estimation (O) were (O) preferred (O) over (O) the (O) transform (O) mapping (O) approach. (O) 
In (O) light (O) of (O) this (O) final (O) observation, (O) we (O) performed (O) further (O) analysis (O) to (O) ascertain (O) where (O) the (O) state (O) mapping (O) techniques (O) may (O) still (O) be (O) inferior (O) to (O) intralingual (O) approaches (O) specifically (O) studying (O) the (O) influence (O) of (O) language (O) mismatch (O) during (O) adaptation (O) and (O) synthesis. (O) 
We (O) analysed (O) adaptation (O) performance (O) with (O) respect (O) to (O) differing (O) number (O) of (O) transforms (O) and (O) amount (O) of (O) [adaptation (B) data (I)], from (O) which (O) it (O) became (O) clear (O) that (O) [cross-lingual (B) approaches (I)] were (O) not (O) effectively (O) able (O) to (O) benefit (O) from (O) the (O) availability (O) of (O) larger (O) quantities (O) of (O) [adaptation (B) data (I)]. 
In (O) particular, (O) the (O) [aforementioned (B) data (I)] mapping (O) approach (O) was (O) more (O) susceptible (O) to (O) take (O) on (O) characteristics (O) associated (O) with (O) the (O) input (O) language (O) rather (O) than (O) [input (B) speaker (I)], resulting (O) in (O) increasingly (O) distorted (O) [synthesized (B) speech (I)] in (O) the (O) output (O) language (O) as (O) the (O) number (O) of (O) adaptation (O) transforms (O) was (O) increased (O) (see (O) figure). (O) 
This (O) behaviour (O) was (O) attributed (O) to (O) the (O) inherent (O) mismatch (O) between (O) phones (O) in (O) the (O) source (O) and (O) [target (B) languages (I)] and (O) has (O) made (O) it (O) clear (O) that (O) in (O) order (O) to (O) further (O) improve (O) [cross-lingual (B) adaptation (I)], we (O) will (O) need (O) to (O) counter (O) this (O) mismatch (O) during (O) adaptation. (O) 

Figure. (O) State (O) mapping (O) approaches (O) in (O) which (O) state (O) emission (O) pdfs (O) from (O) [acoustic (B) models (I)] for (O) the (O) input (O) and (O) output (O) languages (O) are (O) associated (O) with (O) each (O) another. (O) 
(a) (O) State (O) mapping (O) using (O) [data (B) mapping (I)] (shown) (O) finds (O) a (O) mapping (O) from (O) a (O) state (O) in (O) the (O) [output (B) language (I) acoustic (I) model (I)] to (O) each (O) state (O) in (O) the (O) input (O) language (O) [acoustic (B) model (I)] according (O) to (O) minimum (O) KLD. (O) 
Transform (O) mapping (O) (not (O) shown) (O) uses (O) the (O) same (O) mapping (O) principle, (O) but (O) in (O) the (O) opposite (O) direction, (O) from (O) input (O) language (O) to (O) output (O) language. (O) 
(b) (O) State (O) mapping (O) using (O) [decision (B) tree (I)] marginalization (O) represents (O) state (O) distributions (O) in (O) the (O) input (O) language (O) as (O) a (O) mixture (O) of (O) state (O) distributions (O) of (O) the (O) output (O) [acoustic (B) model (I)]. 
                                 
Figure. (O) Comparison (O) of (O) objective (O) [speech (B) synthesis (I) results (I)] for (O) intralingual (O) and (O) [cross-lingual (B) adaptation (I)]. 
Intralingual (O) adaptation (O) experiments (O) show (O) consistent (O) reduction (O) in (O) distortion (O) as (O) a (O) greater (O) number (O) of (O) transforms (O) are (O) used, (O) whereas (O) [cross-lingual (B) adaptation (I) experiments (I)] show (O) the (O) converse. (O) This (O) increase (O) in (O) distortion (O) as (O) the (O) number (O) of (O) transforms (O) increases (O) is (O) attributed (O) to (O) language (O) mismatch (O) between (O) models (O) and (O) transforms. (O) 

Summary (O) 

We (O) have (O) presented (O) work (O) being (O) conducted (O) at (O) Idiap (O) as (O) part (O) of (O) the (O) EMIME (O) [speech-to-speech (B) translation (I)] project. (O) 
In (O) this (O) work (O) we (O) are (O) not (O) only (O) having (O) to (O) deal (O) with (O) the (O) challenges (O) of (O) multilingual (O) modelling (O) for (O) [ASR (B)] and (O) [TTS (B)], but (O) are (O) addressing (O) more (O) fundamental (O) issues (O) concerning (O) unified (O) modelling. (O) 
Our (O) results (O) so (O) far (O) indicate (O) that (O) direct (O) attempts (O) at (O) unified (O) modelling (O) do (O) not (O) necessarily (O) present (O) themselves (O) as (O) a (O) realistic (O) alternative (O) to (O) separate (O) [ASR (B)] and (O) [TTS (B)] modelling, (O) but (O) several (O) promising (O) directions (O) have (O) emerged (O) when (O) unified (O) modelling (O) has (O) been (O) considered (O) as (O) a (O) means (O) to (O) port (O) knowledge (O) and (O) approaches (O) between (O) [ASR (B)] and (O) [TTS (B)]. 
Furthermore, (O) our (O) efforts (O) towards (O) [unsupervised (B) cross-lingual (I) adaptation (I)] have (O) shown (O) that (O) good (O) results (O) can (O) be (O) achieved (O) by (O) using (O) largely (O) conventional (O) approaches. (O) 

[MLP (B) features (I)] and (O) [multilingual (B) ASR (I)] 

There (O) have (O) been (O) shortcomings (O) in (O) the (O) current (O) approaches (O) but (O) efforts (O) are (O) underway (O) to (O) overcome (O) these (O) issues. (O) 
There (O) has (O) been (O) sustained (O) interest (O) in (O) using (O) [MLP (B)]-based [discriminative (B) features (I)] for (O) [automatic (B) speech (I) recognition (I)] for (O) several (O) reasons. (O) 
(i) (O) Discriminative (O) nature (O) of (O) the (O) features (O)  
(ii) (O) Ability (O) of (O) [MLP (B)] to (O) handle (O) different (O) types (O) of (O) features (O) and (O) long (O) temporal (O) context (O) at (O) the (O) input. (O) 
(iii) (O) Robustness (O) towards (O) speaker (O)   and (O) environmental (O) variation (O)   (iv) (O) Ability (O) to (O) combine (O) [multiple (B) feature (I)] streams (O) at (O) [MLP (B) output (I)] level (O) using (O) probabilistic (O) methods (O) (v) (O) Performance (O) improvements (O) obtained (O) are (O) scalable (O) across (O) different (O) training (O) criteria, (O) as (O) well (O) as (O) with (O) amount (O) of (O) data. (O) 
Figure (O)   depicts (O) a (O) typical (O) [MLP (B) feature (I)]-based [ASR (B) system (I)]. 
The (O) main (O) components (O) of (O) [MLP (B) feature (I)] extraction (O) are (O) (i) (O) an (O) [MLP (B)] trained (O) to (O) classify (O) [phonemes (B)] / phones (O) and (O) (ii) (O) Karhunen (O) Loeve (O) transformation (O) (KLT) (O) matrix (O) (estimated (O) on (O) a (O) data (O) other (O) than (O) [test (B) data (I)]) to (O) decorrelate (O) the (O) [feature (B) vectors (I)]. 
Optionally, (O) during (O) KLT (O) dimensionality, (O) reduction (O) could (O) be (O) done. (O) 
This (O) helps (O) in (O) controlling (O) the (O) dimensionality (O) of (O) the (O) [feature (B) space (I)], especially (O) when (O) the (O) [MLP (B) features (I)] are (O) concatenated (O) with (O) the (O) standard (O) [spectral (B) features (I)]. 
Traditionally, (O) a (O) [HMM (B)] / [GMM (B) system (I)] models (O) [acoustic (B) features (I)] that (O) are (O) extracted (O) from (O) shortterm (O) [spectrum (B)] (usually (O) 20–30 (O) ms) (O) of (O) [speech (B) signal (I)]. 
The (O) extraction (O) of (O) these (O) [acoustic (B) features (I)] is (O) assumed (O) to (O) be (O) [language-independent (B)]. 
When (O) compared (O) to (O) [GMM-based (B) modelling (I)], [MLPs (B)] have (O) the (O) capability (O) to (O) model (O) higher (O) [dimensional (B) feature (I) vector (I)] (e.g., (O) standard (O) [spectral (B) features (I)] with (O) temporal (O) context). (O) 
Furthermore, (O) [MLPs (B)] avoid (O) the (O) need (O) to (O) make (O) assumption (O) about (O) parametric (O) distribution (O) of (O) [input (B) features (I)]. 
As (O) a (O) result, (O) the (O) use (O) of (O) [MLP (B) features (I)] has (O) led (O) to (O) exploration (O) of (O) spectro-temporal (O) [acoustic (B) features (I)], i.e., (O) features (O) that (O) span (O) across (O) and (O) characterize (O) both (O) time (O) and (O) frequency. (O) 
The (O) time (O) span (O) or (O) temporal (O) context (O) in (O) this (O) case (O) can (O) vary (O) from (O) about (O) 250 (O) ms (O) to (O) 1 (O) s, (O) which (O) is (O) about (O) the (O) duration (O) of (O) a (O) syllable. (O) 
In (O) literature, (O) in (O) the (O) context (O) of (O) [MLP (B) feature (I)], it (O) has (O) been (O) found (O) that (O) a (O) combination (O) of (O) spectro-[temporal (B) feature (I)] processing (O) and (O) conventional (O) [spectral (B) processing (I)] can (O) lead (O) to (O) a (O) better (O) [ASR (B) system (I)]. 
However, (O) as (O) the (O) acoustic (O) – (O) [phonetic (B)] relationship (O) can (O) differ (O) across (O) languages, (O) one (O) may (O) ask (O) if (O) such (O) spectro-temporal (O) [speech (B) processing (I) techniques (I)] can (O) also (O) be (O) considered (O) languageindependent. (O) 
Along (O) this (O) line, (O) we (O) present (O) [ASR (B) studies (I)] using (O) hierarchical (O) [MRASTA (B) features (I)] for (O) two (O) different (O) languages (O) in (O) section. (O) 
An (O) interesting (O) aspect (O) of (O) [MLP (B) feature (I)] extraction (O) is (O) that (O) the (O) [MLP (B)] can (O) be (O) trained (O) on (O) the (O) data (O) that (O) is (O) different (O) from (O) the (O) domain (O) of (O) the (O) task, (O) while (O) still (O) yielding (O) good (O) generalization (O) performance. (O) 
In (O) the (O) context (O) of (O) multilingual (O) processing, (O) this (O) aspect (O) can (O) be (O) effectively (O) used (O) by (O) training (O) the (O) [MLP (B)] on (O) a (O) language (O) which (O) has (O) more (O) resources (O) (in (O) terms (O) of (O) data), (O) and (O) using (O) the (O) [MLP (B)] for (O) languages (O) that (O) have (O) fewer (O) or (O) no (O) resources. (O) 
This (O) is (O) similar (O) to (O) [cross-lingual (B) transfer (I)] using (O) [acoustic (B) models (I)] (discussed (O) earlier (O) in (O) section), (O) except (O) that (O) [cross-lingual (B) transfer (I)] here (O) is (O) achieved (O) at (O) [feature (B) extraction (I)] level. (O) 
Section (O)   presents (O) a (O) study (O) on (O) [cross-lingual (B) transfer (I)] using (O) [MLP (B) features (I)]. 
Similar (O) to (O) multilingual (O) acoustic (O) modelling, (O) the (O) [MLP (B)] can (O) be (O) trained (O) with (O) data (O) from (O) different (O) languages (O) to (O) classify (O) a (O) ‘ (O) universal (O) / global’ (O) [phone (B) set (I)]. 
By (O) sharing (O) data (O) from (O) different (O) languages, (O) such (O) an (O) approach (O) not (O) only (O) helps (O) in (O) handling (O) [data (B) issues (I)] related (O) to (O) [multilingual (B) ASR (I)], but (O) could (O) also (O) help (O) in (O) extracting (O) [MLP (B) features (I)] that (O) yield (O) a (O) better (O) [multilingual (B) ASR (I) system (I)]. 
We (O) present (O) one (O) such (O) recent (O) study (O) in (O) section. (O) 

Figure. (O) Block (O) diagram (O) of (O) [ASR (B) system (I)] based (O) on (O) stand-alone (O) [MLP (B) features (I)]. 
X (O) = { x1, (O) · (O) · (O) · (O) xn, (O) · (O) · (O) · (O) x (O) N (O) } represents (O) [acoustic (B) feature (I)] sequence (O) of (O) length (O) N. (O) 
W (O) represents (O) the (O) recognized (O) word (O) sequence. (O) P(qn (O) = i|xn) (O) represents (O) the (O) a (O) posteriori (O) probability (O) of (O) phone (O) class (O) i (O) ∈ (O) { 1, (O) · (O) · (O) · (O) I (O) } estimated (O) by (O) [MLP (B)] at (O) time (O) frame (O) n (O) given (O) [acoustic (B) feature (I) vector (I)] xn, (O) where (O) I (O) is (O) the (O) number (O) of (O) phone (O) classes (O) or (O) output (O) units (O) of (O) [MLP (B)]. 
KL (O) transform (O) refers (O) to (O) Karhunen (O) Loeve (O) transform, (O) which (O) can (O) be (O) either (O) applied (O) to (O) the (O) log (O) of (O) the (O) [MLP (B) output (I) vectors (I)] or (O) (more (O) or (O) less (O) equivalently) (O) to (O) the (O) [MLP (B) output (I)] values (O) before (O) the (O) nonlinear (O) ([sigmoid (B) / (I) softmax) (I) function (I)]. 

[MLP (B) features (I)] 

In (O) this (O) section, (O) we (O) present (O) a (O) study (O) using (O) multi-resolution (O) RASTA (O) ([MRASTA (B)]) feature (O) and (O) hiearchical (O) [MRASTA (B) feature (I)] ([hier-MRASTA (B)]). 
These (O) features (O) were (O) first (O) investigated (O) for (O) English (O) language (O) [ASR (B) system (I)], and (O) then (O) extended (O) to (O) Mandarin (O) language (O) [ASR (B) system (I)]. 
These (O) studies (O) were (O) originally (O) conducted (O) as (O) part (O) of (O) the (O) DARPA (O) [GALE (B)] project. (O) 
More (O) description (O) and (O) details (O) can (O) be (O) found (O) in. (O) 
In (O) [MRASTA (B) feature (I)] extraction, (O) first (O) critical (O) band (O) auditory (O) [spectrum (B)] is (O) extracted (O) through (O) short-term (O) analysis (O) of (O) the (O) [speech (B) signal (I)]. 
The (O) number (O) of (O) critical (O) bands (O) depends (O) upon (O) the (O) bandwidth (O) of (O) the (O) [speech (B) signal (I)]. 
On (O) Bark (O) scale, (O) there (O) are (O) 15 (O) and (O) 19 (O) critical (O) bands (O) for (O) [speech (B) signal (I)] of (O) bandwidth (O) 4 (O) kHz (O) and (O) 8 (O) kHz, (O) respectively. (O) 
A (O) 600 (O) ms (O) long (O) temporal (O) trajectory (O) of (O) each (O) critical (O) band (O) auditory (O) [spectrum (B)] is (O) then (O) filtered (O) by (O) a (O) bank (O) of (O) filters, (O) also (O) referred (O) to (O) as (O) [MRASTA (B) filters (I)]. 
The (O) [MRASTA (B) filters (I)] are (O) firstand (O) second-order (O) derivatives (O) of (O) [Gaussian (B) filters (I)] with (O) different (O) variance (O) / time (O) width. (O) 
In (O) essence, (O) [MRASTA (B) filters (I)] are (O) multi-resolution (O) bandpass (O) filters (O) on (O) modulation (O) frequency. (O) 
Finally, (O) approximate (O) derivatives (O) across (O) three (O) consecutive (O) critical (O) bands (O) are (O) computed. (O) 
An (O) [MLP (B)] is (O) then (O) trained (O) to (O) classify (O) phones (O) / [phonemes (B)] using (O) these (O) features (O) as (O) input. (O) 

Table. (O) Comparing (O) stand-alone (O) [MLP (B) feature (I) hier-MRASTA (I)], [MLP (B) feature (I) MRASTA (I)], and (O) standard (O) [cepstral (B) features (I)] across (O) two (O) different (O) languages, (O) namely, (O) English (O) and (O) Mandarin. (O) 
The (O) performance (O) of (O) English (O) [ASR (B) system (I)] is (O) expressed (O) in (O) terms (O) of (O) [WER (B)] on (O) NIST (O) RT05 (O) [evaluation (B) data (I)], whereas, (O) the (O) performance (O) of (O) Mandarin (O) [ASR (B) system (I)] is (O) expressed (O) in (O) terms (O) of (O) CER (O) on (O) [GALE (B)] Mandarin (O) 2006 (O) [evaluation (B) data (I)]. 

In (O) hierarchical (O) [MRASTA (B) feature (I)] extraction (O) instead (O) of (O) training (O) a (O) single (O) [MLP (B)], the (O) filter (O) banks (O) are (O) split (O) into (O) two (O) parts. (O) 
The (O) first (O) part (O) extracting (O) high (O) modulation (O) frequencies (O) (above (O) 10 (O) Hz), (O) and (O) the (O) second (O) part (O) extracting (O) low (O) modulation (O) frequencies (O) (below (O) 10 (O) Hz). (O) 
The (O) higher (O) and (O) lower (O) modulation (O) frequencies (O) are (O) then (O) processed (O) in (O) a (O) sequential (O) fashion (O) using (O) a (O) hierarchy (O) of (O) [MLPs (B)]. 
More (O) specifically, (O) the (O) first (O) stage (O) [MLP (B) processes (I)] high (O) modulation (O) frequencies, (O) and (O) the (O) second (O) stage (O) [MLP (B)] jointly (O) processes (O) low (O) modulation (O) frequencies (O) along (O) with (O) [MLP (B) features (I)] extracted (O) using (O) first (O) stage (O) [MLP (B)]. 
For (O) details, (O) the (O) reader (O) is (O) referred (O) to. (O) 
The (O) [MRASTA (B)] and (O) [hier-MRASTA (B) features (I)] were (O) first (O) studied (O) for (O) English (O) language. (O) 
The (O) [training (B) data (I)] consisted (O) of (O) 112 (O) hours (O) of (O) [meeting (B) data (I)] from (O) different (O) sites. (O) 
Thirty-nine-dimensional (O) [PLP (B) cepstral (I) features (I)] consisting (O) of (O) 13 (O) static (O) coefficients, (O) their (O) approximate (O) first-order (O) and (O) second-order (O) derivatives (O) was (O) used (O) as (O) [baseline (B) feature (I)]. 
The (O) [MLP (B)] of (O) [MRASTA (B) feature (I)] extractor (O) and (O) [MLPs (B)] of (O) [hier-MRASTA (B) feature (I) extractor (I)] were (O) trained (O) to (O) classify (O) 45 (O) [context-independent (B) phonemes (I)]. 
The (O) [HMM (B)] / [GMM (B) system (I)] was (O) trained (O) using (O) HTK (O) with (O) [maximum (B) likelihood (I)] criterion. (O) 
The (O) [ASR (B) systems (I)] were (O) tested (O) using (O) NIST (O) RT05 (O) [evaluation (B) data (I)]. 
The (O) resulting (O) [WER (B)] for (O) the (O) systems (O) using (O) [PLP (B) feature (I)], [MRASTA (B) feature (I)] and (O) [hier-MRASTA (B) feature (I)] are (O) shown (O) in (O) table. (O) 
For (O) Mandarin (O) language (O) [ASR (B) studies (I)], we (O) used (O) 100 (O) hours (O) [training (B) data (I)] setup (O) consisting (O) of (O) broadcast (O) news (O) and (O) broadcast (O) [conversation (B) data (I)]. 
The (O) [spectral (B) feature (I)] baseline (O) system (O) was (O) trained (O) with (O) 39-dimensional (O) MFCC15 (O) [feature (B) vector (I)] consisting (O) of (O) 13 (O) static (O) coefficients (O) (extracted (O) after (O) vocal (O) tract (O) length (O) normalization), (O) their (O) approximate (O) first-order (O) and (O) second-order (O) time (O) derivatives. (O) 
The (O) [MRASTA (B) MLP (I)] and (O) [hier-MRASTA (B) MLPs (I)] were (O) trained (O) to (O) classify (O) 71 (O) contextindependent (O) [phonemes (B)] (with (O) tone). (O) 
During (O) [MLP (B) feature (I)] extraction (O) (i.e., (O) during (O) KLT), (O) the (O) dimension (O) of (O) [MLP (B) feature (I)] was (O) reduced (O) to (O) 35. (O) 
The (O) studies (O) were (O) conducted (O) using (O) SRI (O) / UW (O) / ICSI (O) Mandarin (O) system (O) with (O) [maximum (B) likelihood (I) training (I)] criteria. (O) 
The (O) [ASR (B) systems (I)] were (O) tested (O) using (O) [GALE (B)] Mandarin (O) 2006 (O) [evaluation (B) data (I)]. 
Table (O) 1 (O) presents (O) the (O) the (O) performance (O) of (O) the (O) three (O) systems (O) in (O) terms (O) of (O) CER. (O) 
It (O) can (O) be (O) observed (O) that (O) [MRASTA (B) features (I)] consistently (O) yield (O) the (O) lowest (O) recognition (O) performance (O) across (O) both (O) the (O) languages, (O) while (O) [hier-MRASTA (B) features (I)] consistently (O) yield (O) a (O) better (O) system (O) compared (O) to (O) the (O) standard (O) [spectral (B) feature (I)] PLP (O) (in (O) the (O) case (O) of (O) English) (O) and (O) MFCC (O) (in (O) the (O) case (O) of (O) Mandarin). (O) 
These (O) results (O) tend (O) to (O) show (O) that (O) the (O) trends (O) of (O) [MLP (B) features (I) MRASTA (I)] and (O) [hier-MRASTA (B)], which (O) span (O) longer (O) temporal (O) context, (O) can (O) generalize (O) across (O) languages. (O) 
The (O) observation (O) about (O) generalization (O) to (O) other (O) languages (O) is (O) further (O) supported (O) by (O) studies (O) reported (O) in (O)   the (O) literature (O) for (O) languages (O) such (O) as (O) English, (O) Mandarin (O) and (O) Arabic (O) ; where (O) it (O) has (O) been (O) observed (O) that (O) [MLP (B) features (I)] (including (O) [MRASTA (B)], [hier-MRASTA (B)], and (O) similar (O) processing (O) techniques) (O) are (O) complementary (O) to (O) standard (O) short-term (O) [spectral (B)]-based features. (O) 
In (O) other (O) words, (O) a (O) system (O) trained (O) with (O) standard (O) [cepstral (B) features (I)] concatenated (O) with (O) [MLP (B) features (I)] consistently (O) yield (O) a (O) better (O) performance (O) than (O) a (O) system (O) trained (O) with (O) only (O) standard (O) [cepstral (B) feature (I)]. 
It (O) has (O) to (O) be (O) noted (O) that (O) while (O) [multilingual (B) ASR (I) systems (I)] aim (O) to (O) use (O) the (O) [feature (B) set (I)] that (O) is (O) most (O) [language-independent (B)], this (O) [feature (B) set (I)] may (O) not (O) be (O) the (O) best (O) set (O) for (O) an (O) individual (O) language, (O) i.e., (O) there (O) may (O) be (O) features (O) that (O) are (O) more (O) specific (O) to (O) a (O) language (O) or (O) more (O) specifically (O) applicable (O) to (O) a (O) language. (O) 
For (O) instance, (O) Mandarin (O) is (O) a (O) tonal (O) language (O) ; in (O) this (O) case, (O) it (O) has (O) been (O) observed (O) that (O) using (O) [pitch (B) frequency (I) features (I)] in (O) addition (O) to (O) [cepstral (B) feature (I)] usually (O) leads (O) to (O) better (O) performance. (O) 

In, (O) the (O) baseline (O) system (O) was (O) trained (O) with (O) 42-dimensional (O) [feature (B) vector (I)] consisting (O) of (O) 39-dimensional (O) [MFCC (B) features (I)] and (O) log (O) [pitch (B)] frequency (O) and (O) its (O) approximate (O) first (O) and (O) second-time (O) derivatives. (O) We (O) dropped (O) the (O) log (O) [pitch (B) frequency (I) features (I)] as (O) English (O) study (O) did (O) not (O) use (O) these (O) features. (O) 

[Cross-lingual (B) MLP (I) features (I)] 

[MLP (B) features (I)] are (O) extracted (O) by (O) projecting (O) [spectral (B) features (I)] along (O) linguistic (O) dimensions, (O) while (O) the (O) projection (O) is (O) ‘ (O) trained (O) / learned’ (O) from (O) data. (O) 
Given (O) this, (O) they (O) can (O) be (O) applied (O) to (O) transfer (O) knowledge (O) across (O) domains (O) or (O) languages, (O) especially (O) for (O) target (O) domains (O) or (O) languages (O) where (O) less (O) amount (O) of (O) data (O) or (O) no (O) data (O) is (O) available. (O) 
In (O) this (O) section, (O) we (O) present (O) a (O) [cross-lingual (B) feature (I) study (I)], where (O) the (O) [MLP (B)] is (O) trained (O) on (O) one (O) language (O) and (O) used (O) for (O) [feature (B) extraction (I)] in (O) another (O) language. (O) 
This (O) study (O) was (O) originally (O) conducted (O) as (O) an (O) extension (O) of (O) JHU (O) WS0616, (O) and (O) as (O) well (O) as (O) part (O) of (O) DARPA (O) [GALE (B)] project. (O) 
The (O) study (O) was (O) conducted (O) on (O) Mandarin (O) language. (O) 
The (O) [training (B) data (I)] consisted (O) of (O) 97 (O) hours (O) of (O) broadcast (O) news, (O) specifically (O) LDC (O) Mandarin (O) Hub4 (O) and (O) TDT4. (O) 
The (O) [GALE (B)] 2004 (O) Mandarin (O) rich (O) transcription (O) development (O) and (O) evaluation (O) sets (O) were (O) used (O) for (O) tuning (O) and (O) evaluating (O) the (O) system, (O) respectively. (O) 
A (O) [monolingual (B) MLP (I)] was (O) trained (O) on (O) the (O) [Mandarin (B) data (I)] to (O) classify (O) 65 (O) Mandarin (O) phones (O) (with (O) tone). (O) 
A (O) [cross-lingual (B) MLP (I)] was (O) trained (O) on (O) 2000 (O) hours (O) of (O) conversational (O) telephone (O) [speech (B) data (I)] of (O) English (O) language (O) to (O) classify (O) 46 (O) [context-independent (B) phones (I)]. 
For (O) both (O) [MLPs (B)], the (O) [spectral (B) features (I)] used (O) were (O) 39-dimensional (O) PLP (O) [cepstral (B) coefficients (I)] with (O) nine-frame (O) temporal (O) context. (O) 
For (O) more (O) details (O) about (O) the (O) experiment, (O) the (O) reader (O) is (O) referred (O) to. (O) 
Table (O) shows (O) the (O) performance (O) of (O) three (O) systems, (O) (i) (O) using (O) only (O) [MFCC (B) features (I)], (ii) (O) using (O) [MFCC (B) features (I)] appended (O) with (O) [tandem (B) features (I)] extracted (O) from (O) the (O) Mandarin (O) [MLP (B)] (referred (O) to (O) as (O) monolingual (O) tandem), (O) and (O) (iii) (O) using (O) [MFCC (B) feature (I)] appended (O) with (O) [tandem (B) feature (I)] extracted (O) from (O) the (O) English (O) [MLP (B)] (referred (O) to (O) as (O) [cross-lingual (B) tandem (I)]). 
It (O) can (O) be (O) observed (O) that (O) both (O) monolingual (O) and (O) [cross-lingual (B) MLP (I) features (I)] in (O) concatenation (O) with (O) [MFCC (B) feature (I)] lead (O) to (O) improvement (O) in (O) the (O) [ASR (B) performance (I)]. 
It (O) can (O) also (O) be (O) noted (O) that (O) the (O) improvement (O) using (O) crosslingual (O) [MLP (B) features (I)] is (O) not (O) as (O) significant (O) as (O) [monolingual (B) MLP (I) features (I)]. 
This (O) could (O) be (O) because (O) Mandarin (O) and (O) English (O) are (O) very (O) different (O) languages, (O) i.e., (O) English (O) [phonetic (B)] space (O) may (O) not (O) represent (O) well (O) the (O) Mandarin (O) [phonetic (B)] space. (O) 
Further, (O) it (O) should (O) be (O) noted (O) that (O) the (O) Mandarin (O) [MLP (B)] was (O) trained (O) with (O) [speech (B) signal (I)] of (O) bandwidth (O) 8 (O) kHz (O) whereas (O) the (O) English (O) [MLP (B)] was (O) trained (O) with (O) [speech (B) signal (I)] of (O) bandwidth (O) 4 (O) kHz. (O) 
Nevertheless, (O) these (O) results (O) suggest (O) that (O) through (O) [MLP (B) features (I)], [speech (B) data (I)] of (O) other (O) languages (O) could (O) be (O) effectively (O) utilized (O) to (O) improve (O) [ASR (B) performance (I)] of (O) another (O) language. (O) 
In (O) the (O) literature, (O) similar (O) [cross-lingual (B) studies (I)] have (O) been (O) reported. (O) 
In, (O) it (O) was (O) shown (O) that (O) using (O) [MLP (B)]-based features (O) extracted (O) from (O) English-trained (O) [MLP (B)] could (O) improve (O) Mandarin (O) and (O) Arabic (O) [ASR (B) performance (I)] over (O) the (O) [spectral (B) feature (I)] baseline (O) system. (O) 

Table. (O) Performance (O) of (O) Mandarin (O) [ASR (B) systems (I)] investigated (O) in (O) the (O) [cross-lingual (B) feature (I) study (I)]. Monolingual (O) tandem (O) refers (O) to (O) [MLP (B) feature (I)] extracted (O) using (O) [MLP (B)] trained (O) on (O) [Mandarin (B) data (I)]. [Cross-lingual (B) tandem (I)] refers (O) to (O) [MLP (B) feature (I)] extracted (O) using (O) [MLP (B)] trained (O) on (O) [English (B) data (I)]. The (O) performance (O) is (O) measured (O) in (O) terms (O) of (O) CER. (O) 

In (O) a (O) more (O) recent (O) study, (O) [cross-lingual (B) portability (I)] of (O) [MLP (B) features (I)] from (O) English (O) language (O) to (O) Hungarian (O) language (O) was (O) investigated (O) by (O) using (O) English-trained (O) phone (O) and (O) [articulatory (B) feature (I) MLPs (I)]. 
In (O) addition, (O) a (O) [cross-lingual (B) MLP (I) adaptation (I) approach (I)] was (O) investigated (O) where (O) the (O) input-to-hidden (O) weights (O) and (O) hidden (O) biases (O) of (O) the (O) [MLP (B)] corresponding (O) to (O) Hungarian (O) language (O) were (O) initialized (O) by (O) English-trained (O) [MLP (B)] weights, (O) while (O) the (O) hidden-to-output (O) weights (O) and (O) output (O) biases (O) were (O) initialized (O) randomly. (O) 
The (O) [MLP (B)] was (O) then (O) trained (O) on (O) [Hungarian (B) data (I)] to (O) classify (O) Hungarian (O) [context-independent (B) phones (I)]. 
It (O) has (O) to (O) be (O) noted (O) that (O) in (O) essence (O) this (O) [cross-lingual (B) MLP (I) adaptation (I) approach (I)] is (O) similar (O) to (O) the (O) regularization (O) approach (O) proposed (O) for (O) [MLP (B)], where (O) the (O) input-to-hidden (O) mapping (O) is (O) kept (O) intact (O) and (O) the (O) hidden-to-output (O) mapping (O) is (O) relearned. (O) 
The (O) [ASR (B) studies (I)] showed (O) that (O) [cross-lingual (B) adaptation (I) approach (I)] often (O) yields (O) the (O) best (O) system (O) even (O) when (O) compared (O) to (O) the (O) case (O) where (O) the (O) [MLP (B) feature (I)] is (O) extracted (O) using (O) [monolingual (B) MLP (I)] (i.e., (O) trained (O) only (O) on (O) [Hungarian (B) data (I)]). 
Although, (O) the (O) studies (O) on (O) [cross-lingual (B) MLP (I) features (I)] are (O) limited, (O) it (O) has (O) been (O) typically (O) found (O) (including (O) the (O) study (O) presented (O) here) (O) that (O) using (O) [MLPs (B)] trained (O) on (O) a (O) different (O) language (O) ‘ (O) directly’ (O) may (O) not (O) yield (O) a (O) system (O) better (O) than (O) [MLP (B)] trained (O) on (O) [target (B) language (I) data (I)] (if (O) available). (O) 
In (O) other (O) words, (O) to (O) make (O) better (O) use (O) of (O) the (O) [MLPs (B)] trained (O) on (O) a (O) different (O) language, (O) [cross-lingual (B) adaptation (I)] or (O) some (O) kind (O) of (O) training (O) on (O) the (O) [target (B) language (I)] may (O) be (O) necessary. (O) 
The (O) [cross-lingual (B) adaptation (I) approach (I)] discussed (O) earlier (O) is (O) one (O) way (O) this (O) could (O) be (O) achieved. (O) 
Another (O) way (O) would (O) be (O) to (O) use (O) the (O) recently (O) proposed (O) hierarchical (O) [MLP (B)]-based phone (O) posterior (O) estimation (O) approach, (O) where (O) two (O) stages (O) (hierarchy) (O) of (O) [MLPs (B)] are (O) trained (O) to (O) classify (O) [context-independent (B) phones (I)]. 
In (O) the (O) first (O) stage, (O) the (O) [input (B) feature (I)] to (O) the (O) [MLPs (B)] is (O) a (O) standard (O) [spectral (B)]-based feature. (O) 
The (O) input (O) to (O) the (O) second (O) stage (O) [MLP (B)] is (O) [phone (B) posterior (I) probabilities (I)] estimated (O) by (O) the (O) first (O) stage (O) [MLP (B)] with (O) temporal (O) context (O) of (O) around (O) 150–230 (O) ms. (O) 
This (O) approach (O) has (O) been (O) shown (O) to (O) yield (O) better (O) [phoneme (B) recognition (I)] performance (O) as (O) well (O) as (O) [ASR (B) performance (I)] than (O) the (O) single (O) [MLP (B)]-based approach. (O) 
In (O) the (O) context (O) of (O) [cross-lingual (B) adaptation (I)], the (O) first (O) [MLP (B)] can (O) be (O) trained (O) on (O) a (O) resource (O) rich (O) language(s) (O) and (O) the (O) second (O) [MLP (B)] can (O) be (O) trained (O) on (O) the (O) [target (B) language (I)] with (O) the (O) [available (B) data (I)]. 

[Multilingual (B) MLP (I) features (I)] 

[Cross-lingual (B) MLP (I) feature (I) extraction (I)] considers (O) training (O) the (O) [MLP (B)] on (O) a (O) secondary (O) language (O) that (O) has (O) more (O) resources. (O) 
In (O) the (O) context (O) of (O) [multilingual (B) speech (I) recognition (I)], it (O) is (O) possible (O) to (O) consider (O) an (O) [MLP (B)] trained (O) to (O) classify (O) a (O) universal (O) / global (O) [phone (B) set (I)] (instead (O) of (O) [phone (B) set (I)] belonging (O) to (O) a (O) particular (O) language) (O) using (O) data (O) from (O) different (O) languages. (O) 
Similar (O) to (O) the (O) case (O) of (O) multilingual (O) acoustic (O) modelling, (O) it (O) can (O) be (O) expected (O) that (O) such (O) an (O) approach (O) can (O) help (O) in (O) sharing (O) data (O) from (O) different (O) languages, (O) and (O) can (O) also (O) yield (O) a (O) compact (O) and (O) better (O) [multilingual (B) ASR (I) system (I)]. 
In (O) this (O) case, (O) we (O) refer (O) to (O) the (O) [MLP (B)] as (O) a (O) [multilingual (B) MLP (I)], and (O) the (O) resulting (O) features (O) as (O) [multilingual (B) MLP (I) features (I)]. 
In (O) a (O) preliminary (O) study, (O) we (O) investigated (O) the (O) [multilingual (B) MLP (I) features (I)] on (O) five (O) European (O) languages, (O) namely, (O) English, (O) Italian, (O) Spanish, (O) Swiss (O) French, (O) and (O) Swiss (O) German (O) from (O) the (O) SpeechDat(II) (O) corpus (O) (Höge (O) et (O) al (O) 1999). (O) 
The (O) data (O) corresponding (O) to (O) the (O) isolated (O) / application (O) words (O) was (O) used (O) for (O) this (O) study. (O) 
Table (O)   shows (O) the (O) [data (B) distribution (I)] for (O) different (O) languages. (O) 
We (O) used (O) the (O) dictionary (O) (based (O) on (O) the (O) [SAMPA (B) phone (I) set (I)]) provided (O) along (O) with (O) the (O) database. (O) 
Table (O)   shows (O) the (O) number (O) of (O) [context-independent (B) phones (I)], and (O) the (O) number (O) of (O) application (O) words (O) (size (O) of (O) [lexicon (B)]) for (O) each (O) language. (O) 
We (O) trained (O) a (O) [monolingual (B) MLP (I)] corresponding (O) to (O) each (O) language (O) classifying (O) their (O) respective (O) [context-independent (B) phones (I)]. 
We (O) adopted (O) the (O) knowledge-driven (O) approach (O) for (O) universal (O) [phone (B) set (I)] creation, (O) i.e., (O) the (O) [phone (B) sets (I)] of (O) all (O) the (O) five (O) languages (O) were (O) pooled (O) together (O) and (O) then (O) merged (O) based (O) on (O) their (O) [SAMPA (B) symbols (I)]. 
This (O) resulted (O) in (O) a (O) universal (O) [phone (B) set (I)] with (O) 92 (O) phones (O) (including (O) silence). (O) 
A (O) [multilingual (B) MLP (I)] with (O) 39-dimensional (O) [PLP (B) cepstral (I) features (I)] and (O) nine (O) frames (O) of (O) temporal (O) context (O) as (O) input (O) was (O) then (O) trained (O) to (O) classify (O) this (O) universal (O) [phone (B) set (I)]. 
We (O) investigated (O) the (O) following (O) systems (O) for (O) [MLP (B) features (I)]. 
(i) (O) Mono-tandem (O) : For (O) each (O) language, (O) a (O) separate (O) [acoustic (B) model (I)] is (O) built (O) using (O) [PLP (B) cepstral (I) features (I)] concatenated (O) with (O) [MLP (B) feature (I)] extracted (O) from (O) their (O) respective (O) [monolingual (B) MLP (I)]. 
(ii) (O) Multi-tandem (O) : The (O) [multilingual (B) MLP (I)] is (O) used (O) here (O) as (O) [feature (B) extractor (I)]. 
For (O) each (O) language, (O) the (O) KLT (O) statistics (O) was (O) estimated (O) using (O) only (O) the (O) data (O) specific (O) to (O) the (O) language, (O) and (O) then (O) while (O) applying (O) KLT (O) the (O) dimensionality (O) was (O) reduced (O) to (O) match (O) the (O) output (O) dimension (O) of (O) the (O) corresponding (O) [monolingual (B) MLP (I)]. 
This (O) dimensionality (O) was (O) reduced (O) to (O) make (O) the (O) system (O) comparable (O) to (O) mono-tandem (O) in (O) terms (O) of (O) complexity. (O) 
A (O) separate (O) [acoustic (B) model (I)] was (O) then (O) built (O) for (O) each (O) language (O) separately (O) using (O) the (O) [PLP (B) cepstral (I) features (I)] concatenated (O) with (O) the (O) [multilingual (B) MLP (I) features (I)]. 
(iii) (O) [Shared-tandem (B)] : Similarly (O) to (O) the (O) multi-tandem (O) system, (O) we (O) used (O) the (O) [multilingual (B) MLP (I)] for (O) [MLP (B) feature (I)] extraction. (O) 
However, (O) in (O) this (O) system, (O) data (O) from (O) all (O) the (O) languages (O) was (O) used (O) for (O) KLT (O) statistics (O) estimation, (O) thus (O) yielding (O) a (O) multlingual (O) [MLP (B) feature (I)] different (O) from (O) multitandem (O) system. (O) 
In (O) addition, (O) we (O) used (O) the (O) data (O) from (O) all (O) the (O) languages (O) to (O) train (O) a (O) common (O) [acoustic (B) model (I)] that (O) is (O) shared (O) across (O) languages. (O) 
In (O) other (O) words, (O) both (O) [MLP (B) feature (I)] extraction (O) and (O) acoustic (O) modelling (O) are (O) [language-independent (B)]. 
It (O) should (O) be (O) noted (O) that (O) here (O) again (O) the (O) [feature (B) observation (I)] for (O) [acoustic (B) model (I)] consists (O) of (O) the (O) [PLP (B) cepstral (I) features (I)] concatenated (O) with (O) [multilingual (B) MLP (I) features (I)]. 
We (O) evaluated (O) the (O) above (O) systems (O) on (O) two (O) different (O) tasks. (O) 
(i) (O) Mono-lingual (O) task (O) : In (O) this (O) case, (O) it (O) is (O) assumed (O) that (O) the (O) language (O) identity (O) is (O) known (O) a (O) priori, (O) and (O) the (O) [ASR (B) system (I)] corresponding (O) to (O) the (O) language (O) is (O) used (O) for (O) decoding (O) the (O) test (O) utterance. (O) 
In (O) other (O) words, (O) this (O) task (O) corresponds (O) to (O) [monolingual (B) speech (I) recognition (I)]. 
(ii) (O) Mixed (O) language (O) task (O) : In (O) this (O) case, (O) it (O) is (O) assumed (O) that (O) the (O) language (O) identity (O) is (O) not (O) known (O) a (O) priori. (O) 
While (O) decoding (O) the (O) test (O) utterance, (O) all (O) the (O) five (O) [ASR (B) systems (I)] are (O) run (O) in (O) parallel (O) and (O) the (O) output (O) hypothesis (O) yielding (O) [maximum (B) likelihood (I)] is (O) selected (O) as (O) the (O) recognized (O) output (O) 17. (O) 
In (O) other (O) words, (O) this (O) task (O) corresponds (O) to (O) [multilingual (B) speech (I) recognition (I)]. 
In (O) the (O) case (O) of (O) the (O) mixed (O) language (O) task, (O) it (O) can (O) be (O) observed (O) that (O) mono-tandem (O) and (O) multi-tandem (O) systems (O) have (O) different (O) complexities, (O) i.e., (O) the (O) dimensionality (O) of (O) the (O) [feature (B) vectors (I)] is (O) different (O) across (O) languages. (O) 
To (O) handle (O) this, (O) a (O) recognizer (O) dependent (O) bias (O) was (O) subtracted (O) from (O) the (O) respective (O) [log (B) likelihood (I) scores (I)]) before (O) making (O) a (O) decision (O) about (O) the (O) word (O) hypothesis. (O) 
The (O) recognizer (O) dependent (O) bias (O) was (O) estimated (O) on (O) the (O) development (O) set. (O) 
In (O) table (O) 5, (O) we (O) show (O) the (O) performances (O) for (O) the (O) different (O) systems (O) and (O) tasks. (O) 
The (O) performance (O) of (O) each (O) system (O) is (O) expressed (O) as (O) the (O) average (O) word (O) accuracy (O) computed (O) across (O) the (O) five (O) languages. (O) 
The (O) results (O) show (O) that (O) [multilingual (B) MLP (I) features (I)] yield (O) the (O) best (O) performance (O) in (O) terms (O) of (O) relative (O) loss (O) in (O) performance (O) between (O) mono (O) and (O) mixed (O) tasks. (O) 
Although, (O) the (O) [shared-tandem (B) system (I)] yields (O) slightly (O) inferior (O) performance (O) compared (O) to (O) other (O) systems (O) on (O) the (O) mono (O) task, (O) it (O) yields (O) significantly (O) better (O) performance (O) on (O) the (O) mixed (O) task. (O) 
A (O) similar (O) trend (O) has (O) been (O) previously (O) reported (O) in (O) the (O) context (O) of (O) [language (B) independent (I)] acoustic (O) modelling. (O) 
In (O) summary, (O) the (O) superiority (O) of (O) the (O) [shared-tandem (B) system (I)] on (O) the (O) mixed (O) task (O) can (O) be (O) attributed (O) to (O) the (O) combination (O) of (O) two (O) factors (O) : (i) (O) sharing (O) of (O) data (O) across (O) languages (O) which (O) results (O) in (O) better (O) [acoustic (B) model (I)], and (O) (ii) (O) use (O) of (O) [multilingual (B) MLP (I) features (I)]. 

Table. (O) Number (O) of (O) available (O) utterances (O) (utt.), (O) and (O) total (O) duration (O) in (O) hours (O) (h), (O) for (O) each (O) of (O) the (O) five (O) involved (O) languages. (O) 
English (O) (EN), (O) Spanish (O) (ES), (O) Italian (O) (IT), (O) Swiss (O) French (O) (SF) (O) and (O) Swiss (O) German (O) (SZ). (O) 


Table. (O) Information (O) about (O) the (O) languages (O) used (O) in (O) the (O) experiments. (O) 
The (O) codes (O) are (O) assigned (O) by (O) SpeechDat. (O) 
The (O) number (O) of (O) [phonemes (B)] is (O) given (O) based (O) on (O) the (O) reduced (O) [lexicon (B)] of (O) the (O) application (O) words (O) (not (O) all (O) the (O) [phonemes (B)] of (O) a (O) language (O) are (O) used). (O) 


Table. (O) Word (O) accuracy (O) of (O) the (O) different (O) systems (O) investigated (O) is (O) presented (O) for (O) the (O) two (O) tasks, (O) mono (O) and (O) mixed. (O) 
Column (O) 4 (O) (rel. (O) loss) (O) presents (O) the (O) relative (O) difference (O) in (O) the (O) performance (O) of (O) the (O) system (O) computed (O) across (O) mono (O) task (O) and (O) mixed (O) task. (O) 
Mono (O) refers (O) to (O) [monolingual (B) speech (I) recognition (I) task (I)]. 
Mixed (O) refers (O) to (O) multilingual (O) / mixed (O) language (O) [speech (B) recognition (I) task (I)]. 


As (O) the (O) [ASR (B) system (I)] in (O) this (O) study (O) is (O) built (O) to (O) recognize (O) isolated (O) words, (O) in (O) case (O) of (O) [shared-tandem (B) system (I)] it (O) amounts (O) to (O) 
  running (O) a (O) single (O) system. (O) 

Summary (O) 

In (O) this (O) section, (O) we (O) presented (O) three (O) studies (O) on (O) [multilingual (B) ASR (I)] using (O) [MLP (B) features (I)]. 
In (O) the (O) first (O) study, (O) we (O) investigated (O) the (O) [MRASTA (B)] and (O) hierarchical (O) [MRASTA (B) features (I)] across (O) two (O) different (O) languages. (O) 
We (O) found (O) the (O) trends (O) to (O) be (O) similar (O) across (O) languages. (O) 
The (O) second (O) study (O) presented (O) the (O) use (O) of (O) [MLP (B) features (I)] for (O) [cross-lingual (B) transfer (I)] (without (O) any (O) adaptation (O) or (O) retraining), (O) where (O) we (O) found (O) that (O) it (O) is (O) possible (O) to (O) obtain (O) improvements (O) using (O) [cross-lingual (B) MLP (I) features (I)]. 
Finally, (O) we (O) presented (O) a (O) preliminary (O) study (O) on (O) the (O) use (O) of (O) [multilingual (B) MLP (I) features (I)]. 
Our (O) studies (O) indicate (O) that (O) shared (O) [multilingual (B) MLP (I) feature (I)] extraction (O) yields (O) better (O) performance (O) when (O) compared (O) to (O) [language-specific (B) multilingual (I) MLP (I) feature (I)] extraction (O) or (O) [monolingual (B) MLP (I) feature (I)] extraction. (O) 

Language (O) identification (O) / detection (O) 

In (O) section, (O) we (O) briefly (O) described (O) that (O) language (O) identification (O) systems (O) use (O) different (O) levels (O) of (O) abstraction (O) related (O) to (O) spoken (O) language (O) processing, (O) including (O) the (O) use (O) of (O) [phonotactic (B) constraints (I)], lexical (O) constraints, (O) or (O) both (O) lexical (O) and (O) language (O) constraints (O) through (O) [ASR (B) system (I)]. 
Section (O)   presents (O) a (O) preliminary (O) study (O) on (O) hierarchical (O) [MLP (B)]-based LID (O) system. (O) 
This (O) system (O) tries (O) to (O) capture (O) implicitly (O) [phonotactic (B) constraints (I)] and (O) acoustic (O) confusions (O) present (O) at (O) the (O) output (O) of (O) a (O) [multilingual (B) MLP (I)] to (O) achieve (O) language (O) identification. (O) 
Another (O) way (O) of (O) framing (O) the (O) language (O) detection (O) (LD) (O) problem (O) is (O) in (O) terms (O) of (O) out-of-vocabulary (O) (OOV) (O) detection. (O) 
[Monolingual (B) automatic (I) speech (I) recognition (I) systems (I)] assume (O) that (O) the (O) test (O) utterances (O) contain (O) only (O) words (O) from (O) the (O) [target (B) language (I)]. 
However, (O) it (O) is (O) possible (O) that (O) segments (O) of (O) test (O) utterances (O) can (O) contain (O) words (O) from (O) foreign (O) language(s), (O) especially (O) in (O) natural (O) conversations. (O) 
In (O) section, (O) we (O) present (O) an (O) approach (O) to (O) detect (O) such (O) out-of-language (O) segments (O) using (O) confidence (O) measures. (O) 

Hierarchical (O) [MLP (B)]-based language (O) identification (O) 

In (O) section, (O) we (O) briefly (O) described (O) the (O) recently (O) proposed (O) hierarchical (O) [MLP (B)]-based [phoneme (B) posterior (I)] estimation (O) approach (O) and (O) discussed (O) about (O) the (O) potential (O) of (O) applying (O) it (O) for (O) [cross-lingual (B) MLP (I) feature (I) extraction (I)]. 
In, (O) we (O) have (O) studied (O) the (O) role (O) of (O) the (O) second (O) [MLP (B) layer (I)] in (O) such (O) hierarchical (O) arrangements (O) using (O) Volterra (O) series (O) and (O) have (O) found (O) that (O) it (O) is (O) predominantly (O) responsible (O) for (O) learning (O) [phonetic (B)]-temporal patterns (O) present (O) in (O) the (O) [posterior (B) features (I)]. 
The (O) learned (O) [phonetic (B)]-temporal patterns (O) consist (O) of (O) acoustic (O) confusions (O) among (O) [phonemes (B)] and (O) [phonotactic (B) constraints (I)] of (O) the (O) language. (O) 

Table. (O) Comparison (O) of (O) different (O) LID (O) systems. (O) 
The (O) System (O) Hier (O) performance (O) was (O) obtained (O) with (O) a (O) temporal (O) context (O) of (O) 290 (O) ms (O) at (O) the (O) input (O) of (O) the (O) second (O) stage (O) [MLP (B)]. 


In (O) the (O) context (O) of (O) LID, (O) such (O) [phonetic (B)]-temporal patterns (O) could (O) possibly (O) be (O) exploited (O) by (O) first (O) training (O) an (O) [MLP (B)] to (O) classify (O) the (O) previously (O) described (O) universal (O) [phoneme (B) set (I)] ([multilingual (B) speech (I) units (I)]), and (O) then (O) modelling (O) a (O) larger (O) temporal (O) context (O) of (O) the (O) resulting (O) [posterior (B) features (I)] by (O) a (O) second (O) [MLP (B)] to (O) classify (O) languages. (O) 
It (O) can (O) be (O) expected (O) that (O) information (O) related (O) to (O) [phonotactic (B)]   constraints (O) and (O) acoustic (O) confusion (O) among (O) [phonemes (B)] (present (O) in (O) the (O) [posterior (B) features (I)] spanning (O) a (O) long (O) temporal (O) context) (O) is (O) [language-specific (B)]. 
We (O) performed (O) a (O) preliminary (O) study (O) on (O) hierarchical (O) [MLP (B)]-based LID (O) system (O) using (O) the (O) five (O) European (O) language (O) setup (O) and (O) the (O) [multilingual (B) MLP (I)] described (O) earlier (O) in (O) section. (O) 
The (O) second (O) stage (O) [MLP (B)] of (O) the (O) hierarchical (O) [MLP (B)]-based LID (O) system (O) was (O) trained (O) with (O) the (O) [posterior (B) features (I)] (universal (O) [phone (B) posterior (I) probabilities (I)]) estimated (O) at (O) the (O) output (O) of (O) the (O) [multilingual (B) MLP (I)]. 
The (O) temporal (O) context (O) at (O) the (O) input (O) of (O) the (O) second (O) [MLP (B)] was (O) varied (O) from (O) 130–310 (O) ms. (O) 
During (O) testing, (O) the (O) decision (O) about (O) the (O) language (O) identity (O) was (O) made (O) by (O) choosing (O) the (O) language (O) that (O) scores (O) the (O) highest (O) log (O) posterior (O) probability (O) over (O) the (O) whole (O) test (O) utterance. (O) 
We (O) refer (O) to (O) this (O) system (O) as (O) Hier. (O) 
We (O) compared (O) the (O) hierarchical (O) [MLP (B)]-based LID (O) system (O) against (O) two (O) different (O) reference (O) systems. (O) 
In (O) both (O) these (O) systems, (O) the (O) [phone (B) posterior (I) probabilities (I)] estimated (O) at (O) the (O) output (O) of (O) the (O) [multilingual (B) MLP (I)] are (O) used (O) as (O) a (O) local (O) score. (O) 
In (O) the (O) first (O) system, (O) the (O) [language-specific (B) phoneme (I) recognizers (I)] are (O) run (O) in (O) parallel (O) with (O) their (O) respective (O) bigram (O) [phonotactic (B)] language (O) model. (O) 
The (O) LID (O) is (O) achieved (O) by (O) selecting (O) the (O) [decoder (B) output (I)] that (O) yields (O) the (O) [maximum (B) likelihood (I)]. 
This (O) system (O) is (O) referred (O) to (O) as (O) PC. (O) 
In (O) the (O) second (O) system, (O) the (O) language (O) identity (O) is (O) inferred (O) through (O) [speech (B) recognition (I)]. 
The (O) second (O) system (O) is (O) similar (O) to (O) the (O) [shared-tandem (B) system (I)] described (O) earlier (O) in (O) section (O)   where (O) the (O) [acoustic (B) model (I)] is (O) shared (O) across (O) languages. (O) 
However, (O) the (O) recognition (O) is (O) done (O) by (O) using (O) hybrid (O) [HMM (B)] / [MLP (B)]-based isolated (O) word (O) recognition (O) system. (O) 
This (O) system (O) is (O) referred (O) to (O) as (O) SR. (O) 
Table (O) 6 (O) shows (O) the (O) performance (O) (measured (O) in (O) terms (O) of (O) percentage (O) accuracy) (O) of (O) the (O) different (O) LID (O) systems (O) investigated. (O) 
In (O) the (O) case (O) of (O) System (O) Hier, (O) the (O) performance (O) is (O) reported (O) for (O) the (O) temporal (O) context (O) of (O) 290 (O) ms. (O) 
For (O) further (O) details (O) on (O) the (O) effect (O) of (O) temporal (O) context, (O) the (O) reader (O) is (O) referred (O) to. (O) 
It (O) can (O) be (O) seen (O) that (O) the (O) hierarchical (O) LID (O) system, (O) i.e., (O) Hier (O) system (O) yields (O) the (O) best (O) performance. (O) 
When (O) comparing (O) the (O) performance (O) of (O) SR (O) and (O) PC (O) systems, (O) the (O) trend (O) is (O) similar (O) to (O) what (O) has (O) been (O) previously (O) reported (O) in (O) the (O) literature. (O) 
More (O) specifically, (O) higher (O) LID (O) performance (O) has (O) been (O) typically (O) reported (O) (although (O) for (O) fewer (O) number (O) of (O) language (O) classes) (O) using (O) the (O) large (O) vocabulary (O) [continuous (B) speech (I) recognition (I) (LVCSR) (I) system (I)] when (O) compared (O) to (O) [acoustic-phonotactic (B) based (I) systems (I)]. 
Overall, (O) the (O) study (O) shows (O) that (O) there (O) is (O) good (O) potential (O) in (O) exploiting (O) the (O) hierarchial (O) [MLP (B)]-based approach (O) for (O) language (O) identification. (O) 

Out-of-language (O) detection (O) 

In (O) [multilingual (B) speech (I) processing (I)], the (O) [speech (B) data (I)] can (O) contain (O) words (O) from (O) different (O) / multiple (O) languages. (O) 
Earlier (O) in (O) section (O) 2.1, (O) we (O) mentioned (O) that (O) in (O) the (O) context (O) of (O) [multilingual (B) ASR (I)] this (O) can (O) be (O) possibly (O) handled (O) by (O) the (O) use (O) of (O) multilingual (O) language (O) models. (O) 
In (O) contrast, (O) there (O) are (O) cases (O) where (O) the (O) goal (O) is (O) to (O) perform (O) [monolingual (B) speech (I) recognition (I)] but (O) the (O) [speech (B) data (I)] may (O) contain (O) words (O) (or (O) a (O) sequence (O) of (O) words) (O) from (O) foreign (O) language. (O) 
For (O) instance, (O) it (O) has (O) been (O) observed (O) that (O)   in (O) spontaneous (O) meeting (O) recordings, (O) the (O) interchangeable (O) use (O) of (O) different (O) languages (O) in (O) short (O) time (O) periods (O) by (O) the (O) [same (B) speaker (I)] can (O) often (O) be (O) registered. (O) 
The (O) existence (O) of (O) such (O) segments (O) from (O) foreign (O) language (O) can (O) have (O) an (O) adverse (O) effect (O) on (O) the (O) performance (O) of (O) the (O) [ASR (B) system (I)]. 
The (O) adverse (O) effect (O) may (O) be (O) limited (O) by (O) the (O) detection (O) of (O) out-of-language (O) (OOL) (O) segments. (O) 
We (O) have (O) proposed (O) a (O) new (O) approach (O) to (O) detect (O) OOL (O) segments (O) through (O) the (O) use (O) of (O) wordand (O) [phone-based (B) confidence (I) measures (I)]. 
In (O) principle, (O) OOL (O) detection (O) can (O) be (O) compared (O) to (O) LD (O) task. (O) 
However, (O) unlike (O) the (O) LD (O) task, (O) in (O) our (O) OOL (O) detection (O) approach (O) no (O) data (O) from (O) other (O) languages (O) is (O) used. (O) 
Instead, (O) given (O) a (O) test (O) utterance (O) / segment (O) the (O) OOL (O) detection (O) is (O) achieved (O) by (O) :   
(i) (O) Running (O) [LVCSR (B) system (I)] of (O) the (O) [target (B) language (I)] to (O) obtain (O) phone (O) lattices (O) or (O) word (O) lattices. (O) 
(ii) (O) Treating (O) the (O) lattices (O) as (O) the (O) model (O) and (O) estimating (O) frame (O) level (O) [phone (B) posterior (I) probabilities (I)] or (O) word (O) posterior (O) probabilities (O) by (O) using (O) standard (O) forward (O) – (O) backward (O) algorithm. (O) 
(iii) (O) Estimating (O) a (O) posterior-based (O) confidence (O) measure (O) (CM) (O) from (O) the (O) posterior (O) probability (O) estimate (O) of (O) either (O) phones (O) or (O) words. (O) 
This (O) is (O) followed (O) by (O) incorporation (O) of (O) temporal (O) context (O) via (O) median (O) filtering (O) of (O) the (O) CM. (O) 
(iv) (O) Finally, (O) using (O) the (O) posterior-based (O) CM (O) as (O) threshold (O) on (O) the (O) individual (O) [speech (B) segments (I)] of (O) the (O) one-best (O) hypothesis (O) obtained (O) from (O) the (O) [LVCSR (B) system (I)]. 
In (O) our (O) work, (O) we (O) have (O) investigated (O) different (O) types (O) of (O) confidence (O) measures (O) and (O) their (O) combination (O) using (O) maximum (O) entropy (O) classifier. (O) 
We (O) evaluated (O) the (O) OOL (O) detection (O) technique (O) on (O) Klewel (O) meeting (O) recordings18. (O) 
The (O) [evaluation (B) data (I)] consists (O) of (O) 3 (O) h (O) of (O) recordings (O) each (O) from (O) three (O) languages, (O) namely, (O) English, (O) French (O) and (O) Italian, (O) i.e., (O) in (O) total (O) 9 (O) h (O) of (O) recordings. (O) 
English (O) recordings (O) represent (O) in-language (O) [speech (B) segments (I)], while (O) French (O) and (O) Italian (O) recordings (O) represent (O) out-of-language (O) segments. (O) 
This (O) data (O) was (O) processed (O) by (O) an (O) English (O) [LVCSR (B) system (I)] to (O) obtain (O) word (O) and (O) phone (O) recognition (O) lattices. (O) 
Experiments (O) on (O) the (O) detection (O) of (O) OOL (O) segments (O) (caused (O) by (O) the (O) French (O) and (O) Italian (O) recordings) (O) yield (O) performances (O) of (O) about (O) 11 (O) % EER. (O) 
Subsequent (O) incorporation (O) of (O) temporal (O) context (O) significantly (O) increased (O) the (O) achieved (O) performance. (O) 
Median (O) filter (O) with (O) a (O) length (O) of (O) 3 (O) s (O) yield (O) relative (O) improvement (O) of (O) about (O) 62 (O) % with (O) respect (O) to (O) the (O) system (O) without (O) application (O) of (O) temporal (O) context. (O) 

Summary (O) 

In (O) this (O) section, (O) we (O) first (O) presented (O) a (O) LID (O) system (O) based (O) on (O) hierarchical (O) [MLP (B)]-based approach. (O) 
Through (O) preliminary (O) studies, (O) we (O) demonstrated (O) that (O) this (O) system (O) could (O) yield (O) better (O) performance (O) than (O) standard (O) approaches, (O) such (O) as (O) modelling (O) [phonotactic (B) constraints (I)]. 
We (O) next (O) presented (O) an (O) outof-language (O) detection (O) approach (O) using (O) confidence (O) measures (O) similar (O) to (O) out-of-vocabulary (O) word (O) detection, (O) and (O) showed (O) its (O) application (O) on (O) real (O) [word (B) data (I)]. 

Future (O) opportunities (O) and (O) challenges (O) 

We (O) have (O) presented (O) an (O) overview (O) of (O) [multilingual (B) speech (I) processing (I)] – (O) past (O) progress (O) and (O) current (O) trends (O) – (O) from (O) the (O) perspective (O) of (O) our (O) own (O) research (O) activities (O) at (O) Idiap (O) Research (O) Institute. (O) 
We (O)   have (O) shown (O) that (O) a (O) prime (O) mover (O) behind (O) current (O) trends (O) has (O) been (O) the (O) rise (O) of (O) statistical (O) [machine (B) translation (I)], which (O) has (O) had (O) a (O) ripple (O) on (O) effect (O) on (O) the (O) general (O) field (O) of (O) [MLSP (B)]. 
It (O) also (O) should (O) be (O) apparent (O) that (O) future (O) trends (O) will (O) still (O) closely (O) follow (O) on (O) from (O) developments (O) made (O) in (O) mainstream (O) [speech (B) and (I) language (I) technologies (I)], but (O) the (O) distinct (O) challenges (O) of (O) [MLSP (B)] will (O) also (O) give (O) rise (O) to (O) novel (O) solutions. (O) 
Similar (O) to (O) the (O) influence (O) of (O) statistical (O) [machine (B) translation (I)] on (O) developments (O) in (O) [MLSP (B)] in (O) recent (O) years, (O) we (O) anticipate (O) future (O) activity (O) will (O) be (O) strongly (O) driven (O) by (O) web-based (O) services, (O) especially (O) those (O) for (O) mobile (O) devices. (O) 
We (O) note (O) that (O) these (O) services (O) are (O) becoming (O) widely (O) available (O) and, (O) combined (O) with (O) affordable (O) broadband (O) wireless (O) access, (O) such (O) services (O) will (O) provide (O) an (O) opportunity (O) to (O) make (O) available (O) a (O) broader (O) range (O) of (O) capabilities (O) to (O) mobile (O) devices, (O) especially (O) those (O) based (O) on (O) computationally (O) demanding (O) tasks (O) in (O) [MLSP (B)]. 
Thus, (O) we (O) are (O) already (O) seeing (O) services (O) being (O) provided (O) by (O) major (O) market (O) players (O) in (O) the (O) domain (O) of (O) [speech (B) processing (I)] and (O) we (O) can (O) expect (O) that (O) will (O) also (O) expand (O) to (O) a (O) greater (O) number (O) of (O) applications (O) in (O) [MLSP (B)] and (O) consequently (O) an (O) increase (O) in (O) research (O) and (O) development (O) activity (O) in (O) both (O) academic (O) and (O) industry (O) alike. (O) 
In (O) our (O) own (O) work, (O) two (O) primary (O) research (O) directions (O) that (O) have (O) emerged (O) are (O) [cross-lingual (B) speaker (I) adaptation (I)] for (O) [HMM-based (B) TTS (I)] and (O) hierarchical (O) architectures (O) for (O) discriminative (O) [MLSP (B)]. 
Work (O) on (O) [cross-lingual (B) speaker (I) adaptation (I)] for (O) [HMM-based (B) TTS (I)] has (O) only (O) just (O) started (O) to (O) scratch (O) the (O) surface (O) and (O) is (O) apparent (O) that (O) the (O) rise (O) of (O) [statistical (B) parametric (I) TTS (I)] will (O) likely (O) lead (O) to (O) many (O) more (O) novel (O) challenges (O) in (O) [MLSP (B)] for (O) [TTS (B)]. 
For (O) example, (O) we (O) have (O) already (O) spoken (O) of (O) joint (O) optimization (O) of (O) combined (O) systems (O) for (O) [speech (B) recognition (I)] and (O) [machine (B) translation (I)]. 
Conceivably, (O) similar (O) principles (O) could (O) be (O) applied (O) to (O) combined (O) [machine (B) translation (I)] and (O) [speech (B) synthesis (I)] to (O) produce (O) more (O) intelligible (O) translated (O) output. (O) 
The (O) adaptive (O) [HMM-based (B) framework (I)] also (O) poses (O) an (O) attractive (O) solution (O) for (O) research (O) in (O) polyglot (O) synthesis, (O) without (O) the (O) need (O) for (O) developing (O) [extensive (B) data (I)] resources (O) for (O) [multilingual (B) speakers (I)]. 
Addressing (O) the (O) tasks (O) of (O) cross (O) [- (B) corpus (I)] normalization (O) and (O) [cross-language (B) contextual (I) modelling (I)] will (O) likely (O) be (O) challenges (O) to (O) overcome (O) if (O) we (O) are (O) to (O) be (O) successful (O) in (O) this. (O) 
In (O) the (O) domain (O) of (O) [multilingual (B) ASR (I)], methods (O) of (O) [cross-language (B) knowledge (I) transfer (I)] still (O) have (O) considerable (O) potential. (O) 
With (O) increasing (O) use (O) of (O) [lightly (B) supervised (I) techniques (I)] and (O) [data (B) mining (I)], there (O) is (O) also (O) increasing (O) need (O) to (O) be (O) able (O) to (O) effectively (O) bootstrap (O) models (O) from (O) other (O) languages. (O) 
Unfortunately, (O) models (O) trained (O) using (O) discriminative (O) criteria (O) are (O) particularly (O) susceptible (O) to (O) transcription (O) errors, (O) possibly (O) making (O) them (O) unsuitable (O) for (O) application (O) in (O) [acoustic (B) model (I)] bootstrapping. (O) 
By (O) combining (O) hierarchical (O) approaches (O) with (O) discriminative (O) techniques, (O) we (O) may (O) obtain (O) an (O) effective (O) technique (O) for (O) [acoustic (B) model (I)] bootstrapping. (O) 
Furthermore, (O) in (O) the (O) context (O) of (O) [MLPbased (B) features (I)], there (O) is (O) also (O) need (O) to (O) investigate (O) extensively (O) the (O) use (O) of (O) other (O) [language-independent (B) representation (I)] of (O) [phonetic (B) information (I)], such (O) as (O) [articulatory (B) features (I)], and (O) modelling (O) of (O) subword (O) unit (O) representations (O) such (O) as (O) [graphemes (B)], especially (O) Roman (O) alphabets (O) which (O) is (O) shared (O) across (O) many (O) different (O) languages. (O) 
The (O) research (O) leading (O) to (O) these (O) results (O) was (O) partially (O) funded (O) by (O) the (O) 7th (O) Framework (O) Programme (O) (FP7/2007-2013) (O) of (O) the (O) European (O) Union (O) under (O) Grant (O) Agreement (O) 213845 (O) (the (O) EMIME (O) project), (O) Swiss (O) National (O) Science (O) Foundation (O) through (O) MULTI (O) and (O) the (O) National (O) Centre (O) of (O) Competence (O) in (O) Research (O) (NCCR) (O) on (O) Interactive (O) Multimodal (O) Information (O) Management (O) (IM2), (O) and (O) by (O) the (O) Defense (O) Advanced (O) Research (O) Projects (O) Agency (O) (DARPA) (O) under (O) Contract (O) No. (O) HR0011-06-C-0023. (O) 
The (O) authors (O) wish (O) to (O) thank (O) all (O) the (O) collaborators (O) in (O) the (O) different (O) projects. (O) 
The (O) authors (O) gratefully (O) acknowledge (O) the (O) International (O) Computer (O) Science (O) Institute (O) (ICSI) (O) for (O) the (O) use (O) of (O) their (O) computing (O) resources. (O) 
The (O) opinions, (O) findings, (O) conclusions (O) or (O) recommendations (O) expressed (O) in (O) this (O) material (O) are (O) those (O) of (O) the (O) author(s) (O) and (O) do (O) not (O) necessarily (O) reflect (O) the (O) views (O) of (O) the (O) DARPA. (O) 

http://www.klewel.com (O) 
