One Model, Many Languages : Meta-learning for [Multilingual Text-to-Speech] 


Abstract 
We introduce an approach to [multilingual speech synthesis] which uses the meta-learning concept of contextual parameter generation and produces natural-sounding [multilingual speech] using more languages and less [training data] than previous approaches. 
Our model is based on [Tacotron 2] with a fully convolutional input text [encoder] whose weights are predicted by a separate parameter generator network. 
To boost voice cloning, the model uses an [adversarial speaker classifier] with a [gradient reversal layer] that removes speaker-specific information from the [encoder]. 

We arranged two experiments to compare our model with baselines using various levels of [cross-lingual parameter sharing], in order to evaluate : stability and performance when training on low amounts of data, pronunciation accuracy and [voice quality] of code-switching synthesis. 
For training, we used the [CSS10 dataset] and our new [small dataset] based on [Common Voice] recordings in five languages. 
Our model is shown to effectively share information across languages and according to a subjective evaluation test, it produces more natural and accurate code-switching [speech] than the baselines.                                   
Index Terms : [text-to-speech], [speech synthesis], multilinguality, code-switching, meta-learning, domain-adversarial training 

Introduction 
Contemporary [end-to-end speech synthesis systems] achieve great results and produce natural-sounding human-like [speech] even in real time. 
They make possible an efficient training that does not put high demands on quality, amount, and preprocessing of [training data]. Based on these advances, researchers aim at, for example, expressiveness, controllability, or few-[shot voice] cloning. 
When extending these models to support multiple languages, one may encounter obstacles such as different [input representations] or pronunciations, and imbalanced amounts of [training data] per language. 
In this work, we examine [cross-lingual knowledge]-sharing aspects of [multilingual text-to-speech] ([TTS]). 
We experiment with more languages simultaneously than most previous [TTS] work known to us. 
We can summarize our contributions as follows : 
We propose a scalable [grapheme-based model] that utilizes the idea of contextual parameter generator network and we compare it with baseline models using different levels of parameter sharing. 
We introduce a new [small dataset] based on [Common Voice] that includes data in five languages from 84 speakers. 
We evaluate effectiveness of the compared models on ten languages with three different scripts and we show their code-switching abilities on five languages. 
For the purposes of the evaluation, we created a new test set of 400 bilingual code-switching sentences. 
Our source code, [hyper-parameters], training and [evaluation data], samples, pre-trained models, and interactive demos are freely available on GitHub. 

https://github.com/Tomiinek/Multilingual_Text_to_Speech 


Figure : Diagram of our model. The meta-network generates parameters of [language-specific convolutional text encoders]. 
Encoded text inputs enhanced with [speaker embeddings] are read by the [decoder]. 
The adversarial classifier suppresses speaker-dependent information in [encoder outputs]. 

Related Work 
So far, several works explored training joint [multilingual models] in [text-to-speech], following similar experiments in the field of [neural machine translation]. 
[Multilingual models] offer a ew key benefits : 
• Transfer learning : We can try to make use of high-resource languages for training [TTS systems] for low-resource lan, e.g., via transfer learning approaches. 
• Knowledge sharing : We may think of using [multilingual data] for joint training of a single shared [text-to-speech model]. 
Inuitively, this enables [cross-lingual sharing] of patterns learned from data. 
The only work in this area to our knowledge is Prakash et al.’s study   on [TTS] for related Indian languages using hand-built unified [phoneme representations]. 
[• Voice] cloning : Under certain circumstances, producing [speech] in multiple languages with the [same voice], i.e., [crosslingual voice] cloning, is desired. 
However, [audio data] where a [single speaker] speaks several languages is scarce. 
That is why [multilingual voice]-cloning systems should be trainable using mixtures of [monolingual data]. Here, used [Tacotron 2] conditioned on [phonemes] and showed voice-cloning abilities on English, Spanish, and Chinese. 
Nachmani and Wolf extended [Voice Loop]   and enabled [voice conversion] for English, Spanish, and German. Chen et al. 
used a [phoneme-based Tacotron 2] with a ResCNN based [speaker encoder]   that enables a massively [multi-speaker speech synthesis], even with [fictitious voices]. 
• Code switching : In this task closely related to [cross-lingual voice] cloning, we would like to alternate languages within sentences. 
This is useful for foreign names in navigation systems or news readers. In view of that, Cao et al. modified [Tacotron] ; their model uses [language-specific encoders]. 
Code-switching itself is done by combining of their outputs. 
Overall, all recent [multilingual text-to-speech systems] were only tested in 2-3 languages simultaneously, or required vast amounts of data to be trained. 

[Model Architecture] 
We base our experiments on [Tacotron 2]. We focus on the [spectrogram generation] part here ; for [vocoding], we use [WaveRNN] in all our configurations. 
We first explain our new model that uses meta-learning for multilingual knowledge sharing in Sec., then describe contrastive baseline models which are based on recent [multilingual TTS architectures] (Sec.). 

Table : [Total data] sizes per language (hours of [audio data]) in our cleaned [CSS10] (CSS) and [Common Voice] (CV) subsets. 

Our Model : Generated (GEN) 
We introduce a scalable [multilingual text-to-speech model] that follows a meta-learning approach of contextual parameter generation proposed by for [NMT] (see Fig.). 
We call the model generated (G EN) further in this text. The backbone of our model is built on our own implementation of [Tacotron 2], composed of these main components : 
an input text [encoder] that includes a stack of [convolutional layers] and a [bidirectional LSTM], 
a location-sensitive [attention mechanism] with the guided attention loss term that supports faster convergence, 
a [decoder] with two stacked [LSTM layers] where the first queries the [attention mechanism] and the second generates outputs. 
We increase tolerance of the guided attention loss exponentially during training. 
We propose the following changes to this basic architecture : 
[Convolutional Encoders] : We use multiple [language-specific input text encoders]. 
However, having a separate [encoder] with [recurrent layers] for each language is not practical as it involves passing the training batches (which should be balanced with respect to languages) through multiple [encoders] sequentially. 
Therefore, we use a fully [convolutional encoder] from DCTTS. 
The [encoders] use grouped layers and are thus processed effectively. 
We enhance the [encoders] with [batch normalization] and [dropout] with a very low rate. 
The normalization layers are situated before activations and [dropouts] after them. 

[Encoder parameter] generation : To enable [cross-lingual knowledge]-sharing, parameters of the [encoders] are generated using a separate network conditioned on language embeddings. 
The parameter generator is composed of multiple site-specific generators, each of which takes a [language embedding] on the input and produces parameters for one layer of the [convolutional encoder] for the given language. 
The generators enable a controllable [cross-lingual parameter] sharing because reduction of their size prevents generation of highly [language-specific parameters]. 
We implement them as fully connected layers. 

Training with multilingual batches : We construct unusual training batches to fully utilize the potential of this architecture. 
We would like to have a batch of B examples that can be reshaped into a batch of size B / L where L is the number of [encoder] groups or languages. 
This new batch should have a new dimension that groups all examples with the same language. 
Thus we use a batch sampler that creates batches where for each l < L and i < B / L, all (l + iL)-th examples are of the same language. 

Speaker embedding : We extend the model with a [speaker embedding] which is concatenated with each element of the encoded sequence that is attended by the [decoder] while generating [spectrogram frames]. 
This makes the model [multi-speaker] and allows [cross-lingual voice cloning]. 

[Adversarial speaker classifier] : We combine the model with an [adversarial speaker classifier] to boost voice cloning. 
The classifier follows principles of domain adversarial training and is used to proactively remove speaker-specific information from the [encoders]. 
It includes a single [hidden layer], a [softmax layer], and a [gradient reversal layer] that scales the gradient flowing to the [encoders] by a factor – λ. 
The gradients are clipped to stabilize training. 
It is optimized to reduce the cross-entropy of [speaker predictions]. 
The predictions are done separately for each element of the [encoders]’ outputs. 
    
Baselines : Shared, Separate & Single 
We compare GEN with baseline models called shared (SHA), separate (SEP), and single (SGL). 
SGL is a basic [Tacotron 2 model], SHA and SEP follow the recent [multilingual TTS] works of Zhang et al. 
 respectively, but were slightly adapted to our tasks for a fairer comparison to GEN – we use more languages and [less data] than the original works. 
In the following, we only describe their differences from G EN. 
Single (SGL) represents a set of monolingual models that follow [vanilla Tacotron 2] with the original recurrent [encoder] and default settings. 
SGL can not be used for [code-switching]. 
Shared (SHA) : Unlike G EN, S HA has a single [encoder] with the original [Tacotron 2 architecture], so it fully shares all [encoder parameters]. 
This sharing implicitly leads to [language-independent encoder outputs]. 
The [language-dependent processing] happens in the [decoder], so the [speaker embeddings] are explicitly factorized into speaker and language parts. 
Separate (SEP) uses multiple [language-specific convolutional encoders] too, but their parameters are not generated. 
It also does not include the [adversarial speaker classifier]. 

Dataset 
We created a [new dataset] for our experiments, based on carefully cleaning and preprocessing freely available [audio] sources : 
[CSS10] and a small fraction of [Common Voice]. Table shows total durations of the used [audio data] per language. 

[CSS10] 
[CSS10] consists of mono-[speaker data] in German, Greek, Spanish, Finnish, French, Hungarian, Japanese, Dutch, Russian, and Chinese. 
It was created from audiobooks and contains various punctuation styles. 
We applied an automated cleaning to normalize transcripts across languages, including punctuation and some spelling variants (e.g., “ œ ” → “ oe ”). 
We romanized Japanese with MeCab and Romkan, Chinese using Pinyin. 
We further filtered the data to remove any potentially problematic transcripts : we preserved just examples with 0.5-10. 
1s of [audio] and 3-190 transcript characters. 
We computed means µ and variances σ of [audio] durations of groups corresponding to examples with the same transcript lengths. 
Then we removed those with durations outside the interval (µ – 3σ, µ + 3σ). 
In total, the resulting dataset includes 125.26 hours of recordings. 


Table : Left : CERs of [ground-truth recordings] (GT) and recordings produced by monolingual and the three examined [multilingual models]. 
Right : CERs of the recordings synthesized by G EN and S HA trained on just 600 or 900 training examples per language. 
Best results for the given language are shown in bold ; “ * ” denotes statistical significance (established using paired t-test ; p < 0.05). 



[Common Voice] 
To train code-switching models, [multi-speaker data] is required to disentangle the connection between languages and speakers. 
We thus enhanced [CSS10] with data from [Common Voice] (CV) for languages included in both sets – the intersection covers German, French, Chinese, Dutch, Russian, Japanese, and Spanish. 
Since CV is mainly aimed at [speech recognition] and rather noisy, we performed extensive filtering : We removed recordings with a negative rating (as provided by CV for each example) and excluded any speakers with less than 50 recordings. 
We checked a sample of recordings for each speaker, and we removed all their data if we considered the sample to have poor quality. 
This resulted in a [small dataset] of 39 German, 22 French, 11 Dutch, 6 Chinese, and 6 [Russian speakers]. 
Japanese and [Spanish data] were removed completely. 
A lot of recordings in CV contain artifacts at the beginning or end. 
Thus we semi-automatically cleaned leading and trailing segments of all recordings. 
The dataset has 13.7 hours of [audio data] in total. 

Experiments 
We compare our models described in Section. 
The experiment in Section was designed to show stability and ability to train on lower amounts of data. 
We conclude that character error rate (CER) evaluation is sufficient for this experiment. In Section, we test pronunciation accuracy and [voice quality] of code-switching synthesis. 
We used a subjective evaluation test as there are no straightforward objective metrics for this task. 
We used the same [vocoder] for all models, i.e., the [WaveRNN model] trained on a training subset of the cleaned [CSS10 dataset]. 

Multilingual training 
Training setup : We used our cleaned [CSS10 dataset] for training ; 64 randomly selected samples per language were reserved for validation and another 64 for testing. 
We did not have an ambition to [clone voices] in this experiment, so we switched off [speaker classifiers] for SHA and GEN (i.e., SHA was reduced to the [vanilla Tacotron 2 model] with a language embedding). 
We trained the three models for 50k steps with the Adam optimizer.2 We used a stepped learning rate that starts from 10–3 and halves every 10k steps. 
In the case of S EP, we used a lower initial learning rate 10–4. 
For SGL, the learning rate schedule was tuned individually per language. 
We stopped training early after [validation data] loss started increasing. 
SHA, SEP, and GEN used [speaker embeddings] of size 32 and G EN used language embeddings and parameter generators of size 10 and 8, respectively. 
We used language-balanced batches of size 60 for all models. 

With    β1 = 0.9, β2 = 0.999, =10–6, and weight decay of 10–6 

Evaluation : We synthesized [evaluation data] using all the models followed by [WaveRNN] and we sent the synthesized recordings to Google Cloud Platform [ASR]. 
Then we computed CERs between [ground-truth] and [ASR]-produced transcripts (we used the native symbols for Chinese and Japanese). 

Results : Table summarizes the obtained CERs. The first column gives us a notion about the performance of the [ASR] engine. 
The rates stay below 20 % for all languages ; higher CERs are mostly caused by noisy [CSS10] recordings. 
We were not able to train the Greek S GL model due to low amount of [training data]. 
The [decoder] started to overfit soon before the attention could have been established. 
The performance of SGL is similar to SHA except for Chinese, Finnish, and Greek. SEP performed noticeably worse than S HA or even S GL. 
This may be caused by the imbalance between the [batch size] of the [encoder] and the [decoder] as the [encoder]’s effective [batch size] is just B / L.4 Sharing of the data probably regularized the [decoder], so the attention was established even in the case of Greek. 
GEN seems to be significantly better than S HA on most languages. It fulfills our expectations as G EN should be more flexible. 

Manual error analysis : We manually inspected the outputs in German, French, Spanish, and Russian. In the case of Spanish, all the models work well ; we noticed just differences in the treatment of punctuation. 
German outputs by G EN seem to be the best. 
Other models sometimes do unnatural pauses when reaching a punctuation mark. 
Right after the pauses, they often skip a few words. G EN is noticeably better on French and Russian, others produce obvious mispronunciations. 

Data-stress training : To further test the models in data-stress situations, we chose random subsets of 600 and 900 examples per language from the training set (i.e., about 80 or 120 minutes of recordings, respectively). 
We trained all models on both reduced datasets, but accomplished training just for S HA and G EN. 
While training on the bigger and [smaller dataset], we decayed the learning rate every 7.5k and 5k training steps, respectively. 
The right half of Table shows that G EN can work better even in data-stress situations. GEN models have, compared to SHA models, significantly better CER values on six languages. 

https://cloud.google.com/speech-to-text 

Our attempts to compensate for this using different [encoder] and [decoder learning] rates were not successful. 

Table : Mean (with std. dev.) ratings of fluency, naturalness, [voice stability] (top) and pronunciation accuracy (middle). 
The bottom row shows the number of sentences with word skips. 


Figure : Language abilities of participants of our survey. 

Code-switching 
Training setup : In this experiment, we only used the five languages where both [CSS10] and [CV data] are available (Table), and trained on all data in our cleaned sets ; 64 and 4 randomly selected samples for each speaker from [CSS10] and CV, respectively, were reserved for validation. 
The S GL models are not applicable to the code-switching scenario. 
SHA, SEP, and GEN models were trained for 50k steps with the same learning rate and schedule settings as in Section, this time with the [adversarial speaker classifiers] enabled.5 We set the size of [speaker embeddings] to 32 and used a language embedding of size 4 in S HA. 
GEN uses language embeddings of size 10 and generator layers of size 4. We used mini-batches of size 50 for all models.             
                                                                             
Code-switching [evaluation dataset] : We created a new small-[scale dataset] especially for code-switching evaluation. 
We used bilingual sentences scraped from Wikipedia. For each language, we picked 80 sentences with a few foreign words (20 sentences for each of the 4 other languages) ; Chinese was romanized. 
We replaced foreign names with their native forms (see Fig.). 

Figure : Examples of code-switching evaluation sentences. 

Subjective evaluation : We synthesized all evaluation sentences using [speaker embedding] of the [CSS10 speaker] for the base language of the sentence. We arranged a subjective evaluation test and used a rating method that combines five-point [mean opinion score] ([MOS]) with [MUSHRA]. For each sample, its transcript and systems’ outputs were shown at the same time. 
Participants were asked to rate them on a scale from 1 to 5 with 0.1 increments and with labels “ Bad ”, “ Poor ”, “ Fair ”, “ Good ”, “ Excellent ”. To distinguish different error types, we asked for two ratings : fluency, naturalness, and stability of the voice ([speaker similarity]) – to check if foreign words cause any change to the speaker’s voice, and accuracy – testing if all words are pronounced and the foreign word pronunciation is correct.                 
Participants could leave a textual note at the end of the survey. 
For each language, we recruited ten [native speakers] that spoke at least one other language fluently via the Prolific platform (Fig.) They were given twelve sentences with the base language matching their native language where each of the other languages was represented by three sentences. 
                                                                             
Results : Table summarizes results of the survey. The rows marked “ All ” show means and variances of the ratings of all 50 participants. 
Fig. visualizes quantiles of the ratings (grouped by dominant languages). 
G EN has significantly higher mean ratings on both scales. 
Unlike S HA or S EP, it allows [cross-lingual mixing] of the [encoder outputs] and enables smooth control over pronunciation. 
S EP scores consistently worst. 
The accuracy ratings are overall slightly higher than the fluency ratings ; this might be caused by improper word stress, which several participants commented on. 

Based on preliminary experiments on [validation data], we set λ=1 and weighted the loss of the classifier by 0.125 and 0.5 for G EN and S HA, respectively. The classifiers include a hidden layer of size 256.                       
https://www.prolific.co ; 4 participants who reported as Chinese [native speakers] on Prolific only reported non-native fluency in our survey.   
 In 3 sentences, a random model output was distorted and used as sanity check (expected to be rated lowest). All participants passed. 

Figure : Graphs showing distributions of fluency and accuracy ratings grouped by the dominant language of rated sentences. 

Manual error analysis : We found that the models sometimes skip words, especially when reaching foreign words in Chinese sentences. 
Therefore, we manually inspected all 400 outputs of all models and counted sentences where any word skip occurred, see the “ Word skips ” row in Table. 
We found that the G EN model makes much fewer of these errors than S HA and S EP. 

Conclusion 
We presented a new [grapheme-based model] that uses metalearning for [multilingual TTS] 
We showed that it significantly outperforms multiple strong baselines on two tasks : data-stress training and [code-switching], where our model was favored in oth [voice fluency] as well as pronunciation accuracy. 
Our code is available on GitHub.1 For future work, we consider changes to our model’s attention module to further improve accuracy. 

Acknowledgements 
This research was supported by the Charles University grant PRIMUS/19 / SCI/10. 
