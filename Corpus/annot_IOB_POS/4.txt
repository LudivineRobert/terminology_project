One NUM (O) Model NOUN , (O) Many ADJ (O) Languages PROPN (O) : Meta NOUN - learning NOUN (O) for ADP (O) [Multilingual ADJ (B) Text NOUN - to ADP - Speech NOUN (I)] 


Abstract PROPN (O) 
 We PRON (O) introduce VERB (O) an DET (O) approach NOUN (O) to ADP (O) [multilingual ADJ (B) speech NOUN (I) synthesis NOUN (I)] which DET (O) uses VERB (O) the DET (O) meta NOUN - learning NOUN (O) concept NOUN (O) of ADP (O) contextual PROPN (O) parameter NOUN (O) generation NOUN (O) and CCONJ (O) produces VERB (O) natural ADV - sounding NOUN (O) [multilingual ADJ (B) speech NOUN (I)] using VERB (O) more ADJ (O) languages NOUN (O) and CCONJ (O) less ADJ (O) [training NOUN (B) data NOUN (I)] than SCONJ (O) previous ADJ (O) approaches VERB . (O) 
 Our DET (O) model NOUN (O) is AUX (O) based VERB (O) on ADP (O) [Tacotron PROPN (B) 2 NUM (I)] with ADP (O) a NOUN (O) fully ADV (O) convolutional ADJ (O) input NOUN (O) text NOUN (O) [encoder NOUN (B)] whose DET (O) weights NOUN (O) are AUX (O) predicted VERB (O) by ADP (O) a NOUN (O) separate ADJ (O) parameter NOUN (O) generator NOUN (O) network NOUN . (O) 
 To NOUN (O) boost NOUN (O) voice NOUN (O) cloning VERB , (O) the DET (O) model NOUN (O) uses VERB (O) an DET (O) [adversarial ADJ (B) speaker NOUN (I) classifier NOUN (I)] with ADP (O) a NOUN (O) [gradient NOUN (B) reversal NOUN (I) layer NOUN (I)] that DET (O) removes VERB (O) speaker NOUN - specific ADJ (O) information NOUN (O) from ADP (O) the DET (O) [encoder NOUN (B)] . 

We PRON (O) arranged VERB (O) two NUM (O) experiments VERB (O) to ADP (O) compare VERB (O) our DET (O) model NOUN (O) with ADP (O) baselines NOUN (O) using VERB (O) various ADJ (O) levels NOUN (O) of ADP (O) [cross ADJ - lingual ADJ (B) parameter NOUN (I) sharing VERB (I)] , in ADP (O) order NOUN (O) to ADP (O) evaluate VERB (O) : stability NOUN (O) and CCONJ (O) performance NOUN (O) when ADV (O) training NOUN (O) on ADP (O) low NOUN (O) amounts VERB (O) of ADP (O) data NOUN , (O) pronunciation NOUN (O) accuracy NOUN (O) and CCONJ (O) [voice NOUN (B) quality NOUN (I)] of ADP (O) code NOUN - switching VERB (O) synthesis NOUN . (O) 
 For ADP (O) training NOUN , (O) we PRON (O) used VERB (O) the DET (O) [CSS10 PROPN (B) dataset NOUN (I)] and CCONJ (O) our DET (O) new ADJ (O) [small ADJ (B) dataset NOUN (I)] based VERB (O) on ADP (O) [Common PROPN (B) Voice PROPN (I)] recordings VERB (O) in ADP (O) five NUM (O) languages NOUN . (O) 
 Our DET (O) model NOUN (O) is AUX (O) shown VERB (O) to ADP (O) effectively ADV (O) share NOUN (O) information NOUN (O) across ADP (O) languages NOUN (O) and CCONJ (O) according VERB (O) to ADP (O) a NOUN (O) subjective ADJ (O) evaluation NOUN (O) test NOUN , (O) it PRON (O) produces VERB (O) more ADJ (O) natural ADJ (O) and CCONJ (O) accurate ADJ (O) code NOUN - switching VERB (O) [speech NOUN (B)] than SCONJ (O) the DET (O) baselines NOUN . (O)                                   
Index NOUN (O) Terms NOUN (O) : [text NOUN - to ADP - speech NOUN (B)] , [speech NOUN (B) synthesis NOUN (I)] , multilinguality NOUN , (O) code NOUN - switching NOUN , (O) meta NOUN - learning NOUN , (O) domain NOUN - adversarial ADJ (O) training NOUN (O) 

Introduction NOUN (O) 
 Contemporary PROPN (O) [end NOUN - to ADP - end NOUN (B) speech NOUN (I) synthesis NOUN (I) systems NOUN (I)] achieve VERB (O) great ADJ (O) results VERB (O) and CCONJ (O) produce NOUN (O) natural ADV - sounding NOUN (O) human NOUN - like ADJ (O) [speech NOUN (B)] even ADV (O) in ADP (O) real NOUN (O) time NOUN . (O) 
 They PRON (O) make VERB (O) possible ADJ (O) an DET (O) efficient ADJ (O) training NOUN (O) that SCONJ (O) does AUX (O) not PART (O) put VERB (O) high ADJ (O) demands VERB (O) on ADP (O) quality NOUN , (O) amount NOUN , (O) and CCONJ (O) preprocessing VERB (O) of ADP (O) [training NOUN (B) data NOUN (I)] . Based VERB (O) on ADP (O) these DET (O) advances VERB , (O) researchers NOUN (O) aim NOUN (O) at ADP , (O) for ADP (O) example NOUN , (O) expressiveness NOUN , (O) controllability NOUN , (O) or CCONJ (O) few-[shot PROPN (B) voice NOUN (I)] cloning VERB . (O) 
 When ADV (O) extending VERB (O) these DET (O) models NOUN (O) to ADP (O) support NOUN (O) multiple NOUN (O) languages NOUN , (O) one NUM (O) may VERB (O) encounter NOUN (O) obstacles NOUN (O) such ADJ (O) as SCONJ (O) different ADJ (O) [input NOUN (B) representations NOUN (I)] or CCONJ (O) pronunciations NOUN , (O) and CCONJ (O) imbalanced ADJ (O) amounts VERB (O) of ADP (O) [training NOUN (B) data NOUN (I)] per ADP (O) language NOUN . (O) 
 In ADP (O) this DET (O) work NOUN , (O) we PRON (O) examine VERB (O) [cross ADJ - lingual ADJ (B) knowledge NOUN (I)]-sharing VERB aspects NOUN (O) of ADP (O) [multilingual ADJ (B) text NOUN - to ADP - speech NOUN (I)] ([TTS PROPN (B)]) . 
 We PRON (O) experiment NOUN (O) with ADP (O) more ADJ (O) languages NOUN (O) simultaneously ADV (O) than SCONJ (O) most ADJ (O) previous ADJ (O) [TTS PROPN (B)] work NOUN (O) known VERB (O) to ADP (O) us PROPN . (O) 
 We PRON (O) can VERB (O) summarize VERB (O) our DET (O) contributions NOUN (O) as SCONJ (O) follows VERB (O) : 
 We PRON (O) propose NOUN (O) a NOUN (O) scalable ADJ (O) [grapheme NOUN - based VERB (B) model NOUN (I)] that DET (O) utilizes VERB (O) the DET (O) idea NOUN (O) of ADP (O) contextual PROPN (O) parameter NOUN (O) generator NOUN (O) network NOUN (O) and CCONJ (O) we PRON (O) compare VERB (O) it PRON (O) with ADP (O) baseline NOUN (O) models NOUN (O) using VERB (O) different ADJ (O) levels NOUN (O) of ADP (O) parameter NOUN (O) sharing NOUN . (O) 
 We PRON (O) introduce VERB (O) a NOUN (O) new ADJ (O) [small ADJ (B) dataset NOUN (I)] based VERB (O) on ADP (O) [Common PROPN (B) Voice PROPN (I)] that DET (O) includes VERB (O) data NOUN (O) in ADP (O) five NUM (O) languages NOUN (O) from ADP (O) 84 NUM (O) speakers NOUN . (O) 
 We PRON (O) evaluate VERB (O) effectiveness NOUN (O) of ADP (O) the DET (O) compared VERB (O) models NOUN (O) on ADP (O) ten NUM (O) languages NOUN (O) with ADP (O) three NUM (O) different ADJ (O) scripts NOUN (O) and CCONJ (O) we PRON (O) show NOUN (O) their DET (O) code NOUN - switching VERB (O) abilities NOUN (O) on ADP (O) five NUM (O) languages NOUN . (O) 
 For ADP (O) the DET (O) purposes VERB (O) of ADP (O) the DET (O) evaluation NOUN , (O) we PRON (O) created VERB (O) a NOUN (O) new ADJ (O) test NOUN (O) set NOUN (O) of ADP (O) 400 NUM (O) bilingual ADJ (O) code NOUN - switching VERB (O) sentences NOUN . (O) 
 Our DET (O) source NOUN (O) code NOUN , (O) [hyper NOUN - parameters NOUN (B)] , training NOUN (O) and CCONJ (O) [evaluation NOUN (B) data NOUN (I)] , samples NOUN , (O) pre VERB - trained ADJ (O) models NOUN , (O) and CCONJ (O) interactive NOUN (O) demos PROPN (O) are AUX (O) freely ADV (O) available ADJ (O) on ADP (O) GitHub PROPN . (O) 

https://github.com/Tomiinek/Multilingual_Text_to_Speech NUM (O) 


Figure NOUN (O) : Diagram PROPN (O) of ADP (O) our DET (O) model NOUN . (O) The DET (O) meta NOUN - network NOUN (O) generates VERB (O) parameters NOUN (O) of ADP (O) [language NOUN - specific ADJ (B) convolutional ADJ (I) text NOUN (I) encoders NOUN (I)] . 
 Encoded PROPN (O) text NOUN (O) inputs VERB (O) enhanced VERB (O) with ADP (O) [speaker NOUN (B) embeddings NOUN (I)] are AUX (O) read VERB (O) by ADP (O) the DET (O) [decoder NOUN (B)] . 
 The DET (O) adversarial ADJ (O) classifier NOUN (O) suppresses VERB (O) speaker NOUN - dependent ADJ (O) information NOUN (O) in ADP (O) [encoder NOUN (B) outputs NOUN (I)] . 

Related ADJ (O) Work NOUN (O) 
 So ADV (O) far ADV , (O) several ADJ (O) works VERB (O) explored VERB (O) training NOUN (O) joint NOUN (O) [multilingual ADJ (B) models NOUN (I)] in ADP (O) [text NOUN - to ADP - speech NOUN (B)] , following VERB (O) similar ADJ (O) experiments VERB (O) in ADP (O) the DET (O) field NOUN (O) of ADP (O) [neural NOUN (B) machine NOUN (I) translation NOUN (I)] . 
 [Multilingual PROPN (B) models NOUN (I)] offer VERB (O) a NOUN (O) ew NOUN (O) key NOUN (O) benefits NOUN (O) : 
 • X (O) Transfer NOUN (O) learning NOUN (O) : We PRON (O) can VERB (O) try VERB (O) to ADP (O) make VERB (O) use NOUN (O) of ADP (O) high ADJ - resource NOUN (O) languages NOUN (O) for ADP (O) training NOUN (O) [TTS PROPN (B) systems NOUN (I)] for ADP (O) low ADJ - resource NOUN (O) lan PROPN , (O) e.g. ADV , (O) via ADP (O) transfer NOUN (O) learning NOUN (O) approaches VERB . (O) 
 • X (O) Knowledge NOUN (O) sharing NOUN (O) : We PRON (O) may VERB (O) think VERB (O) of ADP (O) using VERB (O) [multilingual ADJ (B) data NOUN (I)] for ADP (O) joint NOUN (O) training NOUN (O) of ADP (O) a NOUN (O) single ADJ (O) shared VERB (O) [text NOUN - to ADP - speech NOUN (B) model NOUN (I)] . 
 Inuitively ADV , (O) this DET (O) enables VERB (O) [cross ADJ - lingual ADJ (B) sharing NOUN (I)] of ADP (O) patterns NOUN (O) learned VERB (O) from ADP (O) data NOUN . (O) 
 The DET (O) only ADV (O) work NOUN (O) in ADP (O) this DET (O) area NOUN (O) to ADP (O) our DET (O) knowledge NOUN (O) is AUX (O) Prakash PROPN (O) et NOUN (O) al PROPN . ’s PROPN (O) study NOUN (O)     on ADP (O) [TTS PROPN (B)] for ADP (O) related ADJ (O) Indian PROPN (O) languages NOUN (O) using VERB (O) hand NOUN - built VERB (O) unified ADJ (O) [phoneme NOUN (B) representations NOUN (I)] . 
 [• PROPN (B) Voice PROPN (I)] cloning VERB (O) : Under ADP (O) certain ADJ (O) circumstances NOUN , (O) producing VERB (O) [speech NOUN (B)] in ADP (O) multiple NOUN (O) languages NOUN (O) with ADP (O) the DET (O) [same ADJ (B) voice NOUN (I)] , i.e. X , (O) [crosslingual NOUN (B) voice NOUN (I)] cloning VERB , (O) is AUX (O) desired VERB . (O) 
 However ADV , (O) [audio NOUN (B) data NOUN (I)] where ADV (O) a NOUN (O) [single ADJ (B) speaker NOUN (I)] speaks VERB (O) several ADJ (O) languages NOUN (O) is AUX (O) scarce ADJ . (O) 
 That DET (O) is AUX (O) why ADV (O) [multilingual ADJ (B) voice NOUN (I)]-cloning ADJ systems NOUN (O) should VERB (O) be AUX (O) trainable NOUN (O) using VERB (O) mixtures NOUN (O) of ADP (O) [monolingual ADJ (B) data NOUN (I)] . Here ADV , (O) used VERB (O) [Tacotron PROPN (B) 2 NUM (I)] conditioned VERB (O) on ADP (O) [phonemes NOUN (B)] and CCONJ (O) showed VERB (O) voice NOUN - cloning VERB (O) abilities NOUN (O) on ADP (O) English PROPN , (O) Spanish PROPN , (O) and CCONJ (O) Chinese PROPN . (O) 
 Nachmani PROPN (O) and CCONJ (O) Wolf PROPN (O) extended ADJ (O) [Voice PROPN (B) Loop PROPN (I)]     and CCONJ (O) enabled VERB (O) [voice NOUN (B) conversion NOUN (I)] for ADP (O) English PROPN , (O) Spanish PROPN , (O) and CCONJ (O) German ADJ . (O) Chen PROPN (O) et NOUN (O) al PROPN . (O) 
 used VERB (O) a NOUN (O) [phoneme NOUN - based VERB (B) Tacotron PROPN (I) 2 NUM (I)] with ADP (O) a NOUN (O) ResCNN NOUN (O) based VERB (O) [speaker NOUN (B) encoder NOUN (I)]     that DET (O) enables VERB (O) a NOUN (O) massively ADV (O) [multi ADJ - speaker ADJ (B) speech NOUN (I) synthesis NOUN (I)] , even ADV (O) with ADP (O) [fictitious PROPN (B) voices NOUN (I)] . 
 • X (O) Code PROPN (O) switching NOUN (O) : In ADP (O) this DET (O) task NOUN (O) closely ADV (O) related ADJ (O) to ADP (O) [cross ADJ - lingual ADJ (B) voice NOUN (I)] cloning VERB , (O) we PRON (O) would VERB (O) like INTJ (O) to ADP (O) alternate NOUN (O) languages NOUN (O) within ADP (O) sentences NOUN . (O) 
 This DET (O) is AUX (O) useful ADJ (O) for ADP (O) foreign ADJ (O) names NOUN (O) in ADP (O) navigation NOUN (O) systems NOUN (O) or CCONJ (O) news NOUN (O) readers NOUN . (O) In ADP (O) view NOUN (O) of ADP (O) that SCONJ , (O) Cao PROPN (O) et NOUN (O) al PROPN . (O) modified VERB (O) [Tacotron PROPN (B)] ; their DET (O) model NOUN (O) uses VERB (O) [language NOUN - specific ADJ (B) encoders NOUN (I)] . 
 Code NOUN - switching VERB (O) itself PRON (O) is AUX (O) done VERB (O) by ADP (O) combining VERB (O) of ADP (O) their DET (O) outputs NOUN . (O) 
 Overall ADV , (O) all DET (O) recent ADJ (O) [multilingual ADJ (B) text NOUN - to ADP - speech NOUN (I) systems NOUN (I)] were AUX (O) only ADV (O) tested VERB (O) in ADP (O) 2 NUM - 3 NUM (O) languages NOUN (O) simultaneously ADV , (O) or CCONJ (O) required VERB (O) vast ADJ (O) amounts VERB (O) of ADP (O) data NOUN (O) to ADP (O) be AUX (O) trained VERB . (O) 

[Model NOUN (B) Architecture PROPN (I)] 
 We PRON (O) base NOUN (O) our DET (O) experiments VERB (O) on ADP (O) [Tacotron PROPN (B) 2 NUM (I)] . We PRON (O) focus NOUN (O) on ADP (O) the DET (O) [spectrogram NOUN (B) generation NOUN (I)] part NOUN (O) here ADV (O) ; for ADP (O) [vocoding VERB (B)] , we PRON (O) use NOUN (O) [WaveRNN PUNCT (B)] in ADP (O) all DET (O) our DET (O) configurations NOUN . (O) 
 We PRON (O) first ADJ (O) explain VERB (O) our DET (O) new ADJ (O) model NOUN (O) that SCONJ (O) uses VERB (O) meta NOUN - learning NOUN (O) for ADP (O) multilingual ADJ (O) knowledge NOUN (O) sharing NOUN (O) in ADP (O) Sec PROPN . , (O) then ADV (O) describe VERB (O) contrastive ADJ (O) baseline NOUN (O) models NOUN (O) which DET (O) are AUX (O) based VERB (O) on ADP (O) recent ADJ (O) [multilingual ADJ (B) TTS PROPN (I) architectures VERB (I)] (Sec PROPN .) . (O) 

Table NOUN (O) : [Total NOUN (B) data NOUN (I)] sizes VERB (O) per ADP (O) language NOUN (O) (hours NOUN (O) of ADP (O) [audio NOUN (B) data NOUN (I)]) in ADP (O) our DET (O) cleaned VERB (O) [CSS10 PROPN (B)] (CSS NOUN) (O) and CCONJ (O) [Common PROPN (B) Voice PROPN (I)] (CV PROPN) (O) subsets NOUN . (O) 

Our DET (O) Model NOUN (O) : Generated VERB (O) (GEN PROPN) (O) 
 We PRON (O) introduce VERB (O) a NOUN (O) scalable ADJ (O) [multilingual ADJ (B) text NOUN - to ADP - speech NOUN (I) model NOUN (I)] that DET (O) follows VERB (O) a NOUN (O) meta NOUN - learning NOUN (O) approach NOUN (O) of ADP (O) contextual PROPN (O) parameter NOUN (O) generation NOUN (O) proposed VERB (O) by ADP (O) for ADP (O) [NMT PROPN (B)] (see VERB (O) Fig PROPN .) . (O) 
 We PRON (O) call NOUN (O) the DET (O) model NOUN (O) generated VERB (O) (G NOUN (O) EN PROPN) (O) further NOUN (O) in ADP (O) this DET (O) text NOUN . (O) The DET (O) backbone NOUN (O) of ADP (O) our DET (O) model NOUN (O) is AUX (O) built VERB (O) on ADP (O) our DET (O) own ADJ (O) implementation NOUN (O) of ADP (O) [Tacotron PROPN (B) 2 NUM (I)] , composed VERB (O) of ADP (O) these DET (O) main ADJ (O) components NOUN (O) : 
 an DET (O) input NOUN (O) text NOUN (O) [encoder NOUN (B)] that DET (O) includes VERB (O) a NOUN (O) stack NOUN (O) of ADP (O) [convolutional NOUN (B) layers NOUN (I)] and CCONJ (O) a NOUN (O) [bidirectional ADJ (B) LSTM PROPN (I)] , 
 a DET (O) location NOUN - sensitive ADJ (O) [attention NOUN (B) mechanism NOUN (I)] with ADP (O) the DET (O) guided VERB (O) attention NOUN (O) loss NOUN (O) term NOUN (O) that SCONJ (O) supports VERB (O) faster ADV (O) convergence NOUN , (O) 
 a NOUN (O) [decoder NOUN (B)] with ADP (O) two NUM (O) stacked VERB (O) [LSTM PROPN (B) layers NOUN (I)] where ADV (O) the DET (O) first ADJ (O) queries VERB (O) the DET (O) [attention NOUN (B) mechanism NOUN (I)] and CCONJ (O) the DET (O) second NOUN (O) generates VERB (O) outputs NOUN . (O) 
 We PRON (O) increase NOUN (O) tolerance NOUN (O) of ADP (O) the DET (O) guided VERB (O) attention NOUN (O) loss NOUN (O) exponentially ADV (O) during ADP (O) training NOUN . (O) 
 We PRON (O) propose NOUN (O) the DET (O) following VERB (O) changes VERB (O) to ADP (O) this DET (O) basic ADJ (O) architecture NOUN (O) : 
 [Convolutional PROPN (B) Encoders PROPN (I)] : We PRON (O) use NOUN (O) multiple NOUN (O) [language NOUN - specific ADJ (B) input NOUN (I) text NOUN (I) encoders NOUN (I)] . 
 However ADV , (O) having VERB (O) a NOUN (O) separate ADJ (O) [encoder NOUN (B)] with ADP (O) [recurrent NOUN (B) layers NOUN (I)] for ADP (O) each DET (O) language NOUN (O) is AUX (O) not PART (O) practical ADJ (O) as SCONJ (O) it PRON (O) involves VERB (O) passing NOUN (O) the DET (O) training NOUN (O) batches NOUN (O) (which DET (O) should VERB (O) be AUX (O) balanced ADJ (O) with ADP (O) respect NOUN (O) to ADP (O) languages NOUN) (O) through ADP (O) multiple NOUN (O) [encoders NOUN (B)] sequentially ADV . (O) 
 Therefore ADV , (O) we PRON (O) use NOUN (O) a NOUN (O) fully ADV (O) [convolutional NOUN (B) encoder NOUN (I)] from ADP (O) DCTTS PROPN . (O) 
 The DET (O) [encoders NOUN (B)] use NOUN (O) grouped VERB (O) layers NOUN (O) and CCONJ (O) are AUX (O) thus ADV (O) processed VERB (O) effectively ADV . (O) 
 We PRON (O) enhance NOUN (O) the DET (O) [encoders NOUN (B)] with ADP (O) [batch NOUN (B) normalization NOUN (I)] and CCONJ (O) [dropout NOUN (B)] with ADP (O) a NOUN (O) very ADV (O) low NOUN (O) rate NOUN . (O) 
 The DET (O) normalization NOUN (O) layers NOUN (O) are AUX (O) situated ADJ (O) before ADP (O) activations VERB (O) and CCONJ (O) [dropouts NOUN (B)] after ADP (O) them PRON . (O) 

[Encoder PROPN (B) parameter NOUN (I)] generation NOUN (O) : To PART (O) enable NOUN (O) [cross ADJ - lingual ADJ (B) knowledge NOUN (I)]-sharing NOUN , parameters NOUN (O) of ADP (O) the DET (O) [encoders NOUN (B)] are AUX (O) generated VERB (O) using VERB (O) a NOUN (O) separate ADJ (O) network NOUN (O) conditioned VERB (O) on ADP (O) language NOUN (O) embeddings NOUN . (O) 
 The DET (O) parameter NOUN (O) generator NOUN (O) is AUX (O) composed VERB (O) of ADP (O) multiple NOUN (O) site NOUN - specific ADJ (O) generators NOUN , (O) each DET (O) of ADP (O) which DET (O) takes VERB (O) a NOUN (O) [language NOUN (B) embedding NOUN (I)] on ADP (O) the DET (O) input NOUN (O) and CCONJ (O) produces VERB (O) parameters NOUN (O) for ADP (O) one NUM (O) layer NOUN (O) of ADP (O) the DET (O) [convolutional NOUN (B) encoder NOUN (I)] for ADP (O) the DET (O) given VERB (O) language NOUN . (O) 
 The DET (O) generators NOUN (O) enable NOUN (O) a NOUN (O) controllable ADJ (O) [cross ADJ - lingual ADJ (B) parameter NOUN (I)] sharing VERB (O) because SCONJ (O) reduction NOUN (O) of ADP (O) their DET (O) size NOUN (O) prevents VERB (O) generation NOUN (O) of ADP (O) highly ADV (O) [language NOUN - specific ADJ (B) parameters NOUN (I)] . 
 We PRON (O) implement VERB (O) them PRON (O) as SCONJ (O) fully ADV (O) connected VERB (O) layers NOUN . (O) 

Training NOUN (O) with ADP (O) multilingual ADJ (O) batches NOUN (O) : We PRON (O) construct VERB (O) unusual ADJ (O) training NOUN (O) batches NOUN (O) to ADP (O) fully ADV (O) utilize VERB (O) the DET (O) potential NOUN (O) of ADP (O) this DET (O) architecture NOUN . (O) 
 We PRON (O) would VERB (O) like INTJ (O) to ADP (O) have AUX (O) a NOUN (O) batch NOUN (O) of ADP (O) B (O) examples VERB (O) that SCONJ (O) can VERB (O) be AUX (O) reshaped X (O) into ADP (O) a NOUN (O) batch NOUN (O) of ADP (O) size NOUN (O) B (O) / L NOUN (O) where ADV (O) L NOUN (O) is AUX (O) the DET (O) number NOUN (O) of ADP (O) [encoder NOUN (B)] groups NOUN (O) or CCONJ (O) languages NOUN . (O) 
 This DET (O) new ADJ (O) batch NOUN (O) should VERB (O) have AUX (O) a NOUN (O) new ADJ (O) dimension NOUN (O) that SCONJ (O) groups NOUN (O) all DET (O) examples VERB (O) with ADP (O) the DET (O) same ADJ (O) language NOUN . (O) 
 Thus ADV (O) we PRON (O) use NOUN (O) a NOUN (O) batch NOUN (O) sampler NOUN (O) that SCONJ (O) creates VERB (O) batches NOUN (O) where ADV (O) for ADP (O) each DET (O) l NOUN (O) < L NOUN (O) and CCONJ (O) i PRON (O) < B (O) / L NOUN , (O) all DET (O) (l NOUN (O) + iL)-th ADJ (O) examples VERB (O) are AUX (O) of ADP (O) the DET (O) same ADJ (O) language NOUN . (O) 

Speaker PROPN (O) embedding NOUN (O) : We PRON (O) extend VERB (O) the DET (O) model NOUN (O) with ADP (O) a NOUN (O) [speaker NOUN (B) embedding NOUN (I)] which DET (O) is AUX (O) concatenated VERB (O) with ADP (O) each DET (O) element NOUN (O) of ADP (O) the DET (O) encoded VERB (O) sequence NOUN (O) that SCONJ (O) is AUX (O) attended VERB (O) by ADP (O) the DET (O) [decoder NOUN (B)] while SCONJ (O) generating NOUN (O) [spectrogram NOUN (B) frames NOUN (I)] . 
 This DET (O) makes VERB (O) the DET (O) model NOUN (O) [multi ADJ - speaker ADJ (B)] and CCONJ (O) allows VERB (O) [cross ADJ - lingual ADJ (B) voice NOUN (I) cloning VERB (I)] . 

[Adversarial DET (B) speaker NOUN (I) classifier NOUN (I)] : We PRON (O) combine VERB (O) the DET (O) model NOUN (O) with ADP (O) an DET (O) [adversarial ADJ (B) speaker NOUN (I) classifier NOUN (I)] to PART (O) boost NOUN (O) voice NOUN (O) cloning VERB . (O) 
 The DET (O) classifier NOUN (O) follows VERB (O) principles VERB (O) of ADP (O) domain NOUN (O) adversarial ADJ (O) training NOUN (O) and CCONJ (O) is AUX (O) used VERB (O) to ADP (O) proactively ADV (O) remove VERB (O) speaker NOUN - specific ADJ (O) information NOUN (O) from ADP (O) the DET (O) [encoders NOUN (B)] . 
 It PRON (O) includes VERB (O) a NOUN (O) single ADJ (O) [hidden VERB (B) layer NOUN (I)] , a DET (O) [softmax PROPN (B) layer NOUN (I)] , and CCONJ (O) a NOUN (O) [gradient NOUN (B) reversal NOUN (I) layer NOUN (I)] that DET (O) scales VERB (O) the DET (O) gradient NOUN (O) flowing VERB (O) to ADP (O) the DET (O) [encoders NOUN (B)] by ADP (O) a NOUN (O) factor NOUN (O) – PUNCT (O) λ PROPN . (O) 
 The DET (O) gradients VERB (O) are AUX (O) clipped VERB (O) to ADP (O) stabilize VERB (O) training NOUN . (O) 
 It PRON (O) is AUX (O) optimized VERB (O) to ADP (O) reduce VERB (O) the DET (O) cross NOUN - entropy ADJ (O) of ADP (O) [speaker NOUN (B) predictions NOUN (I)] . 
 The DET (O) predictions NOUN (O) are AUX (O) done VERB (O) separately ADV (O) for ADP (O) each DET (O) element NOUN (O) of ADP (O) the DET (O) [encoders NOUN (B)] ’ PUNCT outputs NOUN . (O) 
  
Baselines PROPN (O) : Shared VERB , (O) Separate PROPN (O) & Single PROPN (O) 
 We PRON (O) compare VERB (O) GEN PROPN (O) with ADP (O) baseline NOUN (O) models NOUN (O) called VERB (O) shared VERB (O) (SHA PROPN) , (O) separate ADJ (O) (SEP PROPN) , (O) and CCONJ (O) single ADJ (O) (SGL PROPN) . (O) 
 SGL PROPN (O) is AUX (O) a NOUN (O) basic ADJ (O) [Tacotron PROPN (B) 2 NUM (I) model NOUN (I)] , SHA PROPN (O) and CCONJ (O) SEP PROPN (O) follow NOUN (O) the DET (O) recent ADJ (O) [multilingual ADJ (B) TTS PROPN (I)] works VERB (O) of ADP (O) Zhang PROPN (O) et NOUN (O) al PROPN . (O) 
 respectively ADV , (O) but CCONJ (O) were AUX (O) slightly ADV (O) adapted VERB (O) to ADP (O) our DET (O) tasks NOUN (O) for ADP (O) a NOUN (O) fairer NOUN (O) comparison NOUN (O) to ADP (O) GEN PROPN (O) – PUNCT (O) we PRON (O) use NOUN (O) more ADJ (O) languages NOUN (O) and CCONJ (O) [less ADJ (B) data NOUN (I)] than SCONJ (O) the DET (O) original ADJ (O) works VERB . (O) 
 In ADP (O) the DET (O) following VERB , (O) we PRON (O) only ADV (O) describe VERB (O) their DET (O) differences NOUN (O) from ADP (O) G NOUN (O) EN PROPN . (O) 
 Single PROPN (O) (SGL PROPN) (O) represents VERB (O) a NOUN (O) set NOUN (O) of ADP (O) monolingual NOUN (O) models NOUN (O) that SCONJ (O) follow NOUN (O) [vanilla NOUN (B) Tacotron PROPN (I) 2 NUM (I)] with ADP (O) the DET (O) original ADJ (O) recurrent NOUN (O) [encoder NOUN (B)] and CCONJ (O) default NOUN (O) settings NOUN . (O) 
 SGL PROPN (O) can VERB (O) not PART (O) be AUX (O) used VERB (O) for ADP (O) [code NOUN - switching VERB (B)] . 
 Shared PROPN (O) (SHA PROPN) (O) : Unlike ADP (O) G NOUN (O) EN PROPN , (O) S NOUN (O) HA PROPN (O) has AUX (O) a NOUN (O) single ADJ (O) [encoder NOUN (B)] with ADP (O) the DET (O) original ADJ (O) [Tacotron PROPN (B) 2 NUM (I) architecture NOUN (I)] , so CCONJ (O) it PRON (O) fully ADV (O) shares NOUN (O) all DET (O) [encoder NOUN (B) parameters NOUN (I)] . 
 This DET (O) sharing NOUN (O) implicitly ADV (O) leads VERB (O) to ADP (O) [language NOUN - independent ADJ (B) encoder NOUN (I) outputs NOUN (I)] . 
 The DET (O) [language NOUN - dependent ADJ (B) processing NOUN (I)] happens VERB (O) in ADP (O) the DET (O) [decoder NOUN (B)] , so CCONJ (O) the DET (O) [speaker NOUN (B) embeddings NOUN (I)] are AUX (O) explicitly ADV (O) factorized ADJ (O) into ADP (O) speaker NOUN (O) and CCONJ (O) language NOUN (O) parts NOUN . (O) 
 Separate PROPN (O) (SEP PROPN) (O) uses VERB (O) multiple NOUN (O) [language NOUN - specific ADJ (B) convolutional ADJ (I) encoders NOUN (I)] too ADV , (O) but CCONJ (O) their DET (O) parameters NOUN (O) are AUX (O) not PART (O) generated VERB . (O) 
 It PRON (O) also ADV (O) does AUX (O) not PART (O) include VERB (O) the DET (O) [adversarial ADJ (B) speaker NOUN (I) classifier NOUN (I)] . 

Dataset PROPN (O) 
 We PRON (O) created VERB (O) a NOUN (O) [new PROPN (B) dataset NOUN (I)] for ADP (O) our DET (O) experiments NOUN , (O) based VERB (O) on ADP (O) carefully ADV (O) cleaning NOUN (O) and CCONJ (O) preprocessing VERB (O) freely ADV (O) available ADJ (O) [audio NOUN (B)] sources NOUN (O) : 
 [CSS10 PROPN (B)] and CCONJ (O) a NOUN (O) small ADJ (O) fraction NOUN (O) of ADP (O) [Common PROPN (B) Voice PROPN (I)] . Table NOUN (O) shows VERB (O) total NOUN (O) durations NOUN (O) of ADP (O) the DET (O) used VERB (O) [audio NOUN (B) data NOUN (I)] per ADP (O) language NOUN . (O) 

[CSS10 PROPN (B)] 
 [CSS10 PROPN (B)] consists VERB (O) of ADP (O) mono-[speaker NOUN (B) data NOUN (I)] in ADP (O) German ADJ , (O) Greek PROPN , (O) Spanish PROPN , (O) Finnish PROPN , (O) French PROPN , (O) Hungarian PROPN , (O) Japanese PROPN , (O) Dutch PROPN , (O) Russian PROPN , (O) and CCONJ (O) Chinese PROPN . (O) 
 It PRON (O) was AUX (O) created VERB (O) from ADP (O) audiobooks VERB (O) and CCONJ (O) contains VERB (O) various ADJ (O) punctuation NOUN (O) styles NOUN . (O) 
 We PRON (O) applied VERB (O) an DET (O) automated VERB (O) cleaning NOUN (O) to ADP (O) normalize VERB (O) transcripts NOUN (O) across ADP (O) languages NOUN , (O) including VERB (O) punctuation NOUN (O) and CCONJ (O) some DET (O) spelling NOUN (O) variants NOUN (O) (e.g. ADV , (O) “ PUNCT (O) œ NOUN (O) ” PUNCT (O) → NOUN (O) “ PUNCT (O) oe NOUN (O) ” PUNCT) . (O) 
 We PRON (O) romanized VERB (O) Japanese PROPN (O) with ADP (O) MeCab PROPN (O) and CCONJ (O) Romkan PROPN , (O) Chinese PROPN (O) using VERB (O) Pinyin PROPN . (O) 
 We PRON (O) further NOUN (O) filtered VERB (O) the DET (O) data NOUN (O) to ADP (O) remove VERB (O) any DET (O) potentially ADV (O) problematic ADJ (O) transcripts NOUN (O) : we PRON (O) preserved VERB (O) just ADV (O) examples VERB (O) with ADP (O) 0.5 NUM - 10 NUM . (O) 
 1s NOUN (O) of ADP (O) [audio NOUN (B)] and CCONJ (O) 3 NUM - 190 NUM (O) transcript NOUN (O) characters NOUN . (O) 
 We PRON (O) computed VERB (O) means VERB (O) µ NOUN (O) and CCONJ (O) variances NOUN (O) σ NOUN (O) of ADP (O) [audio NOUN (B)] durations NOUN (O) of ADP (O) groups NOUN (O) corresponding VERB (O) to ADP (O) examples VERB (O) with ADP (O) the DET (O) same ADJ (O) transcript NOUN (O) lengths NOUN . (O) 
 Then ADV (O) we PRON (O) removed VERB (O) those DET (O) with ADP (O) durations NOUN (O) outside ADV (O) the DET (O) interval NOUN (O) (µ NOUN (O) – PUNCT (O) 3σ NOUN , (O) µ NOUN (O) + 3σ NOUN) . (O) 
 In ADP (O) total NOUN , (O) the DET (O) resulting VERB (O) dataset NOUN (O) includes VERB (O) 125.26 NUM (O) hours NOUN (O) of ADP (O) recordings NOUN . (O) 


Table NOUN (O) : Left NOUN (O) : CERs PROPN (O) of ADP (O) [ground NOUN - truth NOUN (B) recordings NOUN (I)] (GT PROPN) (O) and CCONJ (O) recordings VERB (O) produced VERB (O) by ADP (O) monolingual NOUN (O) and CCONJ (O) the DET (O) three NUM (O) examined VERB (O) [multilingual ADJ (B) models NOUN (I)] . 
 Right INTJ (O) : CERs PROPN (O) of ADP (O) the DET (O) recordings VERB (O) synthesized VERB (O) by ADP (O) G NOUN (O) EN PROPN (O) and CCONJ (O) S NOUN (O) HA PROPN (O) trained VERB (O) on ADP (O) just ADV (O) 600 NUM (O) or CCONJ (O) 900 NUM (O) training NOUN (O) examples VERB (O) per ADP (O) language NOUN . (O) 
 Best PROPN (O) results VERB (O) for ADP (O) the DET (O) given VERB (O) language NOUN (O) are AUX (O) shown VERB (O) in ADP (O) bold ADJ (O) ; “ PUNCT (O) * ” PUNCT (O) denotes VERB (O) statistical ADJ (O) significance NOUN (O) (established VERB (O) using VERB (O) paired VERB (O) t NOUN - test NOUN (O) ; p NOUN (O) < 0.05 NUM) . (O) 



[Common PROPN (B) Voice PROPN (I)] 
 To PART (O) train NOUN (O) code NOUN - switching VERB (O) models NOUN , (O) [multi ADJ - speaker ADJ (B) data NOUN (I)] is AUX (O) required VERB (O) to ADP (O) disentangle NOUN (O) the DET (O) connection NOUN (O) between ADP (O) languages NOUN (O) and CCONJ (O) speakers NOUN . (O) 
 We PRON (O) thus ADV (O) enhanced VERB (O) [CSS10 PROPN (B)] with ADP (O) data NOUN (O) from ADP (O) [Common PROPN (B) Voice PROPN (I)] (CV PROPN) (O) for ADP (O) languages NOUN (O) included VERB (O) in ADP (O) both DET (O) sets VERB (O) – PUNCT (O) the DET (O) intersection NOUN (O) covers VERB (O) German ADJ , (O) French PROPN , (O) Chinese PROPN , (O) Dutch PROPN , (O) Russian PROPN , (O) Japanese PROPN , (O) and CCONJ (O) Spanish PROPN . (O) 
 Since SCONJ (O) CV NOUN (O) is AUX (O) mainly ADV (O) aimed VERB (O) at ADP (O) [speech NOUN (B) recognition NOUN (I)] and CCONJ (O) rather ADV (O) noisy PROPN , (O) we PRON (O) performed VERB (O) extensive ADJ (O) filtering VERB (O) : We PRON (O) removed VERB (O) recordings VERB (O) with ADP (O) a NOUN (O) negative ADJ (O) rating NOUN (O) (as SCONJ (O) provided VERB (O) by ADP (O) CV PROPN (O) for ADP (O) each DET (O) example NOUN) (O) and CCONJ (O) excluded VERB (O) any DET (O) speakers NOUN (O) with ADP (O) less ADJ (O) than SCONJ (O) 50 NUM (O) recordings NOUN . (O) 
 We PRON (O) checked VERB (O) a NOUN (O) sample NOUN (O) of ADP (O) recordings VERB (O) for ADP (O) each DET (O) speaker NOUN , (O) and CCONJ (O) we PRON (O) removed VERB (O) all DET (O) their DET (O) data NOUN (O) if SCONJ (O) we PRON (O) considered VERB (O) the DET (O) sample NOUN (O) to ADP (O) have AUX (O) poor ADJ (O) quality NOUN . (O) 
 This DET (O) resulted VERB (O) in ADP (O) a NOUN (O) [small ADJ (B) dataset NOUN (I)] of ADP (O) 39 NUM (O) German ADJ , (O) 22 NUM (O) French PROPN , (O) 11 NUM (O) Dutch PROPN , (O) 6 NUM (O) Chinese PROPN , (O) and CCONJ (O) 6 NUM (O) [Russian PROPN (B) speakers NOUN (I)] . 
 Japanese PROPN (O) and CCONJ (O) [Spanish PROPN (B) data NOUN (I)] were AUX (O) removed VERB (O) completely ADV . (O) 
 A NOUN (O) lot NOUN (O) of ADP (O) recordings VERB (O) in ADP (O) CV NOUN (O) contain NOUN (O) artifacts NOUN (O) at ADP (O) the DET (O) beginning NOUN (O) or CCONJ (O) end NOUN . (O) 
 Thus ADV (O) we PRON (O) semi ADV - automatically ADV (O) cleaned VERB (O) leading VERB (O) and CCONJ (O) trailing VERB (O) segments NOUN (O) of ADP (O) all DET (O) recordings NOUN . (O) 
 The DET (O) dataset NOUN (O) has AUX (O) 13.7 NUM (O) hours NOUN (O) of ADP (O) [audio NOUN (B) data NOUN (I)] in ADP (O) total NOUN . (O) 

Experiments NOUN (O) 
 We PRON (O) compare VERB (O) our DET (O) models NOUN (O) described VERB (O) in ADP (O) Section NOUN . (O) 
 The DET (O) experiment NOUN (O) in ADP (O) Section NOUN (O) was AUX (O) designed VERB (O) to ADP (O) show NOUN (O) stability NOUN (O) and CCONJ (O) ability NOUN (O) to ADP (O) train NOUN (O) on ADP (O) lower ADJ (O) amounts VERB (O) of ADP (O) data NOUN . (O) 
 We PRON (O) conclude VERB (O) that DET (O) character NOUN (O) error NOUN (O) rate NOUN (O) (CER PROPN) (O) evaluation NOUN (O) is AUX (O) sufficient ADJ (O) for ADP (O) this DET (O) experiment NOUN . (O) In ADP (O) Section NOUN , (O) we PRON (O) test NOUN (O) pronunciation NOUN (O) accuracy NOUN (O) and CCONJ (O) [voice NOUN (B) quality NOUN (I)] of ADP (O) code NOUN - switching VERB (O) synthesis NOUN . (O) 
 We PRON (O) used VERB (O) a NOUN (O) subjective ADJ (O) evaluation NOUN (O) test NOUN (O) as SCONJ (O) there PRON (O) are AUX (O) no DET (O) straightforward NOUN (O) objective NOUN (O) metrics NOUN (O) for ADP (O) this DET (O) task NOUN . (O) 
 We PRON (O) used VERB (O) the DET (O) same ADJ (O) [vocoder NOUN (B)] for ADP (O) all DET (O) models NOUN , (O) i.e. X , (O) the DET (O) [WaveRNN PUNCT (B) model NOUN (I)] trained VERB (O) on ADP (O) a NOUN (O) training NOUN (O) subset NOUN (O) of ADP (O) the DET (O) cleaned VERB (O) [CSS10 PROPN (B) dataset NOUN (I)] . 

Multilingual ADJ (O) training NOUN (O) 
 Training NOUN (O) setup NOUN (O) : We PRON (O) used VERB (O) our DET (O) cleaned VERB (O) [CSS10 PROPN (B) dataset NOUN (I)] for ADP (O) training NOUN (O) ; 64 NUM (O) randomly ADV (O) selected VERB (O) samples NOUN (O) per ADP (O) language NOUN (O) were AUX (O) reserved VERB (O) for ADP (O) validation NOUN (O) and CCONJ (O) another DET (O) 64 NUM (O) for ADP (O) testing NOUN . (O) 
 We PRON (O) did AUX (O) not PART (O) have AUX (O) an DET (O) ambition NOUN (O) to ADP (O) [clone NOUN (B) voices NOUN (I)] in ADP (O) this DET (O) experiment NOUN , (O) so CCONJ (O) we PRON (O) switched VERB (O) off NOUN (O) [speaker NOUN (B) classifiers NOUN (I)] for ADP (O) SHA PROPN (O) and CCONJ (O) GEN PROPN (O) (i.e. X , (O) SHA PROPN (O) was AUX (O) reduced VERB (O) to ADP (O) the DET (O) [vanilla NOUN (B) Tacotron PROPN (I) 2 NUM (I) model NOUN (I)] with ADP (O) a NOUN (O) language NOUN (O) embedding NOUN) . (O) 
 We PRON (O) trained VERB (O) the DET (O) three NUM (O) models NOUN (O) for ADP (O) 50k NUM (O) steps NOUN (O) with ADP (O) the DET (O) Adam PROPN (O) optimizer.2 NOUN (O) We PRON (O) used VERB (O) a NOUN (O) stepped VERB (O) learning NOUN (O) rate NOUN (O) that SCONJ (O) starts VERB (O) from ADP (O) 10–3 NUM (O) and CCONJ (O) halves VERB (O) every DET (O) 10k NOUN (O) steps NOUN . (O) 
 In ADP (O) the DET (O) case NOUN (O) of ADP (O) S PROPN (O) EP PROPN , (O) we PRON (O) used VERB (O) a NOUN (O) lower ADJ (O) initial NOUN (O) learning NOUN (O) rate NOUN (O) 10–4 PROPN . (O) 
 For ADP (O) SGL PROPN , (O) the DET (O) learning NOUN (O) rate NOUN (O) schedule NOUN (O) was AUX (O) tuned VERB (O) individually ADV (O) per ADP (O) language NOUN . (O) 
 We PRON (O) stopped VERB (O) training NOUN (O) early ADV (O) after ADP (O) [validation NOUN (B) data NOUN (I)] loss NOUN (O) started VERB (O) increasing VERB . (O) 
 SHA PROPN , (O) SEP PROPN , (O) and CCONJ (O) GEN PROPN (O) used VERB (O) [speaker NOUN (B) embeddings NOUN (I)] of ADP (O) size NOUN (O) 32 NUM (O) and CCONJ (O) G NOUN (O) EN PROPN (O) used VERB (O) language NOUN (O) embeddings NOUN (O) and CCONJ (O) parameter NOUN (O) generators NOUN (O) of ADP (O) size NOUN (O) 10 NUM (O) and CCONJ (O) 8 NUM , (O) respectively ADV . (O) 
 We PRON (O) used VERB (O) language NOUN - balanced VERB (O) batches NOUN (O) of ADP (O) size NOUN (O) 60 NUM (O) for ADP (O) all DET (O) models NOUN . (O) 

With ADP (O)      β1 PROPN (O) = 0.9 NUM , (O) β2 PROPN (O) = 0.999 NUM , (O) =10–6 PROPN , (O) and CCONJ (O) weight NOUN (O) decay NOUN (O) of ADP (O) 10–6 NOUN (O) 

Evaluation NOUN (O) : We PRON (O) synthesized VERB (O) [evaluation NOUN (B) data NOUN (I)] using VERB (O) all DET (O) the DET (O) models NOUN (O) followed VERB (O) by ADP (O) [WaveRNN PUNCT (B)] and CCONJ (O) we PRON (O) sent VERB (O) the DET (O) synthesized VERB (O) recordings VERB (O) to ADP (O) Google PROPN (O) Cloud PROPN (O) Platform NOUN (O) [ASR PROPN (B)] . 
 Then ADV (O) we PRON (O) computed VERB (O) CERs PROPN (O) between ADP (O) [ground NOUN - truth NOUN (B)] and CCONJ (O) [ASR PROPN (B)]-produced PROPN transcripts NOUN (O) (we PRON (O) used VERB (O) the DET (O) native NOUN (O) symbols NOUN (O) for ADP (O) Chinese PROPN (O) and CCONJ (O) Japanese PROPN) . (O) 

Results NOUN (O) : Table NOUN (O) summarizes VERB (O) the DET (O) obtained VERB (O) CERs PROPN . (O) The DET (O) first ADJ (O) column NOUN (O) gives VERB (O) us PROPN (O) a NOUN (O) notion NOUN (O) about ADP (O) the DET (O) performance NOUN (O) of ADP (O) the DET (O) [ASR PROPN (B)] engine NOUN . (O) 
 The DET (O) rates NOUN (O) stay VERB (O) below ADP (O) 20 NUM (O) % for ADP (O) all DET (O) languages NOUN (O) ; higher ADJ (O) CERs PROPN (O) are AUX (O) mostly ADV (O) caused VERB (O) by ADP (O) noisy PROPN (O) [CSS10 PROPN (B)] recordings NOUN . (O) 
 We PRON (O) were AUX (O) not PART (O) able ADJ (O) to ADP (O) train NOUN (O) the DET (O) Greek PROPN (O) S NOUN (O) GL PROPN (O) model NOUN (O) due ADJ (O) to ADP (O) low NOUN (O) amount NOUN (O) of ADP (O) [training NOUN (B) data NOUN (I)] . 
 The DET (O) [decoder NOUN (B)] started VERB (O) to ADP (O) overfit PROPN (O) soon ADV (O) before ADP (O) the DET (O) attention NOUN (O) could VERB (O) have AUX (O) been AUX (O) established VERB . (O) 
 The DET (O) performance NOUN (O) of ADP (O) SGL PROPN (O) is AUX (O) similar ADJ (O) to ADP (O) SHA PROPN (O) except SCONJ (O) for ADP (O) Chinese PROPN , (O) Finnish PROPN , (O) and CCONJ (O) Greek PROPN . (O) SEP PROPN (O) performed VERB (O) noticeably ADV (O) worse ADJ (O) than SCONJ (O) S NOUN (O) HA PROPN (O) or CCONJ (O) even ADV (O) S NOUN (O) GL PROPN . (O) 
 This DET (O) may VERB (O) be AUX (O) caused VERB (O) by ADP (O) the DET (O) imbalance NOUN (O) between ADP (O) the DET (O) [batch NOUN (B) size NOUN (I)] of ADP (O) the DET (O) [encoder NOUN (B)] and CCONJ (O) the DET (O) [decoder NOUN (B)] as SCONJ (O) the DET (O) [encoder NOUN (B)] ’s PART effective ADJ (O) [batch NOUN (B) size NOUN (I)] is AUX (O) just ADV (O) B (O) / L.4 PROPN (O) Sharing NOUN (O) of ADP (O) the DET (O) data NOUN (O) probably ADV (O) regularized VERB (O) the DET (O) [decoder NOUN (B)] , so CCONJ (O) the DET (O) attention NOUN (O) was AUX (O) established VERB (O) even ADV (O) in ADP (O) the DET (O) case NOUN (O) of ADP (O) Greek PROPN . (O) 
 GEN PROPN (O) seems VERB (O) to ADP (O) be AUX (O) significantly ADV (O) better ADJ (O) than SCONJ (O) S NOUN (O) HA PROPN (O) on ADP (O) most ADJ (O) languages NOUN . (O) It PRON (O) fulfills VERB (O) our DET (O) expectations NOUN (O) as SCONJ (O) G NOUN (O) EN PROPN (O) should VERB (O) be AUX (O) more ADJ (O) flexible ADJ . (O) 

Manual PROPN (O) error NOUN (O) analysis NOUN (O) : We PRON (O) manually ADV (O) inspected VERB (O) the DET (O) outputs VERB (O) in ADP (O) German ADJ , (O) French PROPN , (O) Spanish PROPN , (O) and CCONJ (O) Russian PROPN . (O) In ADP (O) the DET (O) case NOUN (O) of ADP (O) Spanish PROPN , (O) all DET (O) the DET (O) models NOUN (O) work NOUN (O) well INTJ (O) ; we PRON (O) noticed VERB (O) just ADV (O) differences NOUN (O) in ADP (O) the DET (O) treatment NOUN (O) of ADP (O) punctuation NOUN . (O) 
 German PROPN (O) outputs NOUN (O) by ADP (O) G NOUN (O) EN PROPN (O) seem VERB (O) to ADP (O) be AUX (O) the DET (O) best ADJ . (O) 
 Other PROPN (O) models NOUN (O) sometimes ADV (O) do AUX (O) unnatural NOUN (O) pauses VERB (O) when ADV (O) reaching VERB (O) a NOUN (O) punctuation NOUN (O) mark NOUN . (O) 
 Right INTJ (O) after ADP (O) the DET (O) pauses VERB , (O) they PRON (O) often ADV (O) skip NOUN (O) a NOUN (O) few ADJ (O) words NOUN . (O) G NOUN (O) EN PROPN (O) is AUX (O) noticeably ADV (O) better ADJ (O) on ADP (O) French PROPN (O) and CCONJ (O) Russian PROPN , (O) others NOUN (O) produce NOUN (O) obvious ADJ (O) mispronunciations NOUN . (O) 

Data NOUN - stress NOUN (O) training NOUN (O) : To PART (O) further NOUN (O) test NOUN (O) the DET (O) models NOUN (O) in ADP (O) data NOUN - stress NOUN (O) situations NOUN , (O) we PRON (O) chose VERB (O) random ADJ (O) subsets NOUN (O) of ADP (O) 600 NUM (O) and CCONJ (O) 900 NUM (O) examples VERB (O) per ADP (O) language NOUN (O) from ADP (O) the DET (O) training NOUN (O) set NOUN (O) (i.e. X , (O) about ADP (O) 80 NUM (O) or CCONJ (O) 120 NUM (O) minutes NOUN (O) of ADP (O) recordings NOUN , (O) respectively ADV) . (O) 
 We PRON (O) trained VERB (O) all DET (O) models NOUN (O) on ADP (O) both DET (O) reduced VERB (O) datasets VERB , (O) but CCONJ (O) accomplished VERB (O) training NOUN (O) just ADV (O) for ADP (O) S PROPN (O) HA PROPN (O) and CCONJ (O) G NOUN (O) EN PROPN . (O) 
 While SCONJ (O) training NOUN (O) on ADP (O) the DET (O) bigger ADJ (O) and CCONJ (O) [smaller ADJ (B) dataset NOUN (I)] , we PRON (O) decayed VERB (O) the DET (O) learning NOUN (O) rate NOUN (O) every DET (O) 7.5k NOUN (O) and CCONJ (O) 5k NUM (O) training NOUN (O) steps NOUN , (O) respectively ADV . (O) 
 The DET (O) right ADV (O) half NOUN (O) of ADP (O) Table NOUN (O) shows VERB (O) that SCONJ (O) G NOUN (O) EN PROPN (O) can VERB (O) work NOUN (O) better ADJ (O) even ADV (O) in ADP (O) data NOUN - stress NOUN (O) situations NOUN . (O) GEN PROPN (O) models NOUN (O) have AUX , (O) compared VERB (O) to ADP (O) SHA PROPN (O) models NOUN , (O) significantly ADV (O) better ADJ (O) CER PROPN (O) values NOUN (O) on ADP (O) six NUM (O) languages NOUN . (O) 

https://cloud.google.com/speech-to-text PROPN (O) 

Our DET (O) attempts VERB (O) to ADP (O) compensate NOUN (O) for ADP (O) this DET (O) using VERB (O) different ADJ (O) [encoder NOUN (B)] and CCONJ (O) [decoder NOUN (B) learning NOUN (I)] rates NOUN (O) were AUX (O) not PART (O) successful ADJ . (O) 

Table NOUN (O) : Mean VERB (O) (with ADP (O) std NOUN . (O) dev PROPN .) (O) ratings NOUN (O) of ADP (O) fluency NOUN , (O) naturalness NOUN , (O) [voice NOUN (B) stability NOUN (I)] (top NOUN) (O) and CCONJ (O) pronunciation NOUN (O) accuracy NOUN (O) (middle PROPN) . (O) 
 The DET (O) bottom NOUN (O) row NOUN (O) shows VERB (O) the DET (O) number NOUN (O) of ADP (O) sentences NOUN (O) with ADP (O) word NOUN (O) skips VERB . (O) 


Figure NOUN (O) : Language NOUN (O) abilities NOUN (O) of ADP (O) participants NOUN (O) of ADP (O) our DET (O) survey NOUN . (O) 

Code NOUN - switching VERB (O) 
 Training NOUN (O) setup NOUN (O) : In ADP (O) this DET (O) experiment NOUN , (O) we PRON (O) only ADV (O) used VERB (O) the DET (O) five NUM (O) languages NOUN (O) where ADV (O) both DET (O) [CSS10 PROPN (B)] and CCONJ (O) [CV NOUN (B) data NOUN (I)] are AUX (O) available ADJ (O) (Table NOUN) , (O) and CCONJ (O) trained VERB (O) on ADP (O) all DET (O) data NOUN (O) in ADP (O) our DET (O) cleaned VERB (O) sets VERB (O) ; 64 NUM (O) and CCONJ (O) 4 NUM (O) randomly ADV (O) selected VERB (O) samples NOUN (O) for ADP (O) each DET (O) speaker NOUN (O) from ADP (O) [CSS10 PROPN (B)] and CCONJ (O) CV NOUN , (O) respectively ADV , (O) were AUX (O) reserved VERB (O) for ADP (O) validation NOUN . (O) 
 The DET (O) S NOUN (O) GL PROPN (O) models NOUN (O) are AUX (O) not PART (O) applicable ADJ (O) to ADP (O) the DET (O) code NOUN - switching VERB (O) scenario NOUN . (O) 
 SHA PROPN , (O) SEP PROPN , (O) and CCONJ (O) GEN PROPN (O) models NOUN (O) were AUX (O) trained VERB (O) for ADP (O) 50k NUM (O) steps NOUN (O) with ADP (O) the DET (O) same ADJ (O) learning NOUN (O) rate NOUN (O) and CCONJ (O) schedule NOUN (O) settings NOUN (O) as SCONJ (O) in ADP (O) Section NOUN , (O) this DET (O) time NOUN (O) with ADP (O) the DET (O) [adversarial ADJ (B) speaker NOUN (I) classifiers NOUN (I)] enabled.5 PROPN (O) We PRON (O) set NOUN (O) the DET (O) size NOUN (O) of ADP (O) [speaker NOUN (B) embeddings NOUN (I)] to PART (O) 32 NUM (O) and CCONJ (O) used VERB (O) a NOUN (O) language NOUN (O) embedding NOUN (O) of ADP (O) size NOUN (O) 4 NUM (O) in ADP (O) S NOUN (O) HA PROPN . (O) 
 GEN PROPN (O) uses VERB (O) language NOUN (O) embeddings NOUN (O) of ADP (O) size NOUN (O) 10 NUM (O) and CCONJ (O) generator NOUN (O) layers NOUN (O) of ADP (O) size NOUN (O) 4 NUM . (O) We PRON (O) used VERB (O) mini NOUN - batches NOUN (O) of ADP (O) size NOUN (O) 50 NUM (O) for ADP (O) all DET (O) models NOUN . (O)             
                                                                           
Code PROPN - switching VERB (O) [evaluation NOUN (B) dataset NOUN (I)] : We PRON (O) created VERB (O) a NOUN (O) new ADJ (O) small-[scale ADJ (B) dataset NOUN (I)] especially ADV (O) for ADP (O) code NOUN - switching VERB (O) evaluation NOUN . (O) 
 We PRON (O) used VERB (O) bilingual ADJ (O) sentences NOUN (O) scraped VERB (O) from ADP (O) Wikipedia PROPN . (O) For ADP (O) each DET (O) language NOUN , (O) we PRON (O) picked VERB (O) 80 NUM (O) sentences NOUN (O) with ADP (O) a NOUN (O) few ADJ (O) foreign ADJ (O) words NOUN (O) (20 NUM (O) sentences NOUN (O) for ADP (O) each DET (O) of ADP (O) the DET (O) 4 NUM (O) other ADJ (O) languages NOUN) (O) ; Chinese PROPN (O) was AUX (O) romanized VERB . (O) 
 We PRON (O) replaced VERB (O) foreign ADJ (O) names NOUN (O) with ADP (O) their DET (O) native NOUN (O) forms NOUN (O) (see VERB (O) Fig PROPN .) . (O) 

Figure NOUN (O) : Examples NOUN (O) of ADP (O) code NOUN - switching VERB (O) evaluation NOUN (O) sentences NOUN . (O) 

Subjective PROPN (O) evaluation NOUN (O) : We PRON (O) synthesized VERB (O) all DET (O) evaluation NOUN (O) sentences NOUN (O) using VERB (O) [speaker NOUN (B) embedding NOUN (I)] of ADP (O) the DET (O) [CSS10 PROPN (B) speaker NOUN (I)] for ADP (O) the DET (O) base NOUN (O) language NOUN (O) of ADP (O) the DET (O) sentence NOUN . (O) We PRON (O) arranged VERB (O) a NOUN (O) subjective ADJ (O) evaluation NOUN (O) test NOUN (O) and CCONJ (O) used VERB (O) a NOUN (O) rating NOUN (O) method NOUN (O) that SCONJ (O) combines VERB (O) five NUM - point NOUN (O) [mean NOUN (B) opinion NOUN (I) score NOUN (I)] ([MOS PROPN (B)]) with ADP (O) [MUSHRA PROPN (B)] . For ADP (O) each DET (O) sample NOUN , (O) its DET (O) transcript NOUN (O) and CCONJ (O) systems NOUN ’ PUNCT (O) outputs VERB (O) were AUX (O) shown VERB (O) at ADP (O) the DET (O) same ADJ (O) time NOUN . (O) 
 Participants NOUN (O) were AUX (O) asked VERB (O) to ADP (O) rate NOUN (O) them PRON (O) on ADP (O) a NOUN (O) scale NOUN (O) from ADP (O) 1 NUM (O) to ADP (O) 5 NUM (O) with ADP (O) 0.1 NUM (O) increments NOUN (O) and CCONJ (O) with ADP (O) labels NOUN (O) “ PUNCT (O) Bad PROPN (O) ” PUNCT , (O) “ PUNCT (O) Poor PROPN (O) ” PUNCT , (O) “ PUNCT (O) Fair PROPN (O) ” PUNCT , (O) “ PUNCT (O) Good ADJ (O) ” PUNCT , (O) “ PUNCT (O) Excellent PROPN (O) ” PUNCT . (O) To ADP (O) distinguish NOUN (O) different ADJ (O) error NOUN (O) types NOUN , (O) we PRON (O) asked VERB (O) for ADP (O) two NUM (O) ratings NOUN (O) : fluency NOUN , (O) naturalness NOUN , (O) and CCONJ (O) stability NOUN (O) of ADP (O) the DET (O) voice NOUN (O) ([speaker NOUN (B) similarity NOUN (I)]) – PUNCT (O) to ADP (O) check NOUN (O) if SCONJ (O) foreign ADJ (O) words NOUN (O) cause NOUN (O) any DET (O) change NOUN (O) to ADP (O) the DET (O) speaker NOUN ’s PUNCT (O) voice NOUN , (O) and CCONJ (O) accuracy NOUN (O) – PUNCT (O) testing NOUN (O) if SCONJ (O) all DET (O) words NOUN (O) are AUX (O) pronounced VERB (O) and CCONJ (O) the DET (O) foreign ADJ (O) word NOUN (O) pronunciation NOUN (O) is AUX (O) correct ADJ . (O)                 
Participants PROPN (O) could VERB (O) leave VERB (O) a NOUN (O) textual NOUN (O) note NOUN (O) at ADP (O) the DET (O) end NOUN (O) of ADP (O) the DET (O) survey NOUN . (O) 
 For ADP (O) each DET (O) language NOUN , (O) we PRON (O) recruited VERB (O) ten NUM (O) [native NOUN (B) speakers NOUN (I)] that DET (O) spoke VERB (O) at ADP (O) least ADJ (O) one NUM (O) other ADJ (O) language NOUN (O) fluently ADV (O) via ADP (O) the DET (O) Prolific PROPN (O) platform NOUN (O) (Fig PROPN .) (O) They PRON (O) were AUX (O) given VERB (O) twelve NUM (O) sentences NOUN (O) with ADP (O) the DET (O) base NOUN (O) language NOUN (O) matching NOUN (O) their DET (O) native NOUN (O) language NOUN (O) where ADV (O) each DET (O) of ADP (O) the DET (O) other ADJ (O) languages NOUN (O) was AUX (O) represented VERB (O) by ADP (O) three NUM (O) sentences NOUN . (O) 
                                                                           
Results NOUN (O) : Table NOUN (O) summarizes VERB (O) results VERB (O) of ADP (O) the DET (O) survey NOUN . (O) The DET (O) rows VERB (O) marked VERB (O) “ PUNCT (O) All DET (O) ” PUNCT (O) show NOUN (O) means VERB (O) and CCONJ (O) variances NOUN (O) of ADP (O) the DET (O) ratings NOUN (O) of ADP (O) all DET (O) 50 NUM (O) participants NOUN . (O) 
 Fig PROPN . (O) visualizes VERB (O) quantiles NOUN (O) of ADP (O) the DET (O) ratings NOUN (O) (grouped VERB (O) by ADP (O) dominant ADJ (O) languages NOUN) . (O) 
 G NOUN (O) EN PROPN (O) has AUX (O) significantly ADV (O) higher ADJ (O) mean VERB (O) ratings NOUN (O) on ADP (O) both DET (O) scales VERB . (O) 
 Unlike PROPN (O) S NOUN (O) HA PROPN (O) or CCONJ (O) S NOUN (O) EP PROPN , (O) it PRON (O) allows VERB (O) [cross ADJ - lingual ADJ (B) mixing VERB (I)] of ADP (O) the DET (O) [encoder NOUN (B) outputs NOUN (I)] and CCONJ (O) enables VERB (O) smooth VERB (O) control NOUN (O) over ADP (O) pronunciation NOUN . (O) 
 S NOUN (O) EP PROPN (O) scores NOUN (O) consistently ADV (O) worst ADJ . (O) 
 The DET (O) accuracy NOUN (O) ratings NOUN (O) are AUX (O) overall NOUN (O) slightly ADV (O) higher ADJ (O) than SCONJ (O) the DET (O) fluency NOUN (O) ratings NOUN (O) ; this DET (O) might VERB (O) be AUX (O) caused VERB (O) by ADP (O) improper ADJ (O) word NOUN (O) stress NOUN , (O) which DET (O) several ADJ (O) participants NOUN (O) commented VERB (O) on ADV . (O) 

Based VERB (O) on ADP (O) preliminary ADJ (O) experiments VERB (O) on ADP (O) [validation NOUN (B) data NOUN (I)] , we PRON (O) set NOUN (O) λ=1 VERB (O) and CCONJ (O) weighted ADJ (O) the DET (O) loss NOUN (O) of ADP (O) the DET (O) classifier NOUN (O) by ADP (O) 0.125 NUM (O) and CCONJ (O) 0.5 NUM (O) for ADP (O) G NOUN (O) EN PROPN (O) and CCONJ (O) S NOUN (O) HA PROPN , (O) respectively ADV . (O) The DET (O) classifiers NOUN (O) include VERB (O) a NOUN (O) hidden VERB (O) layer NOUN (O) of ADP (O) size NOUN (O) 256 NUM . (O)                       
https://www.prolific.co PROPN (O) ; 4 NUM (O) participants NOUN (O) who PRON (O) reported VERB (O) as SCONJ (O) Chinese PROPN (O) [native NOUN (B) speakers NOUN (I)] on ADP (O) Prolific PROPN (O) only ADV (O) reported VERB (O) non ADJ - native ADJ (O) fluency NOUN (O) in ADP (O) our DET (O) survey NOUN . (O)   
 In ADP (O) 3 NUM (O) sentences NOUN , (O) a NOUN (O) random ADJ (O) model NOUN (O) output NOUN (O) was AUX (O) distorted VERB (O) and CCONJ (O) used VERB (O) as SCONJ (O) sanity NOUN (O) check NOUN (O) (expected VERB (O) to ADP (O) be AUX (O) rated VERB (O) lowest ADJ) . (O) All DET (O) participants NOUN (O) passed VERB . (O) 

Figure NOUN (O) : Graphs PROPN (O) showing VERB (O) distributions NOUN (O) of ADP (O) fluency NOUN (O) and CCONJ (O) accuracy NOUN (O) ratings NOUN (O) grouped VERB (O) by ADP (O) the DET (O) dominant ADJ (O) language NOUN (O) of ADP (O) rated VERB (O) sentences NOUN . (O) 

Manual PROPN (O) error NOUN (O) analysis NOUN (O) : We PRON (O) found VERB (O) that SCONJ (O) the DET (O) models NOUN (O) sometimes ADV (O) skip NOUN (O) words NOUN , (O) especially ADV (O) when ADV (O) reaching VERB (O) foreign ADJ (O) words NOUN (O) in ADP (O) Chinese PROPN (O) sentences NOUN . (O) 
 Therefore ADV , (O) we PRON (O) manually ADV (O) inspected VERB (O) all DET (O) 400 NUM (O) outputs NOUN (O) of ADP (O) all DET (O) models NOUN (O) and CCONJ (O) counted VERB (O) sentences NOUN (O) where ADV (O) any DET (O) word NOUN (O) skip NOUN (O) occurred VERB , (O) see VERB (O) the DET (O) “ PUNCT (O) Word NOUN (O) skips VERB (O) ” PUNCT (O) row NOUN (O) in ADP (O) Table NOUN . (O) 
 We PRON (O) found VERB (O) that SCONJ (O) the DET (O) G NOUN (O) EN PROPN (O) model NOUN (O) makes VERB (O) much ADJ (O) fewer ADJ (O) of ADP (O) these DET (O) errors NOUN (O) than SCONJ (O) S NOUN (O) HA PROPN (O) and CCONJ (O) S NOUN (O) EP PROPN . (O) 

Conclusion NOUN (O) 
 We PRON (O) presented VERB (O) a NOUN (O) new ADJ (O) [grapheme NOUN - based VERB (B) model NOUN (I)] that DET (O) uses VERB (O) metalearning NOUN (O) for ADP (O) [multilingual ADJ (B) TTS PROPN (I)] 
 We PRON (O) showed VERB (O) that SCONJ (O) it PRON (O) significantly ADV (O) outperforms NOUN (O) multiple NOUN (O) strong ADJ (O) baselines NOUN (O) on ADP (O) two NUM (O) tasks NOUN (O) : data NOUN - stress NOUN (O) training NOUN (O) and CCONJ (O) [code NOUN - switching VERB (B)] , where ADV (O) our DET (O) model NOUN (O) was AUX (O) favored VERB (O) in ADP (O) oth NOUN (O) [voice NOUN (B) fluency NOUN (I)] as SCONJ (O) well INTJ (O) as SCONJ (O) pronunciation NOUN (O) accuracy NOUN . (O) 
 Our DET (O) code NOUN (O) is AUX (O) available ADJ (O) on ADP (O) GitHub.1 NUM (O) For ADP (O) future NOUN (O) work NOUN , (O) we PRON (O) consider VERB (O) changes VERB (O) to ADP (O) our DET (O) model NOUN ’s PUNCT (O) attention NOUN (O) module NOUN (O) to ADP (O) further NOUN (O) improve VERB (O) accuracy NOUN . (O) 

Acknowledgements NOUN (O) 
 This DET (O) research NOUN (O) was AUX (O) supported VERB (O) by ADP (O) the DET (O) Charles PROPN (O) University PROPN (O) grant NOUN (O) PRIMUS/19 NOUN (O) / SCI/10 INTJ . (O) 
