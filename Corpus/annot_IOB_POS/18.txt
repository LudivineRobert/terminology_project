[MELLOTRON PROPN (B)] : [MULTISPEAKER PROPN (B) EXPRESSIVE PROPN (I) VOICE PROPN (I) SYNTHESIS PROPN (I)] BY PROPN (O) CONDITIONING NOUN (O) ON PROPN (O) RHYTHM PROPN , (O) [PITCH PROPN (B)] AND CCONJ (O) GLOBAL PROPN (O) STYLE PROPN (O) TOKENS PROPN (O) 

ABSTRACT PROPN (O) 

[Mellotron PROPN (B)] is AUX (O) a NOUN (O) [multispeaker NOUN (B) voice NOUN (I) synthesis NOUN (I) model NOUN (I)] based VERB (O) on ADP (O) [Tacotron PROPN (B) 2 NUM (I) GST PROPN (I)] that DET (O) can VERB (O) make VERB (O) a NOUN (O) [voice NOUN (B) emote NOUN (I)] and CCONJ (O) sing VERB (O) without ADP (O) emotive NOUN (O) or CCONJ (O) singing NOUN (O) [training NOUN (B) data NOUN (I)] . 
 By ADP (O) explicitly ADV (O) conditioning NOUN (O) on ADP (O) rhythm NOUN (O) and CCONJ (O) [continuous ADJ (B) pitch NOUN (I) contours NOUN (I)] from ADP (O) an DET (O) [audio NOUN (B)] 
 signal NOUN (O) or CCONJ (O) music NOUN (O) score NOUN , (O) [Mellotron PROPN (B)] is AUX (O) able ADJ (O) to ADP (O) generate NOUN (O) [speech NOUN (B)] in ADP (O) a NOUN (O) variety NOUN (O) of ADP (O) styles NOUN (O) ranging VERB (O) from ADP (O) read VERB (O) [speech NOUN (B)] to PART (O) [expressive NOUN (B) speech NOUN (I)] , from ADP (O) slow ADJ (O) drawls NOUN (O) to ADP (O) rap NOUN (O) and CCONJ (O) from ADP (O) [monotonous ADJ (B) voice NOUN (I)] to PART (O) singing NOUN (O) voice NOUN . (O) 
 Unlike PROPN (O) other ADJ (O) methods NOUN , (O) we PRON (O) train NOUN (O) [Mellotron PROPN (B)] using VERB (O) only ADV (O) read VERB (O) [speech NOUN (B) data NOUN (I)] without ADP (O) alignments VERB (O) between ADP (O) text NOUN (O) and CCONJ (O) [audio NOUN (B)] . 
 We PRON (O) evaluate VERB (O) our DET (O) models NOUN (O) using VERB (O) the DET (O) [LJSpeech NOUN (B)] and CCONJ (O) [LibriTTS NOUN (B) datasets VERB (I)] . 
 We PRON (O) provide VERB (O) F0 NOUN (O) Frame PROPN (O) Errors NOUN (O) and CCONJ (O) synthesized VERB (O) samples NOUN (O) that SCONJ (O) include VERB (O) style NOUN (O) transfer NOUN (O) from ADP (O) [other ADJ (B) speakers NOUN (I)] , singers NOUN (O) and CCONJ (O) styles NOUN (O) not PART (O) seen VERB (O) during ADP (O) training NOUN , (O) procedural ADJ (O) manipulation NOUN (O) of ADP (O) rhythm NOUN (O) and CCONJ (O) [pitch NOUN (B)] and CCONJ (O) choir NOUN (O) synthesis NOUN . (O) 

Index NOUN (O) Terms NOUN (O) — PUNCT (O) [Text NOUN - to ADP - Speech NOUN (B) Synthesis PROPN (I)] , Singing NOUN (O) [Voice PROPN (B) Synthesis PROPN (I)] , Style NOUN (O) Transfer NOUN , (O) [Deep ADJ (B) learning NOUN (I)] 

INTRODUCTION VERB (O) 

[Speech NOUN (B) synthesis NOUN (I)] is AUX (O) typically ADV (O) formulated VERB (O) as SCONJ (O) the DET (O) conversion NOUN (O) of ADP (O) [text NOUN (B) to ADP (I) speech NOUN (I)] ([TTS PROPN (B)]) . 
 This DET (O) formulation NOUN , (O) however ADV , (O) leaves VERB (O) out NOUN (O) control NOUN (O) for ADP (O) all DET (O) the DET (O) aspects NOUN (O) of ADP (O) [speech NOUN (B)] not PART (O) contained VERB (O) in ADP (O) the DET (O) text NOUN . (O) 
 Here ADV (O) we PRON (O) approach NOUN (O) the DET (O) problem NOUN (O) of ADP (O) [expressive NOUN (B) speech NOUN (I) synthesis NOUN (I)] which DET (O) includes VERB (O) not PART (O) just ADV (O) text NOUN , (O) but CCONJ (O) other ADJ (O) characteristics NOUN (O) such ADJ (O) as SCONJ (O) [pitch NOUN (B)] , rhythm NOUN (O) and CCONJ (O) emphasis NOUN . (O) There ADV (O) are AUX (O) formulations NOUN (O) to ADP (O) [expressive NOUN (B) speech NOUN (I) synthesis NOUN (I)] that DET (O) require VERB (O) animated VERB (O) and CCONJ (O) [emotive NOUN (B) voice NOUN (I) data NOUN (I)] . 
 This DET (O) is AUX (O) an DET (O) inconvenient NOUN (O) drawback NOUN (O) given VERB (O) the DET (O) limited ADJ (O) access NOUN (O) to ADP (O) [such ADJ (B) data NOUN (I)] . 
 In ADP (O) our DET (O) approach NOUN , (O) we PRON (O) can VERB (O) make VERB (O) a NOUN (O) [voice NOUN (B) emote NOUN (I)] and CCONJ (O) sing VERB (O) without ADP (O) any DET (O) [such ADJ (B) data NOUN (I)] . 
 Recent ADJ (O) approaches VERB (O) that SCONJ (O) utilize VERB (O) [deep ADJ (B) learning NOUN (I)] for ADP (O) [expressive NOUN (B) speech NOUN (I) synthesis NOUN (I)] combine VERB (O) text NOUN (O) and CCONJ (O) a NOUN (O) learned VERB (O) latent NOUN (O) embedding NOUN (O) for ADP (O) [prosody NOUN (B)] or CCONJ (O) global ADJ (O) style NOUN . (O) 
 While SCONJ (O) these DET (O) approaches VERB (O) have AUX (O) shown VERB (O) promise NOUN , (O) manipulating NOUN (O) such ADJ (O) latent NOUN (O) variables X (O) only ADV (O) offers VERB (O) a NOUN (O) coarse PROPN (O) control NOUN (O) over ADP (O) expressive NOUN (O) characteristics NOUN (O) of ADP (O) [speech NOUN (B)] . 
 [Mellotron PROPN (B)] was AUX (O) motivated X (O) by ADP (O) the DET (O) desire NOUN (O) for ADP (O) fine NOUN (O) grained VERB (O) control NOUN (O) over ADP (O) these DET (O) expressive NOUN (O) characteristics NOUN . (O) 
 Notably ADV , (O) we PRON (O) show NOUN (O) that SCONJ (O) it PRON (O) is AUX (O) easy ADJ (O) to ADP (O) condition NOUN (O) [Mellotron PROPN (B)] on ADP (O) [pitch NOUN (B)] and CCONJ (O) rhythm NOUN (O) information NOUN (O) automatically ADV (O) extracted VERB (O) from ADP (O) an DET (O) [audio NOUN (B) signal NOUN (I)] or CCONJ (O) music NOUN (O) score NOUN . (O) 
 By ADP (O) accounting NOUN (O) for ADP (O) melodic PROPN (O) information NOUN (O) such ADJ (O) as SCONJ (O) [pitch NOUN (B)] and CCONJ (O) rhythm NOUN , (O) [expressive NOUN (B) speech NOUN (I) synthesis NOUN (I)] with ADP (O) [Mellotron PROPN (B)] can VERB (O) be AUX (O) easily ADV (O) extended ADJ (O) to ADP (O) singing NOUN (O) [voice NOUN (B) synthesis NOUN (I)] (SVS PROPN) . (O) 
 Unfortunately ADV , (O) recent ADJ (O) attempts VERB (O) require VERB (O) a NOUN (O) [singing NOUN (B) voice NOUN (I) dataset PROPN (I)] and CCONJ (O) heavily ADV (O) quantized VERB (O) [pitch NOUN (B)] and CCONJ (O) [rhythm NOUN (B) data NOUN (I)] obtained VERB (O) from ADP (O) a NOUN (O) digital PROPN (O) representation NOUN (O) of ADP (O) a NOUN (O) music NOUN (O) score NOUN , (O) for ADP (O) example NOUN (O) MIDI PROPN (O) or CCONJ (O) musicXML X . (O) 
 [Mellotron PROPN (B)] does AUX (O) not PART (O) require VERB (O) any DET (O) [singing NOUN (B) voice NOUN (I)] in ADP (O) the DET (O) dataset NOUN (O) nor CCONJ (O) manually ADV (O) aligned VERB (O) [pitch NOUN (B)] and CCONJ (O) text NOUN (O) in ADP (O) order NOUN (O) to ADP (O) synthesize VERB (O) [singing NOUN (B) voice NOUN (I)] . 
 [Mellotron PROPN (B)] can VERB (O) make VERB (O) a NOUN (O) [voice NOUN (B) emote NOUN (I)] and CCONJ (O) sing VERB (O) without ADP (O) emotion NOUN (O) or CCONJ (O) [singing NOUN (B) data NOUN (I)] . 
 [Training NOUN (B) Mellotron PROPN (I)] is AUX (O) very ADV (O) simple ADJ (O) and CCONJ (O) only ADV (O) requires VERB (O) read VERB (O) [speech NOUN (B)] and CCONJ (O) transcriptions NOUN . (O) 
 During ADP (O) inference NOUN , (O) we PRON (O) can VERB (O) change NOUN (O) the DET (O) generated VERB (O) voice NOUN ’s PUNCT (O) speaking VERB (O) style NOUN , (O) make VERB (O) it PRON (O) emote NOUN (O) or CCONJ (O) sing VERB (O) by ADP (O) extracting NOUN (O) [pitch NOUN (B)] and CCONJ (O) rhythm NOUN (O) characteristics NOUN (O) from ADP (O) an DET (O) [audio NOUN (B) file NOUN (I)] or CCONJ (O) a NOUN (O) music NOUN (O) score NOUN . (O) 
 As SCONJ (O) a NOUN (O) bonus NOUN , (O) with ADP (O) [Mellotron PROPN (B)] we PRON (O) can VERB (O) explore VERB (O) latent NOUN (O) characteristics NOUN (O) from ADP (O) an DET (O) [audio NOUN (B) corpus NOUN (I)] by ADP (O) sampling NOUN (O) a NOUN (O) dictionary NOUN (O) of ADP (O) learned VERB (O) latent NOUN (O) characteristics NOUN . (O) 
 In ADP (O) summary NOUN , (O) [Mellotron PROPN (B)] is AUX (O) a NOUN (O) [versatile NOUN (B) voice NOUN (I) synthesis NOUN (I) model NOUN (I)] that DET (O) enables VERB (O) the DET (O) combination NOUN (O) of ADP (O) characteristics NOUN (O) from ADP (O) different ADJ (O) sources NOUN (O) and CCONJ (O) generalizes VERB (O) to ADP (O) characteristics NOUN (O) not PART (O) seen VERB (O) in ADP (O) [training NOUN (B) data NOUN (I)] . 

Includes VERB (O) [speech NOUN (B) synthesis NOUN (I)] , singing VERB (O) [voice NOUN (B) synthesis NOUN (I)] , etc X . (O) 

METHOD PROPN (O) 

[Mellotron PROPN (B)] is AUX (O) a NOUN (O) [voice NOUN (B) synthesis NOUN (I) model NOUN (I)] that DET (O) uses VERB (O) a NOUN (O) combination NOUN (O) of ADP (O) explicit NOUN (O) and CCONJ (O) latent NOUN (O) variables NOUN . (O) 
 Whereas PROPN (O) well ADV - established VERB (O) signal NOUN (O) processing NOUN (O) algorithms PROPN (O) provide VERB (O) explicit NOUN (O) variables X (O) that SCONJ (O) are AUX (O) valuable ADJ (O) to ADP (O) [expressive NOUN (B) speech NOUN (I)] such ADJ (O) as SCONJ (O) [fundamental ADJ (B) frequency NOUN (I) contours NOUN (I)] and CCONJ (O) [voicing VERB (B) decisions NOUN (I)] , [deep ADJ (B) learning NOUN (I)] strategies VERB (O) can VERB (O) be AUX (O) used VERB (O) to ADP (O) learn VERB (O) latent NOUN (O) variables X (O) that SCONJ (O) express VERB (O) characteristics NOUN (O) of ADP (O) an DET (O) [audio NOUN (B) corpus NOUN (I)] that DET (O) are AUX (O) unknown ADJ (O) to ADP (O) the DET (O) user NOUN (O) and CCONJ (O) hard ADJ (O) to ADP (O) formalize VERB . (O) 
 We PRON (O) factorize VERB (O) a NOUN (O) [single ADJ (B) speaker NOUN (I) mel PROPN - spectrogram PROPN (I) M PROPN (I)] into ADP (O) explicit NOUN (O) variables X (O) such ADJ (O) as SCONJ (O) text NOUN , (O) [speaker NOUN (B) identity NOUN (I)] , a DET (O) [fundamental ADJ (B) frequency NOUN (I) contour NOUN (I)] augmented VERB (O) with ADP (O) voiced VERB (O) / unvoiced ADJ (O) decisions NOUN (O) and CCONJ (O) two NUM (O) latent NOUN (O) variables X (O) learned VERB (O) by ADP (O) the DET (O) model NOUN (O) during ADP (O) training NOUN . (O) 
 The DET (O) first ADJ (O) latent NOUN (O) variable NOUN (O) refers VERB (O) to ADP (O) a NOUN (O) dictionary NOUN (O) of ADP (O) [vectors NOUN (B)] that DET (O) can VERB (O) be AUX (O) queried VERB (O) with ADP (O) an DET (O) [audio NOUN (B) input NOUN (I)] or CCONJ (O) sampled VERB (O) directly ADV (O) as SCONJ (O) described VERB (O) in ADP . (O) The DET (O) second NOUN (O) latent NOUN (O) variable NOUN (O) is AUX (O) the DET (O) learned VERB (O) attention NOUN (O) map NOUN (O) between ADP (O) the DET (O) text NOUN (O) and CCONJ (O) the DET (O) [mel PROPN - spectrogram PROPN (B)] as SCONJ (O) described VERB (O) in ADP . (O) 
 From ADP (O) now ADV (O) on ADP (O) we PRON (O) will VERB (O) refer NOUN (O) to ADP (O) the DET (O) augmented VERB (O) [fundamental ADJ (B) frequency NOUN (I) contour NOUN (I)] as SCONJ (O) [pitch NOUN (B) contour NOUN (I)] and CCONJ (O) refer NOUN (O) to ADP (O) the DET (O) first ADJ (O) and CCONJ (O) second NOUN (O) latent NOUN (O) variables X (O) as SCONJ (O) global ADJ (O) style NOUN (O) tokens NOUN (O) ([GST PROPN (B)]) and CCONJ (O) rhythm NOUN (O) respectively ADV . (O) 
 We PRON (O) are AUX (O) interested ADJ (O) in ADP (O) factorizing NOUN (O) M NOUN (O) = (T NOUN , (O) S PROPN , (O) P NOUN , (O) R NOUN , (O) Z NOUN) , (O) where ADV (O) T NOUN (O) represents VERB (O) the DET (O) text NOUN , (O) S NOUN (O) represents VERB (O) the DET (O) [speaker NOUN (B) identity NOUN (I)] , P NOUN (O) represents VERB (O) the DET (O) [pitch NOUN (B) contour NOUN (I)] , R NOUN (O) represents VERB (O) the DET (O) rhythm NOUN (O) and CCONJ (O) Z NOUN (O) represents VERB (O) the DET (O) global ADJ (O) style NOUN (O) tokens NOUN . (O) 
 Given VERB (O) this DET (O) formulation NOUN , (O) during ADP (O) training NOUN (O) we PRON (O) maximize NOUN (O) the DET (O) following VERB (O) : 
 where ADV (O) the DET (O) superscript NOUN (O) i PRON (O) represents VERB (O) the DET (O) i PROPN - th PROPN (O) [mel X (B)] , T(i PROPN) , (O) S(i PROPN) (O) and CCONJ (O) P(i PROPN) (O) represent VERB (O) the DET (O) text NOUN , (O) speaker NOUN , (O) and CCONJ (O) [pitch NOUN (B) contour NOUN (I)] associated VERB (O) with ADP (O) the DET (O) i PROPN - th PROPN (O) [mel X (B)] , Ri PROPN (O) represents VERB (O) the DET (O) learned VERB (O) alignments VERB (O) between ADP (O) the DET (O) text NOUN (O) and CCONJ (O) [mel PROPN - spectrogram PROPN (B) frames NOUN (I)] , Zmel(i PROPN) (O) represents VERB (O) the DET (O) global ADJ (O) style NOUN (O) token NOUN (O) conditioned VERB (O) on ADP (O) [mel X (B)] (i NOUN) (O) as SCONJ (O) presented VERB (O) in ADP , (O) and CCONJ (O) θ NOUN (O) represents VERB (O) the DET (O) model NOUN (O) parameters NOUN . (O) 
 The DET (O) explicit NOUN (O) factors NOUN (O) offers VERB (O) two NUM (O) advantages NOUN . (O) First ADV , (O) by ADP (O) providing VERB (O) the DET (O) model NOUN (O) with ADP (O) text NOUN (O) and CCONJ (O) [speaker NOUN (B) information NOUN (I)] , we PRON (O) prevent NOUN (O) the DET (O) problem NOUN (O) of ADP (O) entanglement NOUN (O) between ADP (O) text NOUN (O) and CCONJ (O) [speaker NOUN (B) information NOUN (I)] . Second ADJ (O) by ADP (O) providing VERB (O) the DET (O) model NOUN (O) with ADP (O) [pitch NOUN (B) contour NOUN (I)] and CCONJ (O) [voicing VERB (B) information NOUN (I)] , we PRON (O) are AUX (O) able ADJ (O) to ADP (O) directly ADV (O) control NOUN (O) [pitch NOUN (B)] and CCONJ (O) [voicing VERB (B) decisions NOUN (I)] during ADP (O) inference NOUN . (O) 
 Similarly ADV (O) the DET (O) latent NOUN (O) factors NOUN (O) offers VERB (O) two NUM (O) advantages NOUN . (O) 
 First ADV , (O) by ADP (O) learning NOUN (O) the DET (O) alignment NOUN (O) map NOUN (O) between ADP (O) the DET (O) text NOUN (O) and CCONJ (O) [mel PROPN - spectrogram PROPN (B)] during ADP (O) training NOUN , (O) we PRON (O) do AUX (O) not PART (O) need NOUN (O) to ADP (O) extract NOUN (O) [phoneme NOUN (B) alignments VERB (I)] for ADP (O) training NOUN (O) and CCONJ (O) can VERB (O) control NOUN (O) the DET (O) rhythm NOUN (O) during ADP (O) inference NOUN (O) by ADP (O) providing VERB (O) the DET (O) model NOUN (O) with ADP (O) an DET (O) alignment NOUN (O) map NOUN . (O) 
 Second PROPN , (O) by ADP (O) providing VERB (O) the DET (O) model NOUN (O) with ADP (O) a NOUN (O) dictionary NOUN (O) of ADP (O) latent NOUN (O) variables NOUN , (O) we PRON (O) are AUX (O) able ADJ (O) to ADP (O) learn VERB (O) latent NOUN (O) factors NOUN (O) that SCONJ (O) are AUX (O) harder ADJ (O) to ADP (O) express VERB (O) or CCONJ (O) extract NOUN (O) explicitly ADV , (O) thus ADV (O) leveraging NOUN (O) the DET (O) full ADJ (O) power NOUN (O) of ADP (O) latent NOUN (O) variables NOUN . (O) 
 Using VERB (O) this DET (O) formulation NOUN (O) we PRON (O) are AUX (O) able ADJ (O) to ADP (O) transfer NOUN (O) the DET (O) text NOUN , (O) rhythm NOUN (O) and CCONJ (O) [pitch NOUN (B) contour NOUN (I)] from ADP (O) a NOUN (O) source NOUN , (O) e. PROPN (O) g. PROPN (O) [audio NOUN (B) signal NOUN (I)] or CCONJ (O) musical ADJ (O) score NOUN , (O) to ADP (O) a NOUN (O) [target NOUN (B) speaker NOUN (I)] by ADP (O) replacing VERB (O) the DET (O) variables X (O) in ADP (O) Equation NOUN (O) (1 NUM) (O) accordingly ADV . (O) 
 For ADP (O) example NOUN , (O) we PRON (O) first ADJ (O) collect VERB (O) the DET (O) text NOUN , (O) [pitch NOUN (B)] and CCONJ (O) rhythm NOUN (O) (Ts INTJ , (O) Ps PROPN , (O) Rs NOUN) , (O) from ADP (O) the DET (O) source NOUN , (O) sample NOUN (O) a NOUN (O) [GST PROPN (B)] Zquery PROPN (O) from ADP (O) the DET (O) [GST PROPN (B) dictionary PROPN (I)] learned VERB (O) by ADP (O) [Mellotron PROPN (B)] , and CCONJ (O) chose VERB (O) a NOUN (O) [target NOUN (B) speaker NOUN (I)] . 
 mel(out NUM) (O) should VERB (O) now ADV (O) have AUX (O) the DET (O) same ADJ (O) text NOUN , (O) [pitch NOUN (B)] and CCONJ (O) rhythm NOUN (O) as SCONJ (O) the DET (O) source NOUN , (O) latent NOUN (O) characteristics NOUN (O) obtained VERB (O) from ADP (O) the DET (O) global ADJ (O) style NOUN (O) token NOUN (O) and CCONJ (O) the DET (O) voice NOUN (O) of ADP (O) the DET (O) [target NOUN (B) speaker NOUN (I)] . 
 In ADP (O) our DET (O) current ADJ (O) formulation NOUN , (O) the DET (O) [target NOUN (B) speaker NOUN (I)] , S PROPN (O) t NOUN , (O) would VERB (O) always ADV (O) be AUX (O) found VERB (O) in ADP (O) the DET (O) training NOUN (O) set VERB , (O) while SCONJ (O) the DET (O) source NOUN (O) text NOUN , (O) [pitch NOUN (B)] and CCONJ (O) rhythm NOUN (O) (Ts INTJ , (O) Ps PROPN , (O) Rs NOUN) (O) could VERB (O) be AUX (O) from ADP (O) outside ADV (O) the DET (O) training NOUN (O) set VERB . (O) 
 This DET (O) allows VERB (O) us PROPN (O) to ADP (O) train NOUN (O) a NOUN (O) model NOUN (O) that SCONJ (O) makes VERB (O) a NOUN (O) [voice NOUN (B) emote NOUN (I)] and CCONJ (O) sing VERB (O) without ADP (O) using VERB (O) any DET (O) [singing NOUN (B) voice NOUN (I)] in ADP (O) the DET (O) [training NOUN (B) dataset NOUN (I)] , without ADP (O) any DET (O) manual NOUN (O) labelling NOUN (O) of ADP (O) emotions NOUN (O) nor CCONJ (O) [pitch NOUN (B)] , and CCONJ (O) without ADP (O) any DET (O) manual NOUN (O) alignments VERB (O) between ADP (O) words NOUN (O) and CCONJ (O) [audio NOUN (B)] , nor CCONJ (O) between ADP (O) [pitch NOUN (B)] and CCONJ (O) [audio NOUN (B)] . 

IMPLEMENTATION NOUN (O) 

In ADP (O) this DET (O) section NOUN (O) we PRON (O) are AUX (O) going VERB (O) to ADP (O) describe VERB (O) our DET (O) [model NOUN (B) architecture NOUN (I)] and CCONJ (O) our DET (O) training NOUN (O) and CCONJ (O) inference NOUN (O) setups NOUN . (O) 
 We PRON (O) plan NOUN (O) to ADP (O) release NOUN (O) our DET (O) implementation NOUN (O) and CCONJ (O) pre VERB - trained ADJ (O) models NOUN (O) on ADP (O) github PROPN . (O) 

Architecture NOUN (O) 

[Mellotron PROPN (B)] extends VERB (O) [Tacotron PROPN (B) 2 NUM (I) GST PROPN (I)] with ADP (O) [speaker NOUN (B) embeddings NOUN (I)] and CCONJ (O) [pitch NOUN (B)] countours VERB . (O) 
 Unlike PROPN , (O) where ADV (O) site NOUN (O) [specific ADJ (B) speaker NOUN (I) embeddings NOUN (I)] are AUX (O) used VERB , (O) we PRON (O) use NOUN (O) a NOUN (O) [single ADJ (B) speaker NOUN (I)] embedding VERB (O) that SCONJ (O) is AUX (O) channel NOUN - wise ADJ (O) concatenated VERB (O) with ADP (O) the DET (O) [encoder NOUN (B) outputs NOUN (I)] over ADP (O) every DET (O) token VERB . (O) 
 The DET (O) [pitch NOUN (B) contour NOUN (I)] goes VERB (O) through ADP (O) a NOUN (O) single ADJ (O) convolution NOUN (O) layer NOUN (O) followed VERB (O) by ADP (O) a NOUN (O) [ReLU NOUN (B)] non ADJ - linearity ADJ . (O) 
 We PRON (O) experiment NOUN (O) with ADP (O) kernel NOUN (O) sizes VERB (O) 1 NUM (O) and CCONJ (O) 3 NUM (O) and CCONJ (O) convolution NOUN (O) dimensions NOUN (O) 1 NUM (O) and CCONJ (O) 8 NUM . (O) 
 The DET (O) [pitch NOUN (B) contour NOUN (I)] is AUX (O) channel NOUN - wise ADJ (O) concatenated VERB (O) with ADP (O) the DET (O) [decoder NOUN (B) inputs NOUN (I)] . 
 We PRON (O) use NOUN (O) [phoneme NOUN (B) representations NOUN (I)] whenever ADV (O) possible ADJ . (O) 

Training NOUN (O) 

Our DET (O) implementation NOUN (O) only ADV (O) requires VERB (O) text NOUN (O) and CCONJ (O) [audio NOUN (B) pairs NOUN (I)] with ADP (O) a NOUN (O) speaker NOUN (O) i PRON (O) d. PROPN (O) 
 Our DET (O) [pitch NOUN (B) contours NOUN (I)] are AUX (O) automatically ADV (O) extracted VERB (O) using VERB (O) the DET (O) [Yin PROPN (B) algorithm NOUN (I)] with ADP (O) harmonicity NOUN (O) thresholds PROPN (O) between ADP (O) 0.1 NUM (O) and CCONJ (O) 0.25 NUM . (O) 
 Unlike PROPN , (O) during ADP (O) training NOUN (O) our DET (O) model NOUN (O) does AUX (O) not PART (O) require VERB (O) manually ADV (O) aligned VERB (O) text NOUN , (O) [pitch NOUN (B)] and CCONJ (O) [mel PROPN - spectrogram PROPN (B)] . 
 We PRON (O) use NOUN (O) the DET (O) L2 NOUN (O) loss NOUN (O) between ADP (O) ground NOUN (O) truth NOUN (O) and CCONJ (O) predicted VERB (O) [mels NOUN (B)] described VERB (O) in ADP (O) without ADP (O) any DET (O) modifications NOUN . (O) 

Inference NOUN (O) 

Following VERB (O) the DET (O) description NOUN (O) in ADP (O) Section NOUN (O) 2 NUM , (O) during ADP (O) inference NOUN (O) we PRON (O) provide VERB (O) Melloron PROPN (O) with ADP (O) text NOUN , (O) rhythm NOUN (O) and CCONJ (O) [pitch NOUN (B) information NOUN (I)] that DET (O) is AUX (O) obtained VERB (O) either CCONJ (O) from ADP (O) an DET (O) [audio NOUN (B) signal NOUN (I)] or CCONJ (O) from ADP (O) a NOUN (O) musical ADJ (O) score NOUN , (O) a NOUN (O) global ADJ (O) style NOUN (O) token NOUN (O) and CCONJ (O) a NOUN (O) speaker NOUN (O) i PRON (O) d. PROPN (O) 

[Audio NOUN (B) Signal PROPN (I)] 

Obtaining VERB (O) text NOUN , (O) rhythm NOUN (O) and CCONJ (O) [pitch NOUN (B) information NOUN (I)] consists VERB (O) of ADP (O) three NUM (O) steps NOUN . (O) 
 First ADV , (O) we PRON (O) extract NOUN (O) text NOUN (O) information NOUN (O) from ADP (O) an DET (O) [audio NOUN (B) file NOUN (I)] by ADP (O) either CCONJ (O) using VERB (O) an DET (O) [automatic PROPN (B) speech NOUN (I) recognition NOUN (I) model NOUN (I)] or CCONJ (O) by ADP (O) manually ADV (O) transcribing VERB (O) the DET (O) text NOUN . (O) 
 The DET (O) text NOUN (O) information NOUN (O) is AUX (O) pre ADJ - processed ADJ (O) with ADP (O) our DET (O) text NOUN (O) cleaners NOUN (O) and CCONJ (O) then ADV (O) converted VERB (O) from ADP (O) [graphemes NOUN (B)] to PART (O) [phonemes NOUN (B)] . 
 Second ADV , (O) we PRON (O) extract NOUN (O) rhythm NOUN (O) information NOUN (O) by ADP (O) using VERB (O) a NOUN (O) forced VERB - alignment NOUN (O) tool NOUN (O) or CCONJ (O) by ADP (O) using VERB (O) [Mellotron PROPN (B)] as SCONJ (O) a NOUN (O) forced VERB - aligner PROPN . (O) 
 Alignment PROPN (O) maps NOUN (O) can VERB (O) be AUX (O) obtained VERB (O) with ADP (O) [Mellotron PROPN (B)] by ADP (O) performing VERB (O) a NOUN (O) teacher NOUN - forced VERB (O) forward NOUN (O) pass VERB (O) using VERB (O) the DET (O) data NOUN (O) from ADP (O) the DET (O) source NOUN (O) signal NOUN . (O) 
 Whenever ADV (O) necessary ADJ , (O) we PRON (O) fine NOUN (O) tune NOUN (O) the DET (O) alignment NOUN (O) maps NOUN (O) by ADP (O) hand NOUN (O) or CCONJ (O) by ADP (O) training NOUN (O) [Mellotron PROPN (B)] on ADP (O) the DET (O) source NOUN (O) signal NOUN (O) for ADP (O) a NOUN (O) few ADJ (O) iterations NOUN (O) with ADP (O) small ADJ (O) learning NOUN (O) rate NOUN . (O) 
 The DET (O) [pitch NOUN (B) data NOUN (I)] is AUX (O) obtained VERB (O) by ADP (O) using VERB (O) Yin PROPN (O) or CCONJ (O) Melodia PROPN . (O) 
 In ADP (O) our DET (O) quantitative NOUN (O) experiments VERB (O) we PRON (O) use NOUN (O) Yin PROPN (O) to ADP (O) replicate VERB (O) the DET (O) setup NOUN (O) described VERB (O) in ADP . (O) 
 In ADP (O) our DET (O) qualitative NOUN (O) experiments VERB (O) we PRON (O) use NOUN (O) Melodia PROPN (O) instead ADV (O) as SCONJ (O) we PRON (O) find VERB (O) it PRON (O) to ADP (O) be AUX (O) more ADJ (O) precise ADJ (O) than SCONJ (O) Yin PROPN , (O) specially ADV (O) with ADP (O) regards VERB (O) to ADP (O) false NOUN (O) voiced VERB (O) decisions NOUN . (O) 

Music NOUN (O) Score NOUN (O) 

We PRON (O) operate VERB (O) on ADP (O) music NOUN (O) scores NOUN (O) in ADP (O) XML NOUN (O) format NOUN (O) containing VERB (O) event NOUN (O) tuples NOUN (O) with ADP (O) [pitch NOUN (B)] , note VERB (O) duration NOUN (O) and CCONJ (O) syllables NOUN (O) for ADP (O) each DET (O) part NOUN (O) in ADP (O) the DET (O) score NOUN . (O) 
 We PRON (O) directly ADV (O) convert NOUN (O) [pitch NOUN (B)] to PART (O) frequency NOUN (O) and CCONJ (O) use NOUN (O) the DET (O) FFT PROPN (O) hop NOUN (O) size NOUN (O) to ADP (O) convert NOUN (O) event NOUN (O) durations NOUN (O) from ADP (O) seconds NOUN (O) to ADP (O) frames NOUN . (O) 
 We PRON (O) remind NOUN (O) the DET (O) reader NOUN (O) that SCONJ (O) although SCONJ (O) we PRON (O) refer NOUN (O) to ADP (O) [pitch NOUN (B)] , our DET (O) model NOUN ’s PUNCT (O) representation NOUN (O) of ADP (O) [pitch NOUN (B)] is AUX (O) continuous ADJ . (O) 
 We PRON (O) concatenate NOUN (O) the DET (O) syllables NOUN (O) into ADP (O) words NOUN (O) and CCONJ (O) convert NOUN (O) [graphemes NOUN (B)] to PART (O) [phonemes NOUN (B)] . 
 For ADP (O) single ADJ (O) phone NOUN (O) events NOUN , (O) the DET (O) duration NOUN (O) of ADP (O) each DET (O) phone NOUN (O) is AUX (O) equal ADJ (O) to ADP (O) the DET (O) duration NOUN (O) of ADP (O) the DET (O) event NOUN . (O) 
 For ADP (O) multi ADJ - phone ADJ (O) events NOUN , (O) the DET (O) duration NOUN (O) of ADP (O) each DET (O) phone NOUN (O) is AUX (O) dependent ADJ (O) on ADP (O) its DET (O) type NOUN (O) : we PRON (O) use NOUN (O) heuristics NOUN (O) to ADP (O) assign NOUN (O) durations NOUN (O) between ADP (O) 20 NUM (O) and CCONJ (O) 100ms PROPN (O) to ADP (O) consonants NOUN (O) and CCONJ (O) assign NOUN (O) the DET (O) remainder NOUN (O) of ADP (O) the DET (O) event NOUN ’s PART (O) duration NOUN (O) to ADP (O) vowels NOUN . (O) 
 For ADP (O) example NOUN , (O) consider VERB (O) a NOUN (O) one NUM (O) second NOUN (O) long ADJ (O) single ADJ (O) note NOUN (O) event NOUN (O) on ADP (O) the DET (O) word NOUN (O) Bass NOUN (O) with ADP (O) [phoneme NOUN (B) representation NOUN (I)] (B , (O) AE NOUN , (O) S. PROPN (O) 
 We PRON (O) set NOUN (O) B (O) to ADP (O) 20 NUM (O) ms PROPN , (O) S NOUN (O) to ADP (O) 100 NUM (O) ms NOUN (O) and CCONJ (O) the DET (O) remaining VERB (O) duration NOUN (O) to ADP (O) AE NOUN , (O) and CCONJ (O) hence ADV (O) have AUX (O) full ADJ (O) control NOUN (O) over ADP (O) the DET (O) duration NOUN (O) of ADP (O) each DET (O) phone NOUN . (O) 

EXPERIMENTS NOUN (O) 

We PRON (O) train NOUN (O) our DET (O) models NOUN (O) using VERB (O) the DET (O) [LJSpeech NOUN (B)] (LJS PROPN) (O) dataset NOUN , (O) the DET (O) [Sally PROPN (B) dataset NOUN (I)] , a DET (O) proprietary ADJ (O) [single ADJ (B) speaker NOUN (I) dataset PROPN (I)] with ADP (O) 20 NUM (O) hours NOUN , (O) and CCONJ (O) a NOUN (O) subset NOUN (O) of ADP (O) LibriTTS X . (O) 
 All DET (O) datasets VERB (O) used VERB (O) in ADP (O) our DET (O) experiments VERB (O) are AUX (O) from ADP (O) read VERB (O) [speech NOUN (B)] . 
 We PRON (O) provide VERB (O) results NOUN (O) that SCONJ (O) include VERB (O) style NOUN (O) transfer NOUN (O) from ADP (O) [source NOUN (B) speakers NOUN (I)] seen VERB (O) and CCONJ (O) unseen ADJ (O) in ADP (O) the DET (O) dataset NOUN , (O) from ADP (O) singers NOUN , (O) procedural ADJ (O) manipulation NOUN (O) of ADP (O) rhythm NOUN (O) and CCONJ (O) choir NOUN (O) synthesis NOUN (O) from ADP (O) music NOUN (O) scores NOUN . (O) 
 Visit NOUN (O) our DET (O) website NOUN (O) to ADP (O) listen VERB (O) to ADP (O) [Mellotron PROPN (B) samples NOUN (I)] . 

Transferring VERB (O) text NOUN , (O) rhythm NOUN (O) and CCONJ (O) [pitch NOUN (B) contour NOUN (I)] to PART (O) a NOUN (O) [target NOUN (B) speaker NOUN (I)] . 
 https://nv-adlr.github.io/Mellotron PROPN (O) 

Training NOUN (O) Setup NOUN (O) 

For ADP (O) all DET (O) the DET (O) experiments NOUN , (O) we PRON (O) trained VERB (O) on ADP (O) LJS PROPN , (O) Sally PROPN (O) and CCONJ (O) the DET (O) train NOUN - clean-100 PROPN (O) subset NOUN (O) of ADP (O) [LibriTTS NOUN (B)] with ADP (O) over ADP (O) 100 NUM (O) speakers NOUN (O) and CCONJ (O) 25 NUM (O) minutes NOUN (O) on ADP (O) average NOUN (O) per ADP (O) speaker NOUN . (O) 
 Speakers PROPN (O) with ADP (O) less ADJ (O) than SCONJ (O) 5 NUM (O) minutes NOUN (O) of ADP (O) data NOUN (O) and CCONJ (O) files NOUN (O) that SCONJ (O) are AUX (O) larger ADJ (O) than SCONJ (O) 10 NUM (O) seconds NOUN (O) were AUX (O) filtered VERB (O) out ADP . (O) 
 We PRON (O) do AUX (O) not PART (O) perform VERB (O) any DET (O) [data NOUN (B) augmentation NOUN (I)] , hence ADV (O) any DET (O) extension NOUN (O) to ADP (O) a NOUN (O) speaker NOUN ’s PUNCT (O) characteristics NOUN (O) such ADJ (O) as SCONJ (O) [vocal NOUN (B) range NOUN (I)] and CCONJ (O) [speech NOUN (B) rate NOUN (I)] is AUX (O) made VERB (O) possible ADJ (O) with ADP (O) [Mellotron PROPN (B)] . 
 We PRON (O) use NOUN (O) a NOUN (O) sampling NOUN (O) rate NOUN (O) of ADP (O) 22050 NUM (O) Hz PROPN (O) and CCONJ (O) [mel NOUN - spectrograms NOUN (B)] with ADP (O) 80 NUM (O) bins PROPN (O) using VERB (O) librosa PROPN (O) [mel X (B) filter NOUN (I)] defaults NOUN . (O) 
 We PRON (O) apply VERB (O) the DET (O) [STFT PROPN (B)] with ADP (O) a NOUN (O) FFT PROPN (O) size NOUN (O) of ADP (O) 1024 NUM , (O) hop NOUN (O) size NOUN (O) of ADP (O) 256 NUM , (O) and CCONJ (O) window NOUN (O) size NOUN (O) of ADP (O) 1024 NUM (O) samples NOUN . (O) 
 We PRON (O) use NOUN (O) the DET (O) [ADAM PROPN (B) optimizer NOUN (I)] with ADP (O) default NOUN (O) parameters NOUN , (O) start VERB (O) with ADP (O) a NOUN (O) 1e-3 NUM (O) [learning NOUN (B) rate NOUN (I)] and CCONJ (O) anneal NOUN (O) the DET (O) learning NOUN (O) rate NOUN (O) as SCONJ (O) the DET (O) loss NOUN (O) starts VERB (O) to ADP (O) plateau NOUN . (O) 
 We PRON (O) decrease NOUN (O) training NOUN (O) time NOUN (O) by ADP (O) using VERB (O) a NOUN (O) single ADJ (O) NVIDIA PROPN (O) DGX-1 NOUN (O) with ADP (O) 8 NUM (O) [GPUs NOUN (B)] . 
 For ADP (O) decoding VERB (O) the DET (O) [mel NOUN - spectrograms NOUN (B)] produced VERB (O) by ADP (O) [Mellotron PROPN (B)] , we PRON (O) use NOUN (O) a NOUN (O) single ADJ (O) [WaveGlow PROPN (B) model NOUN (I)] trained VERB (O) on ADP (O) the DET (O) [Sally PROPN (B) dataset NOUN (I)] . 
 Our DET (O) results VERB (O) suggest VERB (O) that SCONJ (O) [Waveglow PROPN (B)] can VERB (O) be AUX (O) used VERB (O) as SCONJ (O) an DET (O) universal ADJ (O) [decoder NOUN (B)] . 
 In ADP (O) our DET (O) setup NOUN , (O) we PRON (O) find VERB (O) it PRON (O) easier ADJ (O) to ADP (O) first ADJ (O) learn VERB (O) [attention NOUN (B) alignments NOUN (I)] on ADP (O) speakers NOUN (O) with ADP (O) large ADJ (O) amounts VERB (O) of ADP (O) data NOUN (O) and CCONJ (O) then ADV (O) fine NOUN (O) tune NOUN (O) to ADP (O) speakers NOUN (O) with ADP (O) [less ADJ (B) data NOUN (I)] . 
 Thus ADV , (O) we PRON (O) first ADJ (O) train NOUN (O) [Mellotron PROPN (B)] on ADP (O) LJS PROPN (O) and CCONJ (O) Sally PROPN (O) and CCONJ (O) finetune NOUN (O) it PRON (O) with ADP (O) a NOUN (O) [new PROPN (B) speaker NOUN (I)] embedding VERB (O) on ADP (O) [LibriTTS NOUN (B)] , starting VERB (O) with ADP (O) a NOUN (O) [learning NOUN (B) rate NOUN (I)] of ADP (O) 5e-4 NUM (O) and CCONJ (O) annealing VERB (O) the DET (O) [learning NOUN (B) rate NOUN (I)] as SCONJ (O) the DET (O) loss NOUN (O) starts VERB (O) to ADP (O) plateau NOUN . (O) 

Quantitative PROPN (O) Results NOUN (O) 

In ADP (O) this DET (O) section NOUN (O) we PRON (O) provide VERB (O) quantitative NOUN (O) results VERB (O) that SCONJ (O) compare VERB (O) Gross PROPN (O) [Pitch NOUN (B) Error NOUN (I)] (GPE PROPN) , (O) [Voicing VERB (B) Decision NOUN (I)] Error NOUN (O) (VDE PROPN) (O) and CCONJ (O) F0 NOUN (O) Frame PROPN (O) Error NOUN (O) (FFE PROPN) (O) between ADP (O) [Mellotron PROPN (B)] and CCONJ (O) [E2E NOUN - Prosody NOUN (B)] . 
 Following VERB , (O) all DET (O) [pitch NOUN (B)] and CCONJ (O) [voicing VERB (B) metrics NOUN (I)] are AUX (O) computed VERB (O) using VERB (O) the DET (O) [Yin PROPN (B) algorithm NOUN (I)] . 
 Due PROPN (O) to ADP (O) the DET (O) rhythm NOUN (O) conditioning NOUN , (O) our DET (O) reference NOUN (O) and CCONJ (O) predicted VERB (O) [audio NOUN (B)] have AUX (O) the DET (O) same ADJ (O) length NOUN (O) and CCONJ (O) does AUX (O) not PART (O) require VERB (O) padding NOUN . (O) 
 The DET (O) results VERB (O) in ADP (O) Table NOUN (O) 1 NUM (O) below ADP (O) show NOUN (O) that SCONJ (O) by ADP (O) conditioning NOUN (O) on ADP (O) [pitch NOUN (B)] we PRON (O) can VERB (O) drastically ADV (O) reduce VERB (O) the DET (O) error NOUN (O) between ADP (O) the DET (O) source NOUN (O) and CCONJ (O) the DET (O) synthesized VERB (O) voice NOUN . (O) 
 For ADP (O) singing NOUN (O) voice NOUN , (O) low NOUN (O) [pitch NOUN (B) error NOUN (I)] is AUX (O) extremely ADV (O) important ADJ (O) otherwise ADV (O) the DET (O) melody NOUN (O) might VERB (O) lose VERB (O) its DET (O) identity NOUN . (O) 
 For ADP (O) [prosody NOUN (B) transfer NOUN (I)] , a DET (O) lower ADJ (O) FFE PROPN (O) provides VERB (O) evidence NOUN (O) that SCONJ (O) the DET (O) style NOUN (O) will VERB (O) be AUX (O) more ADJ (O) precisely ADV (O) transferred VERB (O) to ADP (O) the DET (O) target VERB . (O) 

Table NOUN (O) : GPE NOUN , (O) VDE PROPN , (O) FFE PROPN (O) for ADP (O) [Mellotron PROPN (B)] and CCONJ (O) [E2E NOUN - Prosody NOUN (B)] . 
 The DET (O) reference NOUN (O) is AUX (O) always ADV (O) the DET (O) [same ADJ (B) speaker NOUN (I)] . 

Style NOUN (O) transfer NOUN (O) from ADP (O) [Audio NOUN (B) Signal PROPN (I)] 

[Mellotron PROPN (B)] is AUX (O) able ADJ (O) to ADP (O) emote NOUN (O) and CCONJ (O) match NOUN (O) the DET (O) style NOUN (O) of ADP (O) an DET (O) input NOUN (O) [audio NOUN (B)] by ADP (O) replicating NOUN (O) its DET (O) rhythm NOUN (O) or CCONJ (O) both DET (O) its DET (O) rhythm NOUN (O) and CCONJ (O) [pitch NOUN (B)] . 
 Overall ADV , (O) we PRON (O) note NOUN (O) that SCONJ (O) our DET (O) experiments VERB (O) using VERB (O) [audio NOUN (B) data NOUN (I)] are AUX (O) directly ADV (O) impacted VERB (O) by ADP (O) the DET (O) quality NOUN (O) of ADP (O) the DET (O) rhythm NOUN (O) and CCONJ (O) [pitch NOUN (B) contours NOUN (I)] provided VERB (O) to ADP (O) the DET (O) model NOUN . (O) 
 Whereas PROPN (O) Melodia PROPN (O) provides VERB (O) rather ADV (O) precise ADJ (O) [pitch NOUN (B) contours NOUN (I)] , we PRON (O) find VERB (O) that SCONJ (O) the DET (O) [rhythm NOUN (B) data NOUN (I)] obtained VERB (O) from ADP (O) forced VERB - alignments NOUN (O) had AUX (O) to ADP (O) be AUX (O) constantly ADV (O) fine ADV - tuned VERB . (O) 
 In ADP (O) all DET (O) [audio NOUN (B) experiments NOUN (I)] we PRON (O) obtain VERB (O) the DET (O) rhythm NOUN (O) by ADP (O) fine ADV - tuning NOUN (O) alignment NOUN (O) maps NOUN (O) obtained VERB (O) by ADP (O) using VERB (O) [Mellotron PROPN (B)] as SCONJ (O) a NOUN (O) forced VERB - aligner PROPN . (O) 
 Occasionally ADV (O) we PRON (O) find VERB (O) that SCONJ (O) some DET (O) of ADP (O) the DET (O) [pitch NOUN (B) contours NOUN (I)] seem VERB (O) to ADP (O) be AUX (O) outside ADV (O) of ADP (O) a NOUN (O) speaker NOUN ’s PUNCT (O) [vocal NOUN (B) range NOUN (I)] . 
 When ADV (O) this DET (O) happens VERB , (O) [Mellotron PROPN (B)] defaults NOUN (O) to ADP (O) a NOUN (O) constant ADJ (O) highest ADJ (O) or CCONJ (O) lowest NOUN (O) [pitch NOUN (B) value NOUN (I)] . 
 We PRON (O) circumvent NOUN (O) this DET (O) by ADP (O) scaling VERB (O) the DET (O) [pitch NOUN (B) contour NOUN (I)] by ADP (O) a NOUN (O) constant ADJ (O) to ADP (O) matches NOUN (O) the DET (O) speaker NOUN ’s PUNCT (O) [vocal NOUN (B) range NOUN (I)] . 

Rhythm PROPN (O) Transfer NOUN (O) 

In ADP (O) this DET (O) experiment NOUN (O) we PRON (O) transfer NOUN (O) the DET (O) rhythm NOUN (O) and CCONJ (O) its DET (O) associated PROPN (O) text NOUN (O) from ADP (O) a NOUN (O) source NOUN (O) [audio NOUN (B) signal NOUN (I)] to PART (O) a NOUN (O) [target NOUN (B) speaker NOUN (I)] . 
 Our DET (O) formulation NOUN (O) provides VERB (O) procedural ADJ (O) control NOUN (O) over ADP (O) the DET (O) duration NOUN (O) of ADP (O) every DET (O) [phoneme NOUN (B)] , hence ADV (O) allowing VERB (O) for ADP (O) simple ADJ (O) manipulations NOUN (O) such ADJ (O) as SCONJ (O) changing VERB (O) the DET (O) [speech NOUN (B) rate NOUN (I)] or CCONJ (O) complex NOUN (O) effects NOUN (O) like INTJ (O) speeding NOUN (O) up NOUN (O) or CCONJ (O) slowing VERB (O) down ADV . (O) 
 In ADP (O) rhythm NOUN (O) transfer NOUN , (O) we PRON (O) provide VERB (O) [Mellotron PROPN (B)] with ADP (O) an DET (O) array NOUN (O) of ADP (O) zeros PROPN (O) as SCONJ (O) the DET (O) [pitch NOUN (B) contour NOUN (I)] . 
 We PRON (O) show NOUN (O) examples VERB (O) where ADV (O) we PRON (O) transfer NOUN (O) the DET (O) rhythm NOUN (O) from ADP (O) an DET (O) excerpt NOUN (O) by ADP (O) Nicki PROPN (O) Minaj PROPN (O) to ADP (O) Sally PROPN . (O) 
 We PRON (O) showcase NOUN (O) the DET (O) procedural ADJ (O) capabilities VERB (O) of ADP (O) [Mellotron PROPN (B)] by ADP (O) processing NOUN (O) the DET (O) source NOUN (O) rhythm NOUN (O) with ADP (O) a NOUN (O) function NOUN (O) that SCONJ (O) produces VERB (O) an DET (O) accelerando NOUN (O) starting VERB (O) at ADP (O) half NOUN (O) the DET (O) speed NOUN (O) and CCONJ (O) accelerating VERB (O) to ADP (O) twice ADV (O) the DET (O) speed NOUN . (O) 
 For ADP (O) comparison NOUN , (O) we PRON (O) also ADV (O) provide VERB (O) samples NOUN (O) conditioned VERB (O) on ADP (O) the DET (O) [pitch NOUN (B) contour NOUN (I)] from ADP (O) Nicki PROPN ’s PART (O) track NOUN . (O) 
 Figure NOUN (O) 1 NUM (O) shows VERB (O) the DET (O) alignment NOUN (O) maps NOUN . (O) 

Rhythm PROPN (O) and CCONJ (O) [Pitch NOUN (B) Transfer NOUN (I)] 

By ADP (O) conditioning NOUN (O) on ADP (O) both DET (O) rhythm NOUN (O) and CCONJ (O) [pitch NOUN (B)] , we PRON (O) can VERB (O) express VERB (O) characteristics NOUN (O) of ADP (O) the DET (O) [source NOUN (B) speaker NOUN (I)] ’s PART style NOUN . (O) 
 An PROPN (O) interesting ADJ (O) application NOUN (O) is AUX (O) the DET (O) creation NOUN (O) of ADP (O) a NOUN (O) hybrid NOUN (O) with ADP (O) the DET (O) style NOUN (O) from ADP (O) a NOUN (O) [source NOUN (B) speaker NOUN (I)] but CCONJ (O) the DET (O) voice NOUN (O) from ADP (O) another DET (O) speaker NOUN . (O) 
 We PRON (O) show NOUN (O) an DET (O) example NOUN (O) where ADV (O) we PRON (O) transfer NOUN (O) the DET (O) characteristics NOUN (O) of ADP (O) a NOUN (O) solemn NOUN (O) [speech NOUN (B)] to PART (O) Sally PROPN . (O) 
 We PRON (O) see VERB (O) that SCONJ (O) [Mellotron PROPN (B)] contains VERB (O) the DET (O) same ADJ (O) pauses VERB (O) and CCONJ (O) [speech NOUN (B) rate NOUN (I)] as SCONJ (O) the DET (O) source NOUN (O) which DET (O) adds VERB (O) to ADP (O) the DET (O) solemnity NOUN (O) of ADP (O) the DET (O) [speech NOUN (B)] . 
 For ADP (O) comparison NOUN , (O) we PRON (O) provide VERB (O) the DET (O) same ADJ (O) phrases NOUN (O) synthesised VERB (O) with ADP (O) the DET (O) original ADJ (O) [Tacotron PROPN (B) 2 NUM (I)] which DET (O) fails VERB (O) to ADP (O) convey NOUN (O) the DET (O) same ADJ (O) solemnity NOUN . (O) 

Singing NOUN (O) [Voice PROPN (B) Synthesis PROPN (I)] 

[Mellotron PROPN (B)] is AUX (O) able ADJ (O) to ADP (O) generalize VERB (O) to ADP (O) rhythm NOUN (O) and CCONJ (O) [pitch NOUN (B)] from ADP (O) styles NOUN (O) and CCONJ (O) speakers NOUN (O) not PART (O) in ADP (O) the DET (O) training NOUN (O) set VERB . (O) 
 We PRON (O) are AUX (O) able ADJ (O) to ADP (O) synthesize VERB (O) [singing NOUN (B) voice NOUN (I)] from ADP (O) a NOUN (O) wide ADJ (O) range NOUN (O) of ADP (O) [input NOUN (B) speakers NOUN (I)] across ADP (O) a NOUN (O) range NOUN (O) of ADP (O) music NOUN (O) styles NOUN (O) such ADJ (O) as SCONJ (O) rap PROPN , (O) pop NOUN , (O) Hindustani PROPN (O) and CCONJ (O) western PROPN (O) European PROPN (O) classical ADJ (O) music NOUN . (O) 

Singing NOUN (O) Voice PROPN (O) from ADP (O) [Audio NOUN (B) Signal PROPN (I)] 

Figure NOUN (O) 2 NUM (O) shows VERB (O) an DET (O) example NOUN (O) where ADV (O) we PRON (O) use NOUN (O) the DET (O) Sweet PROPN (O) Dreams NOUN (O) sample NOUN (O) from ADP (O) the DET (O) [E2E NOUN - Prosody NOUN (B) paper NOUN (I)] and CCONJ (O) transfer NOUN (O) its DET (O) text NOUN , (O) rhythm NOUN (O) and CCONJ (O) scaled VERB (O) [pitch NOUN (B)] to PART (O) Sally PROPN . (O) 
 Figure NOUN (O) 2 NUM (O) shows VERB (O) that SCONJ (O) [Mellotron PROPN (B)] ’s PART [pitch NOUN (B) contour NOUN (I)] is AUX (O) closer ADV (O) to ADP (O) the DET (O) source NOUN (O) than SCONJ (O) [E2E NOUN - Prosody NOUN (B)] is AUX . (O) 

Style NOUN (O) transfer NOUN (O) from ADP (O) Music NOUN (O) Score NOUN (O) 

Unlike ADP (O) the DET (O) experiments VERB (O) on ADP (O) [audio NOUN (B)] , the DET (O) rhythm NOUN (O) and CCONJ (O) [pitch NOUN (B) contours NOUN (I)] provided VERB (O) to ADP (O) the DET (O) model NOUN (O) by ADP (O) a NOUN (O) music NOUN (O) score NOUN (O) are AUX (O) correct ADJ (O) by ADP (O) design NOUN . (O) 
 We PRON (O) provide VERB (O) a NOUN (O) 4-part NUM (O) example NOUN (O) with ADP (O) 20 NUM (O) voices NOUN (O) per ADP (O) part NOUN (O) on ADP (O) an DET (O) excerpt NOUN (O) of ADP (O) Handel PROPN ’s PUNCT (O) Hallelujah PROPN , (O) a NOUN (O) 8-part NUM (O) example NOUN (O) with ADP (O) 1 NUM (O) voice NOUN (O) per ADP (O) part NOUN (O) on ADP (O) Ligeti PROPN ’s PUNCT (O) Lux PROPN (O) Aeterna PROPN (O) and CCONJ (O) a NOUN (O) [single ADJ (B) voice NOUN (I)] example NOUN (O) synthesizing NOUN (O) the DET (O) opening NOUN (O) flute NOUN (O) intro PROPN (O) from ADP (O) Debussy PROPN ’s PUNCT (O) Prélude PROPN (O) l’après PROPN - midi PROPN (O) d’un NUM (O) faune NOUN . (O) 
 Except SCONJ (O) from ADP (O) cases NOUN (O) where ADV (O) the DET (O) [pitch NOUN (B)] is AUX (O) beyond ADP (O) the DET (O) speaker NOUN ’s PUNCT (O) [vocal NOUN (B) range NOUN (I)] , such ADJ (O) as SCONJ (O) in ADP (O) Handel PROPN ’s PUNCT (O) sample NOUN , (O) [Mellotron PROPN (B)] has AUX (O) very ADV (O) precise ADJ (O) [pitch NOUN (B)] and CCONJ (O) rhythm NOUN . (O) 

CONCLUSION NOUN (O) 

In ADP (O) this DET (O) paper NOUN (O) we PRON (O) described VERB (O) [Mellotron PROPN (B)] , a DET (O) [multispeaker NOUN (B) voice NOUN (I) synthesis NOUN (I) model NOUN (I)] that DET (O) allows VERB (O) for ADP (O) direct ADJ (O) control NOUN (O) of ADP (O) style NOUN (O) by ADP (O) conditioning NOUN (O) on ADP (O) rhythm NOUN (O) and CCONJ (O) [pitch NOUN (B)] obtained VERB (O) from ADP (O) an DET (O) [audio NOUN (B) signal NOUN (I)] or CCONJ (O) a NOUN (O) music NOUN (O) score NOUN . (O) 
 Our DET (O) numerical NOUN (O) results VERB (O) show NOUN (O) that SCONJ (O) [Mellotron PROPN (B)] is AUX (O) superior PROPN (O) to ADP (O) other ADJ (O) models NOUN (O) with ADP (O) respect NOUN (O) to ADP (O) F0 NOUN (O) Frame PROPN (O) Error NOUN . (O) 
 Our DET (O) qualitative NOUN (O) results NOUN (O) show NOUN (O) that SCONJ (O) [Mellotron PROPN (B)] is AUX (O) able ADJ (O) to ADP (O) generate NOUN (O) [speech NOUN (B)] in ADP (O) a NOUN (O) variety NOUN (O) of ADP (O) styles NOUN (O) ranging VERB (O) from ADP (O) read VERB (O) [speech NOUN (B)] to PART (O) [expressive NOUN (B) speech NOUN (I)] , from ADP (O) slow ADJ (O) drawls NOUN (O) to ADP (O) rap NOUN , (O) and CCONJ (O) from ADP (O) [monotonous ADJ (B) voice NOUN (I)] to PART (O) singing NOUN (O) voice NOUN (O) although SCONJ (O) none NOUN (O) of ADP (O) these DET (O) styles NOUN (O) are AUX (O) present NOUN (O) in ADP (O) the DET (O) [training NOUN (B) data NOUN (I)] . 
 Recent ADJ (O) [singing NOUN (B) voice NOUN (I) synthesis NOUN (I)] papers NOUN (O) state NOUN (O) that SCONJ (O) ” PUNCT (O) even ADV (O) in ADP (O) the DET (O) case NOUN (O) of ADP (O) a NOUN (O) real NOUN (O) recording NOUN (O) sample NOUN (O) recorded VERB (O) by ADP (O) listening VERB (O) to ADP (O) the DET (O) original ADJ (O) midi PROPN (O) accompaniment NOUN , (O) it PRON (O) is AUX (O) not PART (O) easy ADJ (O) to ADP (O) adjust NOUN (O) the DET (O) timing NOUN (O) and CCONJ (O) [pitch NOUN (B)] of ADP (O) the DET (O) correct ADJ (O) note NOUN (O) ” PUNCT (O) indicating VERB (O) that SCONJ (O) it PRON (O) is AUX (O) difficult ADJ (O) for ADP (O) professional ADJ (O) human NOUN (O) singers NOUN (O) and CCONJ (O) synthesized VERB (O) voice NOUN (O) to ADP (O) match NOUN (O) a NOUN (O) source NOUN (O) [audio NOUN (B)] or CCONJ (O) source NOUN (O) music NOUN (O) score NOUN (O) perfectly ADV . (O) 
 Our DET (O) results VERB (O) show NOUN (O) that SCONJ (O) one NUM (O) of ADP (O) the DET (O) advantages NOUN (O) of ADP (O) [Mellotron PROPN (B)] is AUX (O) that SCONJ (O) the DET (O) rhythm NOUN (O) and CCONJ (O) [pitch NOUN (B) contour NOUN (I)] of ADP (O) a NOUN (O) synthesized VERB (O) sample NOUN (O) is AUX (O) extremely ADV (O) similar ADJ (O) to ADP (O) the DET (O) source NOUN (O) [audio NOUN (B) file NOUN (I)] or CCONJ (O) music NOUN (O) score NOUN , (O) under ADP (O) the DET (O) assumption NOUN (O) that SCONJ (O) the DET (O) [pitch NOUN (B)] is AUX (O) within ADP (O) a NOUN (O) speaker NOUN ’s PUNCT (O) [vocal NOUN (B) range NOUN (I)] . 
 When ADV (O) outside ADV (O) a NOUN (O) speaker NOUN ’s PUNCT (O) [vocal NOUN (B) range NOUN (I)] , [Mellotron PROPN (B)] defaults NOUN (O) to ADP (O) either CCONJ (O) the DET (O) lowest NOUN (O) tone NOUN (O) or CCONJ (O) highest ADJ (O) tone NOUN . (O) 
 For ADP (O) future NOUN (O) work NOUN , (O) we PRON (O) plan NOUN (O) to ADP (O) study NOUN (O) the DET (O) effect NOUN (O) of ADP (O) rhythm NOUN (O) and CCONJ (O) [pitch NOUN (B) contours NOUN (I)] on ADP (O) the DET (O) [audio NOUN (B) quality NOUN (I)] by ADP (O) comparing VERB (O) samples NOUN (O) conditioned VERB (O) on ADP (O) [pitch NOUN (B)] and CCONJ (O) [rhythm NOUN (B) data NOUN (I)] obtained VERB (O) from ADP (O) [audio NOUN (B) signals NOUN (I)] versus X (O) music NOUN (O) scores NOUN . (O) 
 With ADP (O) respect NOUN (O) to ADP (O) [pitch NOUN (B)] , we PRON (O) are AUX (O) also ADV (O) interested ADJ (O) in ADP (O) understanding NOUN (O) the DET (O) effect NOUN (O) of ADP (O) [multi ADJ - speaker ADJ (B) training NOUN (I)] on ADP (O) a NOUN (O) speaker NOUN ’s PUNCT (O) [vocal NOUN (B) range NOUN (I)] and CCONJ (O) extending VERB (O) a NOUN (O) speaker NOUN ’s PUNCT (O) [vocal NOUN (B) range NOUN (I)] as SCONJ (O) much ADJ (O) as SCONJ (O) possible ADJ . (O) 
 Last ADJ , (O) we PRON (O) would VERB (O) like INTJ (O) to ADP (O) train NOUN (O) [Mellotron PROPN (B)] on ADP (O) a NOUN (O) animated VERB (O) and CCONJ (O) emotive NOUN (O) storytelling NOUN (O) [style NOUN (B) dataset NOUN (I)] to PART (O) investigate VERB (O) the DET (O) contribution NOUN (O) of ADP (O) [such ADJ (B) dataset NOUN (I)] to PART (O) [Mellotron PROPN (B)] . 
