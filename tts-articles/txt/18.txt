                                           MELLOTRON: MULTISPEAKER EXPRESSIVE VOICE SYNTHESIS BY CONDITIONING
                                                       ON RHYTHM, PITCH AND GLOBAL STYLE TOKENS

                                                                       Rafael Valle*, Jason Li*, Ryan Prenger, Bryan Catanzaro

                                                                                             NVIDIA Corporation


                                                                 ABSTRACT                                   easily extended to singing voice synthesis (SVS) [3, 4]. Un-
arXiv:1910.11997v1 [cs.SD] 26 Oct 2019




                                                                                                            fortunately, recent attempts [4] require a singing voice dataset
                                         Mellotron is a multispeaker voice synthesis model based on
                                                                                                            and heavily quantized pitch and rhythm data obtained from a
                                         Tacotron 2 GST that can make a voice emote and sing with-
                                                                                                            digital representation of a music score, for example MIDI [5]
                                         out emotive or singing training data. By explicitly condition-
                                                                                                            or musicXML [6]. Mellotron does not require any singing
                                         ing on rhythm and continuous pitch contours from an audio
                                                                                                            voice in the dataset nor manually aligned pitch and text in
                                         signal or music score, Mellotron is able to generate speech
                                                                                                            order to synthesize singing voice.
                                         in a variety of styles ranging from read speech to expressive
                                                                                                                Mellotron can make a voice emote and sing without emo-
                                         speech, from slow drawls to rap and from monotonous voice
                                                                                                            tion or singing data. Training Mellotron is very simple and
                                         to singing voice. Unlike other methods, we train Mellotron
                                                                                                            only requires read speech and transcriptions. During infer-
                                         using only read speech data without alignments between text
                                                                                                            ence, we can change the generated voice’s speaking style,
                                         and audio. We evaluate our models using the LJSpeech and
                                                                                                            make it emote or sing by extracting pitch and rhythm char-
                                         LibriTTS datasets. We provide F0 Frame Errors and synthe-
                                                                                                            acteristics from an audio file or a music score. As a bonus,
                                         sized samples that include style transfer from other speakers,
                                                                                                            with Mellotron we can explore latent characteristics from an
                                         singers and styles not seen during training, procedural manip-
                                                                                                            audio corpus by sampling a dictionary of learned latent char-
                                         ulation of rhythm and pitch and choir synthesis.
                                                                                                            acteristics. In summary, Mellotron is a versatile voice syn-
                                            Index Terms— Text-to-Speech Synthesis, Singing Voice            thesis1 model that enables the combination of characteristics
                                         Synthesis, Style Transfer, Deep learning                           from different sources and generalizes to characteristics not
                                                                                                            seen in training data.
                                                            1. INTRODUCTION
                                                                                                                                          2. METHOD
                                         Speech synthesis is typically formulated as the conversion of
                                         text to speech (TTS). This formulation, however, leaves out        Mellotron is a voice synthesis model that uses a combination
                                         control for all the aspects of speech not contained in the text.   of explicit and latent variables. Whereas well-established sig-
                                         Here we approach the problem of expressive speech synthe-          nal processing algorithms provide explicit variables that are
                                         sis which includes not just text, but other characteristics such   valuable to expressive speech such as fundamental frequency
                                         as pitch, rhythm and emphasis. There are formulations to ex-       contours and voicing decisions, deep learning strategies can
                                         pressive speech synthesis that require animated and emotive        be used to learn latent variables that express characteristics
                                         voice data. This is an inconvenient drawback given the lim-        of an audio corpus that are unknown to the user and hard to
                                         ited access to such data. In our approach, we can make a voice     formalize.
                                         emote and sing without any such data.                                  We factorize a single speaker mel-spectrogram M into ex-
                                              Recent approaches that utilize deep learning for expres-      plicit variables such as text, speaker identity, a fundamental
                                         sive speech synthesis combine text and a learned latent em-        frequency contour augmented with voiced/unvoiced decisions
                                         bedding for prosody or global style [1, 2]. While these ap-        and two latent variables learned by the model during train-
                                         proaches have shown promise, manipulating such latent vari-        ing. The first latent variable refers to a dictionary of vectors
                                         ables only offers a coarse control over expressive character-      that can be queried with an audio input or sampled directly
                                         istics of speech. Mellotron was motivated by the desire for        as described in [2]. The second latent variable is the learned
                                         fine grained control over these expressive characteristics. No-    attention map between the text and the mel-spectrogram as
                                         tably, we show that it is easy to condition Mellotron on pitch     described in [7].
                                         and rhythm information automatically extracted from an au-             From now on we will refer to the augmented fundamen-
                                         dio signal or music score.                                         tal frequency contour as pitch contour and refer to the first
                                              By accounting for melodic information such as pitch and
                                         rhythm, expressive speech synthesis with Mellotron can be            1 Includes   speech synthesis, singing voice synthesis, etc.


and second latent variables as global style tokens (GST) and                              3. IMPLEMENTATION
rhythm respectively.
    We are interested in factorizing M = [T, S, P, R, Z],                In this section we are going to describe our model architecture
where T represents the text, S represents the speaker identity,          and our training and inference setups. We plan to release our
P represents the pitch contour, R represents the rhythm and              implementation and pre-trained models on github.
Z represents the global style tokens. Given this formulation,
during training we maximize the following:                               3.1. Architecture
                                                                         Mellotron extends Tacotron 2 GST [2] with speaker embed-
         P (mel(i) |T (i) , S (i) , P (i) , R(i) , Zmel(i) ; θ),   (1)   dings and pitch countours. Unlike [8, 9], where site specific
                                                                         speaker embeddings are used, we use a single speaker em-
    where the superscript i represents the i-th mel, T (i) , S (i)       bedding that is channel-wise concatenated with the encoder
and P (i) represent the text, speaker, and pitch contour asso-           outputs over every token. The pitch contour goes through a
ciated with the i-th mel, Ri represents the learned alignments           single convolution layer followed by a ReLU non-linearity.
between the text and mel-spectrogram frames, Zmel(i) rep-                We experiment with kernel sizes 1 and 3 and convolution di-
resents the global style token conditioned on mel(i) as pre-             mensions 1 and 8. The pitch contour is channel-wise concate-
sented in [2], and θ represents the model parameters.                    nated with the decoder inputs. We use phoneme representa-
    The explicit factors offers two advantages. First, by pro-           tions whenever possible.
viding the model with text and speaker information, we pre-
vent the problem of entanglement between text and speaker                3.2. Training
information. Second by providing the model with pitch con-
tour and voicing information, we are able to directly control            Our implementation only requires text and audio pairs with
pitch and voicing decisions during inference.                            a speaker id. Our pitch contours are automatically extracted
    Similarly the latent factors offers two advantages. First,           using the Yin algorithm [10] with harmonicity thresholds
by learning the alignment map between the text and mel-                  between 0.1 and 0.25. Unlike [4], during training our
spectrogram during training, we do not need to extract                   model does not require manually aligned text, pitch and mel-
phoneme alignments for training and can control the rhythm               spectrogram. We use the L2 loss between ground truth and
during inference by providing the model with an alignment                predicted mels described in [2] without any modifications.
map. Second, by providing the model with a dictionary of
latent variables, we are able to learn latent factors that are           3.3. Inference
harder to express or extract explicitly, thus leveraging the full
power of latent variables.                                               Following the description in Section 2, during inference we
    Using this formulation we are able to transfer the text,             provide Melloron with text, rhythm and pitch information that
rhythm and pitch contour from a source, e. g. audio signal or            is obtained either from an audio signal or from a musical
musical score, to a target speaker by replacing the variables            score, a global style token and a speaker id.
in Equation 1 accordingly. For example, we first collect the
text, pitch and rhythm (Ts , Ps , Rs ), from the source, sample          3.3.1. Audio Signal
a GST Zquery from the GST dictionary learned by Mellotron,
and chose a target speaker St .                                          Obtaining text, rhythm and pitch information consists of three
                                                                         steps. First, we extract text information from an audio file by
                                                                         either using an automatic speech recognition model [11, 12]
             P (melout |Ts , Ps , St , Rs , Zquery ; θ)            (2)   or by manually transcribing the text. The text information is
                                                                         pre-processed with our text cleaners and then converted from
     melout should now have the same text, pitch and rhythm              graphemes to phonemes.
as the source, latent characteristics obtained from the global               Second, we extract rhythm information by using a forced-
style token and the voice of the target speaker. In our current          alignment tool [13, 14] or by using Mellotron as a forced-
formulation, the target speaker, St , would always be found              aligner. Alignment maps can be obtained with Mellotron by
in the training set, while the source text, pitch and rhythm             performing a teacher-forced forward pass using the data from
(Ts , Ps , Rs ) could be from outside the training set. This al-         the source signal. Whenever necessary, we fine tune the align-
lows us to train a model that makes a voice emote and sing               ment maps by hand or by training Mellotron on the source
without using any singing voice in the training dataset, with-           signal for a few iterations with small learning rate.
out any manual labelling of emotions nor pitch, and without                  The pitch data is obtained by using Yin [10] or Melodia
any manual alignments between words and audio, nor be-                   [15]. In our quantitative experiments we use Yin to replicate
tween pitch and audio.                                                   the setup described in [1]. In our qualitative experiments we


use Melodia instead as we find it to be more precise than Yin,               For decoding the mel-spectrograms produced by Mel-
specially with regards to false voiced decisions.                        lotron, we use a single WaveGlow [19] model trained on the
                                                                         Sally dataset. Our results suggest that Waveglow can be used
                                                                         as an universal decoder.
3.3.2. Music Score
                                                                             In our setup, we find it easier to first learn attention align-
We operate on music scores in XML format containing event                ments on speakers with large amounts of data and then fine
tuples with pitch, note duration and syllables for each part             tune to speakers with less data. Thus, we first train Mellotron
in the score. We directly convert pitch to frequency and use             on LJS and Sally and finetune it with a new speaker embed-
the FFT hop size to convert event durations from seconds to              ding on LibriTTS, starting with a learning rate of 5e-4 and
frames. We remind the reader that although we refer to pitch,            annealing the learning rate as the loss starts to plateau.
our model’s representation of pitch is continuous.
    We concatenate the syllables into words and convert                  4.2. Quantitative Results
graphemes to phonemes. For single phone events, the dura-
                                                                         In this section we provide quantitative results that compare
tion of each phone is equal to the duration of the event. For
                                                                         Gross Pitch Error (GPE) [20], Voicing Decision Error (VDE)
multi-phone events, the duration of each phone is dependent
                                                                         [20] and F0 Frame Error (FFE) [21] between Mellotron and
on its type: we use heuristics to assign durations between
                                                                         E2E-Prosody [1]. Following [1], all pitch and voicing metrics
20 and 100ms to consonants and assign the remainder of the
                                                                         are computed using the Yin algorithm [10]. Due to the rhythm
event’s duration to vowels. For example, consider a one sec-
                                                                         conditioning, our reference and predicted audio have the same
ond long single note event on the word Bass with phoneme
                                                                         length and does not require padding.
representation [B, AE, S]. We set B to 20 ms, S to 100 ms
                                                                             The results in Table 1 below show that by conditioning on
and the remaining duration to AE, and hence have full control
                                                                         pitch we can drastically reduce the error between the source
over the duration of each phone.
                                                                         and the synthesized voice. For singing voice, low pitch error
                                                                         is extremely important otherwise the melody might lose its
                          4. EXPERIMENTS                                 identity. For prosody transfer, a lower FFE provides evidence
                                                                         that the style will be more precisely transferred to the target.
We train our models using the LJSpeech (LJS) dataset [16],
the Sally dataset, a proprietary single speaker dataset with 20
hours, and a subset of LibriTTS [17]. All datasets used in our                Model             Voice        GPE       VDE        FFE
experiments are from read speech.                                           E2E-Prosody        Single          -          -      28.1%
    We provide results that include style transfer2 from source             E2E-Prosody         Multi          -          -      27.5%
speakers seen and unseen in the dataset, from singers, proce-                Mellotron        LJS-Sally     0.08%      9.19%     9.28%
dural manipulation of rhythm and choir synthesis from music                  Mellotron        LibriTTS      0.08%      8.69%     8.77%
scores. Visit our website3 to listen to Mellotron samples.
                                                                         Table 1: GPE, VDE, FFE for Mellotron and E2E-Prosody.
                                                                         The reference is always the same speaker.
4.1. Training Setup
For all the experiments, we trained on LJS, Sally and the
train-clean-100 subset of LibriTTS with over 100 speakers
and 25 minutes on average per speaker. Speakers with less                4.3. Style transfer from Audio Signal
than 5 minutes of data and files that are larger than 10 seconds
were filtered out. We do not perform any data augmentation,              Mellotron is able to emote and match the style of an in-
hence any extension to a speaker’s characteristics such as vo-           put audio by replicating its rhythm or both its rhythm and
cal range and speech rate is made possible with Mellotron.               pitch. Overall, we note that our experiments using audio
                                                                         data are directly impacted by the quality of the rhythm and
    We use a sampling rate of 22050 Hz and mel-spectrograms
                                                                         pitch contours provided to the model. Whereas Melodia pro-
with 80 bins using librosa mel filter defaults. We apply the
                                                                         vides rather precise pitch contours, we find that the rhythm
STFT with a FFT size of 1024, hop size of 256, and window
                                                                         data obtained from forced-alignments had to be constantly
size of 1024 samples.
                                                                         fine-tuned. In all audio experiments we obtain the rhythm
    We use the ADAM [18] optimizer with default parame-
                                                                         by fine-tuning alignment maps obtained by using Mellotron
ters, start with a 1e-3 learning rate and anneal the learning
                                                                         as a forced-aligner. Occasionally we find that some of the
rate as the loss starts to plateau. We decrease training time by
                                                                         pitch contours seem to be outside of a speaker’s vocal range.
using a single NVIDIA DGX-1 with 8 GPUs.
                                                                         When this happens, Mellotron defaults to a constant highest
  2 Transferring   text, rhythm and pitch contour to a target speaker.   or lowest pitch value. We circumvent this by scaling the pitch
  3 https://nv-adlr.github.io/Mellotron                                  contour by a constant to matches the speaker’s vocal range.


4.3.1. Rhythm Transfer

In this experiment we transfer the rhythm and its associated
text from a source audio signal to a target speaker. Our formu-
lation provides procedural control over the duration of every
phoneme, hence allowing for simple manipulations such as
                                                                  Fig. 2: Source, Mellotron and E2E-Prosody pitch contours.
changing the speech rate or complex effects like speeding up
or slowing down. In rhythm transfer, we provide Mellotron
with an array of zeros as the pitch contour.                      4.4.2. Style transfer from Music Score
    We show examples where we transfer the rhythm from an
excerpt by Nicki Minaj to Sally. We showcase the procedu-         Unlike the experiments on audio, the rhythm and pitch con-
ral capabilities of Mellotron by processing the source rhythm     tours provided to the model by a music score are correct by
with a function that produces an accelerando starting at half     design. We provide a 4-part example with 20 voices per part
the speed and accelerating to twice the speed. For compari-       on an excerpt of Handel’s Hallelujah, a 8-part example with
son, we also provide samples conditioned on the pitch contour     1 voice per part on Ligeti’s Lux Aeterna and a single voice
from Nicki’s track. Figure 1 shows the alignment maps.            example synthesizing the opening flute intro from Debussy’s
                                                                  Prlude l’aprs-midi d’un faune. Except from cases where the
                                                                  pitch is beyond the speaker’s vocal range, such as in Handel’s
                                                                  sample, Mellotron has very precise pitch and rhythm.

                                                                                       5. CONCLUSION

Fig. 1: Left: source alignment. Right: processed alignment.       In this paper we described Mellotron, a multispeaker voice
                                                                  synthesis model that allows for direct control of style by con-
                                                                  ditioning on rhythm and pitch obtained from an audio signal
                                                                  or a music score.
4.3.2. Rhythm and Pitch Transfer                                      Our numerical results show that Mellotron is superior to
                                                                  other models with respect to F0 Frame Error. Our qualita-
By conditioning on both rhythm and pitch, we can express          tive results show that Mellotron is able to generate speech
characteristics of the source speaker’s style. An interesting     in a variety of styles ranging from read speech to expressive
application is the creation of a hybrid with the style from a     speech, from slow drawls to rap, and from monotonous voice
source speaker but the voice from another speaker. We show        to singing voice although none of these styles are present in
an example where we transfer the characteristics of a solemn      the training data.
speech to Sally. We see that Mellotron contains the same              Recent singing voice synthesis papers [4] state that ”even
pauses and speech rate as the source which adds to the solem-     in the case of a real recording sample recorded by listening
nity of the speech. For comparison, we provide the same           to the original midi accompaniment, it is not easy to adjust
phrases synthesised with the original Tacotron 2 which fails      the timing and pitch of the correct note” indicating that it is
to convey the same solemnity.                                     difficult for professional human singers and synthesized voice
                                                                  to match a source audio or source music score perfectly. Our
                                                                  results show that one of the advantages of Mellotron is that
4.4. Singing Voice Synthesis
                                                                  the rhythm and pitch contour of a synthesized sample is ex-
Mellotron is able to generalize to rhythm and pitch from styles   tremely similar to the source audio file or music score, under
and speakers not in the training set. We are able to synthesize   the assumption that the pitch is within a speaker’s vocal range.
singing voice from a wide range of input speakers across a        When outside a speaker’s vocal range, Mellotron defaults to
range of music styles such as rap, pop, Hindustani and west-      either the lowest tone or highest tone.
ern European classical music.                                         For future work, we plan to study the effect of rhythm
                                                                  and pitch contours on the audio quality by comparing samples
                                                                  conditioned on pitch and rhythm data obtained from audio
4.4.1. Singing Voice from Audio Signal                            signals versus music scores. With respect to pitch, we are also
                                                                  interested in understanding the effect of multi-speaker train-
Figure 2 shows an example where we use the Sweet Dreams           ing on a speaker’s vocal range and extending a speaker’s vocal
sample from the E2E-Prosody paper [1] and transfer its            range as much as possible. Last, we would like to train Mel-
text, rhythm and scaled pitch to Sally. Figure 2 shows that       lotron on a animated and emotive storytelling style dataset to
Mellotron’s pitch contour is closer to the source than E2E-       investigate the contribution of such dataset to Mellotron.
Prosody is.


                    6. REFERENCES                                 [11] Jason Li, Vitaly Lavrukhin, Boris Ginsburg, Ryan Leary,
                                                                       Oleksii Kuchaiev, Jonathan M Cohen, Huyen Nguyen,
 [1] RJ Skerry-Ryan, Eric Battenberg, Ying Xiao, Yuxuan                and Ravi Teja Gadde, “Jasper: An end-to-end con-
     Wang, Daisy Stanton, Joel Shor, Ron J Weiss, Rob                  volutional neural acoustic model,”      arXiv preprint
     Clark, and Rif A Saurous, “Towards end-to-end prosody             arXiv:1904.03288, 2019.
     transfer for expressive speech synthesis with tacotron,”
     arXiv preprint arXiv:1803.09047, 2018.                       [12] Chung-Cheng Chiu, Tara N Sainath, Yonghui Wu, Ro-
                                                                       hit Prabhavalkar, Patrick Nguyen, Zhifeng Chen, An-
 [2] Yuxuan Wang, Daisy Stanton, Yu Zhang, RJ Skerry-                  juli Kannan, Ron J Weiss, Kanishka Rao, Ekaterina Go-
     Ryan, Eric Battenberg, Joel Shor, Ying Xiao, Fei Ren,             nina, et al., “State-of-the-art speech recognition with
     Ye Jia, and Rif A Saurous, “Style tokens: Unsuper-                sequence-to-sequence models,” in 2018 IEEE Interna-
     vised style modeling, control and transfer in end-to-end          tional Conference on Acoustics, Speech and Signal Pro-
     speech synthesis,” arXiv preprint arXiv:1803.09017,               cessing (ICASSP). IEEE, 2018, pp. 4774–4778.
     2018.
                                                                  [13] R. M. Ochshorn and M. Hawkins,            “Gentle forced
 [3] Masanari Nishimura, Kei Hashimoto, Keiichiro Oura,                aligner,” .
     Yoshihiko Nankaku, and Keiichi Tokuda, “Singing              [14] Michael McAuliffe, Michaela Socolof, Sarah Mihuc,
     voice synthesis based on deep neural networks,” in In-            Michael Wagner, and Morgan Sonderegger, “Montreal
     terspeech 2016, 2016, pp. 2478–2482.                              forced aligner: Trainable text-speech alignment using
 [4] Juheon Lee, Hyeong-Seok Choi, Chang-Bin Jeon,                     kaldi.,” in Interspeech, 2017, pp. 498–502.
     Junghyun Koo, and Kyogu Lee, “Adversarially trained          [15] Justin Salamon and Emilia Gómez, “Melody extraction
     end-to-end korean singing voice synthesis system,”                from polyphonic music signals using pitch contour char-
     arXiv preprint arXiv:1908.01919, 2019.                            acteristics,” IEEE Transactions on Audio, Speech, and
                                                                       Language Processing, vol. 20, no. 6, pp. 1759–1770,
 [5] Robert A Moog, “Midi: musical instrument digital in-
                                                                       2012.
     terface,” Journal of the Audio Engineering Society, vol.
     34, no. 5, pp. 394–404, 1986.                                [16] Keith Ito, “The lj speech dataset,” https://
                                                                       keithito.com/LJ-Speech-Dataset/, 2017.
 [6] Michael Good, “Musicxml for notation and analysis,”
     The virtual score: representation, retrieval, restoration,   [17] Heiga Zen, Viet Dang, Rob Clark, Yu Zhang, Ron J
     vol. 12, pp. 113–124, 2001.                                       Weiss, Ye Jia, Zhifeng Chen, and Yonghui Wu, “Lib-
                                                                       ritts: A corpus derived from librispeech for text-to-
 [7] Jonathan Shen, Ruoming Pang, Ron J Weiss, Mike                    speech,” arXiv preprint arXiv:1904.02882, 2019.
     Schuster, Navdeep Jaitly, Zongheng Yang, Zhifeng
     Chen, Yu Zhang, Yuxuan Wang, Rj Skerrv-Ryan, et al.,         [18] Diederik P Kingma and Jimmy Ba,        “Adam: A
     “Natural tts synthesis by conditioning wavenet on mel             method for stochastic optimization,” arXiv preprint
     spectrogram predictions,” in 2018 IEEE International              arXiv:1412.6980, 2014.
     Conference on Acoustics, Speech and Signal Processing        [19] Ryan Prenger, Rafael Valle, and Bryan Catanzaro,
     (ICASSP). IEEE, 2018, pp. 4779–4783.                              “Waveglow: A flow-based generative network for
                                                                       speech synthesis,” in ICASSP 2019-2019 IEEE Inter-
 [8] Andrew Gibiansky, Sercan Arik, Gregory Diamos, John
                                                                       national Conference on Acoustics, Speech and Signal
     Miller, Kainan Peng, Wei Ping, Jonathan Raiman, and
                                                                       Processing (ICASSP). IEEE, 2019, pp. 3617–3621.
     Yanqi Zhou, “Deep voice 2: Multi-speaker neural text-
     to-speech,” in Advances in neural information process-       [20] Tomohiro Nakatani, Shigeaki Amano, Toshio Irino,
     ing systems, 2017, pp. 2962–2970.                                 Kentaro Ishizuka, and Tadahisa Kondo, “A method for
                                                                       fundamental frequency estimation and voicing decision:
 [9] Wei Ping, Kainan Peng, Andrew Gibiansky, Sercan O                 Application to infant utterances recorded in real acousti-
     Arik, Ajay Kannan, Sharan Narang, Jonathan Raiman,                cal environments,” Speech Communication, vol. 50, no.
     and John Miller, “Deep voice 3: Scaling text-to-speech            3, pp. 203–214, Mar. 2008.
     with convolutional sequence learning,” arXiv preprint
     arXiv:1710.07654, 2017.                                      [21] Wei Chu and Abeer Alwan, “Reducing f0 frame error
                                                                       of f0 tracking algorithms under noisy conditions with an
[10] Alain De Cheveigné and Hideki Kawahara, “Yin, a fun-             unvoiced/voiced classification frontend,” in 2009 IEEE
     damental frequency estimator for speech and music,”               International Conference on Acoustics, Speech and Sig-
     The Journal of the Acoustical Society of America, vol.            nal Processing. IEEE, 2009, pp. 3969–3972.
     111, no. 4, pp. 1917–1930, 2002.
